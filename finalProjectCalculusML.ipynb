{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the **House Pricing** dataset, where you have a lot of information about the houses being sold and you aim to produce the price of the house. \n",
    "\n",
    "Firstly, let us import basic libraries (`numpy` ([docs](https://numpy.org/)) for matrix operations and `pandas` ([docs](https://pandas.pydata.org/)) for convinient dataset workaround):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datX=np.load('x_train.npy')\n",
    "datY=np.log(np.load('y_train.npy'))\n",
    "datX=pd.DataFrame(datX, columns=datX.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.36138028, 13.031782  , 12.36307639, ..., 12.01370075,\n",
       "       12.89921983, 12.9456262 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do it let us plot every feature vs the price. Firstly, we import nice plotting modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax=plt.subplots(4, 4, figsize=(16,16))\n",
    "for i, name in enumerate(datX.columns):\n",
    "    ax[i//4][i%4].scatter(datX[name], datY)\n",
    "    ax[i//4][i%4].set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'condition', 'grade', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datX.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say, that we choose to work the following set of features:\n",
    "+ `bedrooms`\n",
    "+ `bathrooms`\n",
    "+ `sqft_living`\n",
    "+ `floors`\n",
    "+ `condition`\n",
    "+ `grade`\n",
    "+ `sqft_above`\n",
    "+ `sqft_basement`\n",
    "+ `long`\n",
    "+ `lat`\n",
    "\n",
    "Clear the dataset from all the other features and create:\n",
    "1. matrix $X$, all elements should be real numbers\n",
    "2. number $N$ -- number of considered houses\n",
    "3. number $m$ -- number of new features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datX = datX.drop(['date', 'sqft_lot' , 'waterfront' , 'yr_built' , 'yr_renovated' , 'zipcode'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datX\n",
    "datX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to automatically check results of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All passed\n",
      "Total check:  3 /3\n"
     ]
    }
   ],
   "source": [
    "X = datX.to_numpy()\n",
    "N = 15000\n",
    "m = 10\n",
    "from checker import Reader\n",
    "Reader(X,N,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider that we are interested in the loss of the model we discussed in the video:\n",
    "\n",
    "+ Assume we have input data that is denoted as $\\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_N$\n",
    "+ House prices for this input data are known $y_1, y_2, \\ldots, y_N$\n",
    "\n",
    "We propose a **simple linear model** for this task:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$\n",
    "\n",
    "As a loss function we will use the mean squared error (**MSE**):\n",
    "\n",
    "$$\n",
    "Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(w_k, X, y):\n",
    "    lossValue = 0\n",
    "    y_calc = np.empty(15000)\n",
    "    for j in range(15000):\n",
    "         y_calc[j] = w_k[0] + w_k[1]*X[j][0] + w_k[2]*X[j][1] + w_k[3]*X[j][2] + w_k[4]*X[j][3] + w_k[5]*X[j][4] + w_k[6]*X[j][5] + w_k[7]*X[j][6] + w_k[8]*X[j][7] + w_k[9]*X[j][8] + w_k[10]*X[j][9]\n",
    "    for i in range(15000):\n",
    "        lossValue = lossValue + (y_calc[i] - y[i])**2\n",
    "    lossValue = lossValue/y.size\n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to automatically check results of your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random test  0 :  OK, time_User= 0.33139967918395996 s;  timeBaseline= 0.050864458084106445 s\n",
      "Random test  1 :  OK, time_User= 0.36704397201538086 s;  timeBaseline= 0.003988742828369141 s\n",
      "Random test  2 :  OK, time_User= 0.6991887092590332 s;  timeBaseline= 0.003989458084106445 s\n",
      "Random test  3 :  OK, time_User= 0.3779876232147217 s;  timeBaseline= 0.0029926300048828125 s\n",
      "Random test  4 :  OK, time_User= 0.35905909538269043 s;  timeBaseline= 0.0029926300048828125 s\n",
      "Random test  5 :  OK, time_User= 0.35157132148742676 s;  timeBaseline= 0.0029914379119873047 s\n",
      "Random test  6 :  OK, time_User= 0.35504817962646484 s;  timeBaseline= 0.001995086669921875 s\n",
      "Random test  7 :  OK, time_User= 0.33462095260620117 s;  timeBaseline= 0.002993345260620117 s\n",
      "Random test  8 :  OK, time_User= 0.34758520126342773 s;  timeBaseline= 0.0029926300048828125 s\n",
      "Random test  9 :  OK, time_User= 0.3431065082550049 s;  timeBaseline= 0.0029916763305664062 s\n",
      "All passed\n",
      "Total check:  10 /10\n"
     ]
    }
   ],
   "source": [
    "from checker import lossOK\n",
    "lossOK(loss, X, datY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ Loss(\\vec{w})=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2 $$ \n",
    "taking partial derivative with respect to w_k\n",
    "$$ \\frac{\\partial Loss}{\\partial w_k} = \\frac{1}{N}\\sum_{i=1}^N 2 \\, (y_i-\\hat{y}_i) \\left(\\! -\\frac{\\partial \\hat{y}_i}{\\partial w_k} \\!\\right) $$\n",
    "and with the other given equation above \n",
    "$$ \\hat{y}_i = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_m x_m = \\sum_{j=1}^N X_{ij} w_j $$ \n",
    "where\n",
    "$$ (X_{ij}) = (\\vec{1}, \\vec{x}_1, \\vec{x}_2, \\ldots, \\vec{x}_N) $$\n",
    "is the extended $X$ matrix, we find\n",
    "$$ \\frac{\\partial Loss}{\\partial w_k} = \\frac{-2}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i) X_{ik} \\;.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad(w_k, X, y):\n",
    "    y_calc = np.empty(15000)\n",
    "    for j in range(15000):\n",
    "        y_calc[j] = w_k[0] + w_k[1]*X[j][0] + w_k[2]*X[j][1] + w_k[3]*X[j][2] + w_k[4]*X[j][3] + w_k[5]*X[j][4] + w_k[6]*X[j][5] + w_k[7]*X[j][6] + w_k[8]*X[j][7] + w_k[9]*X[j][8] + w_k[10]*X[j][9]\n",
    "    sum = 0\n",
    "    for i in range(15000):\n",
    "        sum = sum + y[i] - y_calc[i]\n",
    "    lvu = np.zeros(11)\n",
    "    lvu[0] = sum\n",
    "    for i in range(1,11):\n",
    "        sum = 0\n",
    "        for j in range(15000):\n",
    "            constant = X[j][i-1]\n",
    "            temp_sum = y[j] - w_k[0] - w_k[1]*X[j][0] - w_k[2]*X[j][1] - w_k[3]*X[j][2] - w_k[4]*X[j][3] - w_k[5]*X[j][4] - w_k[6]*X[j][5] - w_k[7]*X[j][6] - w_k[8]*X[j][7] - w_k[9]*X[j][8] - w_k[10]*X[j][9]\n",
    "            temp_sum = temp_sum*constant\n",
    "            sum = sum + temp_sum\n",
    "        lvu[i] = sum\n",
    "    lvu = (-2/15000)*lvu\n",
    "    return lvu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to automatically check your function. You will also see time comparison: if your function works sufficiently slower, you probably should think of faster alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random test  0 :  OK, time_User= 3.945237159729004 s;  timeBaseline= 0.03988933563232422 s\n",
      "Random test  1 :  OK, time_User= 3.941283702850342 s;  timeBaseline= 0.002992868423461914 s\n",
      "Random test  2 :  OK, time_User= 3.8715221881866455 s;  timeBaseline= 0.0029935836791992188 s\n",
      "Random test  3 :  OK, time_User= 4.775700330734253 s;  timeBaseline= 0.002994537353515625 s\n",
      "Random test  4 :  OK, time_User= 3.5667002201080322 s;  timeBaseline= 0.000997304916381836 s\n",
      "Random test  5 :  OK, time_User= 4.081505537033081 s;  timeBaseline= 0.002996683120727539 s\n",
      "Random test  6 :  OK, time_User= 5.162919998168945 s;  timeBaseline= 0.0019948482513427734 s\n",
      "Random test  7 :  OK, time_User= 3.9640262126922607 s;  timeBaseline= 0.0019965171813964844 s\n",
      "Random test  8 :  OK, time_User= 5.121780157089233 s;  timeBaseline= 0.0019838809967041016 s\n",
      "Random test  9 :  OK, time_User= 3.612471103668213 s;  timeBaseline= 0.0019974708557128906 s\n",
      "All passed\n",
      "Total check:  10 /10\n"
     ]
    }
   ],
   "source": [
    "from checker import gradOK\n",
    "gradOK(grad, X, datY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How it is time to formulate the gradient descent! As you remeember, the idea here is that:\n",
    "$$\n",
    "\\vec{w}^{k+1}=\\vec{w}^{k}-\\alpha_k\\cdot \\nabla Loss(\\vec{w}^{k})\n",
    "$$\n",
    "We propose that you use constant $\\alpha_k=\\alpha$. Assume that the method should stop in two cases:\n",
    "+ if the number of iterations is too high (`maxiter`)\n",
    "+ if the length of the gradient is low enough (<`eps`) to call an extremum\n",
    "\n",
    "Please, keep the signature of the function and enter the code only under `your code goes here`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task5\n",
    "#Gradient descent and prediction\n",
    "\n",
    "\n",
    "import math\n",
    "def calculate_gradient_length(nabla):\n",
    "    length = 0\n",
    "    for i in range(11):\n",
    "        length = length + (nabla[i])**2\n",
    "    length = math.sqrt(length)\n",
    "    return length\n",
    "\n",
    "\n",
    "def gradDescent(w_init, alpha, X, y, maxiter=500, eps=1e-2):\n",
    "        losses = []\n",
    "        weights = [w_init]\n",
    "        curiter = 0\n",
    "        w_k = weights[-1]\n",
    "        print ()\n",
    "        #your code goes here\\n\",\n",
    "        lossValue_k = 10000000000\n",
    "        grad_length = 10000000000\n",
    "        w_k_old = w_init\n",
    "        #vec{w}^{k+1}=\\\\vec{w}^{k}-\\\\alpha_k\\\\cdot \\\\nabla Loss(\\\\vec{w}^{k})\n",
    "        while curiter < maxiter and lossValue_k > eps :\n",
    "            print(\"Iteration no. : \",curiter)\n",
    "            curr_grad = grad(w_k_old,X,y)\n",
    "            w_k = w_k_old - alpha*curr_grad\n",
    "#             print(\":\",curr_grad)\n",
    "#             print(\"Old weights:\",w_k_old)\n",
    "#             print(\"New weights:\",w_k)\n",
    "            grad_length = calculate_gradient_length(curr_grad)\n",
    "            print(\"Gradient length:\",grad_length)\n",
    "            lossValue_k = loss(w_k,X,y)\n",
    "            print(\"Loss:\",lossValue_k)\n",
    "            print(\"\\n\\n\")\n",
    "            weights.append(w_k)\n",
    "            losses.append(lossValue_k)\n",
    "            curiter = curiter + 1\n",
    "            w_k_old = w_k\n",
    "            print()\n",
    "        return weights, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with several alphas and several intial values of weights. To illustrate, provide graphs for the Loss function over iterations in each case (and, optionally, the distance between weigths from one iteration to the next):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient: [ 8.21066810e+03  2.97047809e+04  1.96142486e+04  2.05721892e+07\n",
      "  1.30079381e+04  2.78528771e+04  6.62440799e+04  1.74581321e+07\n",
      "  3.11405708e+06  3.90531292e+05 -1.00332388e+06]\n",
      "Old weights: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "New weights: [ 0.99917893  0.99702952  0.99803858 -1.05721892  0.99869921  0.99721471\n",
      "  0.99337559 -0.74581321  0.68859429  0.96094687  1.10033239]\n",
      "Loss: 14008624.051782243\n",
      "\n",
      "Gradient: [-6.85312294e+03 -2.45593331e+04 -1.62290378e+04 -1.68817882e+07\n",
      " -1.10697065e+04 -2.30858531e+04 -5.52486424e+04 -1.48019705e+07\n",
      " -2.07981774e+06 -3.25942478e+05  8.37400809e+05]\n",
      "Old weights: [ 0.99917893  0.99702952  0.99803858 -1.05721892  0.99869921  0.99721471\n",
      "  0.99337559 -0.74581321  0.68859429  0.96094687  1.10033239]\n",
      "New weights: [0.99986425 0.99948546 0.99966148 0.6309599  0.99980618 0.9995233\n",
      " 0.99890046 0.73438383 0.89657607 0.99354112 1.01659231]\n",
      "Loss: 9674576.431098636\n",
      "\n",
      "Gradient: [ 5.65173007e+03  2.04775589e+04  1.35217759e+04  1.42001884e+07\n",
      "  8.93135969e+03  1.91903058e+04  4.56100289e+04  1.19965339e+07\n",
      "  2.20365444e+06  2.68820416e+05 -6.90631434e+05]\n",
      "Old weights: [0.99986425 0.99948546 0.99966148 0.6309599  0.99980618 0.9995233\n",
      " 0.99890046 0.73438383 0.89657607 0.99354112 1.01659231]\n",
      "New weights: [ 0.99929907  0.9974377   0.9983093  -0.78905894  0.99891304  0.99760427\n",
      "  0.99433945 -0.46526956  0.67621062  0.96665908  1.08565545]\n",
      "Loss: 6685501.32935997\n",
      "\n",
      "Gradient: [-4.72989948e+03 -1.69211092e+04 -1.11810373e+04 -1.16133518e+07\n",
      " -7.66087293e+03 -1.59166071e+04 -3.81195617e+04 -1.02343039e+07\n",
      " -1.37904785e+06 -2.24957477e+05  5.77955308e+05]\n",
      "Old weights: [ 0.99929907  0.9974377   0.9983093  -0.78905894  0.99891304  0.99760427\n",
      "  0.99433945 -0.46526956  0.67621062  0.96665908  1.08565545]\n",
      "New weights: [0.99977206 0.99912981 0.99942741 0.37227624 0.99967913 0.99919593\n",
      " 0.99815141 0.55816083 0.81411541 0.98915482 1.02785992]\n",
      "Loss: 4623581.233352227\n",
      "\n",
      "Gradient: [ 3.88805363e+03  1.41159165e+04  9.32189699e+03  9.80634125e+06\n",
      "  6.12469324e+03  1.32177843e+04  3.13895380e+04  8.23637459e+06\n",
      "  1.56996666e+06  1.84934377e+05 -4.75116427e+05]\n",
      "Old weights: [0.99977206 0.99912981 0.99942741 0.37227624 0.99967913 0.99919593\n",
      " 0.99815141 0.55816083 0.81411541 0.98915482 1.02785992]\n",
      "New weights: [ 0.99938326  0.99771822  0.99849522 -0.60835789  0.99906666  0.99787415\n",
      "  0.99501246 -0.26547663  0.65711874  0.97066139  1.07537156]\n",
      "Loss: 3200841.7377423076\n",
      "\n",
      "Gradient: [-3.26674687e+03 -1.16593928e+04 -7.70313488e+03 -7.98478182e+06\n",
      " -5.30894831e+03 -1.09781176e+04 -2.63146976e+04 -7.08277431e+06\n",
      " -9.02007511e+05 -1.55367053e+05  3.99167069e+05]\n",
      "Old weights: [ 0.99938326  0.99771822  0.99849522 -0.60835789  0.99906666  0.99787415\n",
      "  0.99501246 -0.26547663  0.65711874  0.97066139  1.07537156]\n",
      "New weights: [0.99970993 0.99888416 0.99926553 0.1901203  0.99959755 0.99897196\n",
      " 0.99764393 0.4428008  0.74731949 0.98619809 1.03545486]\n",
      "Loss: 2218795.6582802474\n",
      "\n",
      "Gradient: [ 2.67246488e+03  9.72942846e+03  6.42646499e+03  6.77605007e+06\n",
      "  4.19291223e+03  9.09945004e+03  2.15891136e+04  5.64838487e+06\n",
      "  1.12766519e+06  1.27116925e+05 -3.26575358e+05]\n",
      "Old weights: [0.99970993 0.99888416 0.99926553 0.1901203  0.99959755 0.99897196\n",
      " 0.99764393 0.4428008  0.74731949 0.98619809 1.03545486]\n",
      "New weights: [ 0.99944269  0.99791122  0.99862288 -0.48748471  0.99917826  0.99806202\n",
      "  0.99548501 -0.12203768  0.63455297  0.9734864   1.06811239]\n",
      "Loss: 1540630.5201629985\n",
      "\n",
      "Gradient: [-2.25848392e+03 -8.03524516e+03 -5.30718781e+03 -5.48611434e+06\n",
      " -3.68570455e+03 -7.57674549e+03 -1.81791321e+04 -4.90760228e+06\n",
      " -5.78512062e+05 -1.07412267e+05  2.75963955e+05]\n",
      "Old weights: [ 0.99944269  0.99791122  0.99862288 -0.48748471  0.99917826  0.99806202\n",
      "  0.99548501 -0.12203768  0.63455297  0.9734864   1.06811239]\n",
      "New weights: [0.99966853 0.99871474 0.9991536  0.06112672 0.99954683 0.99881969\n",
      " 0.99730293 0.36872254 0.69240418 0.98422763 1.040516  ]\n",
      "Loss: 1072040.6412196609\n",
      "\n",
      "Gradient: [ 1.83461654e+03  6.70440740e+03  4.43013142e+03  4.68572155e+06\n",
      "  2.86380793e+03  6.25921888e+03  1.48348914e+04  3.86785189e+06\n",
      "  8.17869666e+05  8.72658888e+04 -2.24192576e+05]\n",
      "Old weights: [0.99966853 0.99871474 0.9991536  0.06112672 0.99954683 0.99881969\n",
      " 0.99730293 0.36872254 0.69240418 0.98422763 1.040516  ]\n",
      "New weights: [ 0.99948507  0.9980443   0.99871059 -0.40744543  0.99926045  0.99819377\n",
      "  0.99581944 -0.01806264  0.61061721  0.97550104  1.06293525]\n",
      "Loss: 748016.8702358542\n",
      "\n",
      "Gradient: [-1.56370668e+03 -5.53943697e+03 -3.65679357e+03 -3.76592287e+06\n",
      " -2.56490428e+03 -5.23448143e+03 -1.25723410e+04 -3.40566073e+06\n",
      " -3.60262145e+05 -7.43675885e+04  1.91067103e+05]\n",
      "Old weights: [ 0.99948507  0.9980443   0.99871059 -0.40744543  0.99926045  0.99819377\n",
      "  0.99581944 -0.01806264  0.61061721  0.97550104  1.06293525]\n",
      "New weights: [ 0.99964144  0.99859824  0.99907627 -0.03085315  0.99951694  0.99871722\n",
      "  0.99707667  0.32250343  0.64664343  0.9829378   1.04382854]\n",
      "Loss: 523742.2095876767\n",
      "\n",
      "Gradient: [ 1.25710840e+03  4.61788986e+03  3.05352905e+03  3.24340075e+06\n",
      "  1.94981084e+03  4.30004195e+03  1.01800396e+04  2.64346561e+06\n",
      "  5.99935135e+05  5.97973963e+04 -1.53622310e+05]\n",
      "Old weights: [ 0.99964144  0.99859824  0.99907627 -0.03085315  0.99951694  0.99871722\n",
      "  0.99707667  0.32250343  0.64664343  0.9829378   1.04382854]\n",
      "New weights: [ 0.99951573  0.99813645  0.99877091 -0.35519322  0.99932196  0.99828721\n",
      "  0.99605867  0.05815687  0.58664991  0.97695806  1.05919077]\n",
      "Loss: 368317.12794512574\n",
      "\n",
      "Gradient: [-1.08496463e+03 -3.82104605e+03 -2.52012358e+03 -2.58203787e+06\n",
      " -1.79060283e+03 -3.62191942e+03 -8.70830607e+03 -2.36800430e+06\n",
      " -2.14033566e+05 -5.15980269e+04  1.32568545e+05]\n",
      "Old weights: [ 0.99951573  0.99813645  0.99877091 -0.35519322  0.99932196  0.99828721\n",
      "  0.99605867  0.05815687  0.58664991  0.97695806  1.05919077]\n",
      "New weights: [ 0.99962423  0.99851856  0.99902293 -0.09698943  0.99950102  0.9986494\n",
      "  0.9969295   0.2949573   0.60805327  0.98211786  1.04593392]\n",
      "Loss: 260435.041248055\n",
      "\n",
      "Gradient: [ 8.59030057e+02  3.17835704e+03  2.10411493e+03  2.24785713e+06\n",
      "  1.32167932e+03  2.94825979e+03  6.97202594e+03  1.80206589e+06\n",
      "  4.45791238e+05  4.08631295e+04 -1.04977677e+05]\n",
      "Old weights: [ 0.99962423  0.99851856  0.99902293 -0.09698943  0.99950102  0.9986494\n",
      "  0.9969295   0.2949573   0.60805327  0.98211786  1.04593392]\n",
      "New weights: [ 0.99953833  0.99820072  0.99881252 -0.32177515  0.99936885  0.99835458\n",
      "  0.9962323   0.11475071  0.56347415  0.97803155  1.05643169]\n",
      "Loss: 185401.81656917222\n",
      "\n",
      "Gradient: [-7.55098627e+02 -2.63824834e+03 -1.73741762e+03 -1.76758413e+06\n",
      " -1.25530244e+03 -2.51206411e+03 -6.04532532e+03 -1.65059939e+06\n",
      " -1.16984735e+05 -3.59093270e+04  9.22617742e+04]\n",
      "Old weights: [ 0.99953833  0.99820072  0.99881252 -0.32177515  0.99936885  0.99835458\n",
      "  0.9962323   0.11475071  0.56347415  0.97803155  1.05643169]\n",
      "New weights: [ 0.99961384  0.99846455  0.99898626 -0.14501673  0.99949438  0.99860579\n",
      "  0.99683683  0.27981065  0.57517262  0.98162248  1.04720551]\n",
      "Loss: 133081.55750457654\n",
      "\n",
      "Gradient: [ 5.84619113e+02  2.18488090e+03  1.44918419e+03  1.56038754e+06\n",
      "  8.90366899e+02  2.01525229e+03  4.76114152e+03  1.22434744e+06\n",
      "  3.36040099e+05  2.78108744e+04 -7.14447589e+04]\n",
      "Old weights: [ 0.99961384  0.99846455  0.99898626 -0.14501673  0.99949438  0.99860579\n",
      "  0.99683683  0.27981065  0.57517262  0.98162248  1.04720551]\n",
      "New weights: [ 0.99955537  0.99824606  0.99884134 -0.30105549  0.99940535  0.99840426\n",
      "  0.99636071  0.1573759   0.54156861  0.97884139  1.05434999]\n",
      "Loss: 96480.77301350622\n",
      "\n",
      "Gradient: [-5.27825439e+02 -1.82440495e+03 -1.19858201e+03 -1.20757493e+06\n",
      " -8.84896375e+02 -1.74849643e+03 -4.21007288e+03 -1.15415151e+06\n",
      " -5.34234153e+04 -2.51001449e+04  6.44912157e+04]\n",
      "Old weights: [ 0.99955537  0.99824606  0.99884134 -0.30105549  0.99940535  0.99840426\n",
      "  0.99636071  0.1573759   0.54156861  0.97884139  1.05434999]\n",
      "New weights: [ 0.99960816  0.9984285   0.9989612  -0.18029799  0.99949384  0.99857911\n",
      "  0.99678172  0.27279105  0.54691095  0.98135141  1.04790086]\n",
      "Loss: 70772.51637401825\n",
      "\n",
      "Gradient: [ 3.95444598e+02  1.49896845e+03  9.97271804e+02  1.08538327e+06\n",
      "  5.94526004e+02  1.37100847e+03  3.23745819e+03  8.28116859e+05\n",
      "  2.57266410e+05  1.88127466e+04 -4.83274577e+04]\n",
      "Old weights: [ 0.99960816  0.9984285   0.9989612  -0.18029799  0.99949384  0.99857911\n",
      "  0.99678172  0.27279105  0.54691095  0.98135141  1.04790086]\n",
      "New weights: [ 0.99956861  0.9982786   0.99886147 -0.28883632  0.99943438  0.99844201\n",
      "  0.99645798  0.18997937  0.52118431  0.97947013  1.05273361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 52623.75161077805\n",
      "\n",
      "Gradient: [-3.71248817e+02 -1.26469045e+03 -8.27750996e+02 -8.22781649e+05\n",
      " -6.28292518e+02 -1.22343440e+03 -2.94526315e+03 -8.10202528e+05\n",
      " -1.25791217e+04 -1.76534190e+04  4.53593160e+04]\n",
      "Old weights: [ 0.99956861  0.9982786   0.99886147 -0.28883632  0.99943438  0.99844201\n",
      "  0.99645798  0.18997937  0.52118431  0.97947013  1.05273361]\n",
      "New weights: [ 0.99960574  0.99840507  0.99894424 -0.20655816  0.99949721  0.99856435\n",
      "  0.9967525   0.27099962  0.52244222  0.98123547  1.04819768]\n",
      "Loss: 39731.70671211621\n",
      "\n",
      "Gradient: [ 2.65020020e+02  1.02515728e+03  6.85335088e+02  7.56937106e+05\n",
      "  3.91896123e+02  9.25911319e+02  2.18738102e+03  5.56754124e+05\n",
      "  2.00182982e+05  1.26089729e+04 -3.23892363e+04]\n",
      "Old weights: [ 0.99960574  0.99840507  0.99894424 -0.20655816  0.99949721  0.99856435\n",
      "  0.9967525   0.27099962  0.52244222  0.98123547  1.04819768]\n",
      "New weights: [ 0.99957923  0.99830256  0.99887571 -0.28225187  0.99945802  0.99847176\n",
      "  0.99653376  0.21532421  0.50242392  0.97997458  1.0514366 ]\n",
      "Loss: 30504.26431741482\n",
      "\n",
      "Gradient: [-2.63387552e+02 -8.79985935e+02 -5.72646896e+02 -5.58617321e+05\n",
      " -4.50261966e+02 -8.62613527e+02 -2.07358234e+03 -5.71548825e+05\n",
      "  1.29315041e+04 -1.25236718e+04  3.21800895e+04]\n",
      "Old weights: [ 0.99957923  0.99830256  0.99887571 -0.28225187  0.99945802  0.99847176\n",
      "  0.99653376  0.21532421  0.50242392  0.97997458  1.0514366 ]\n",
      "New weights: [ 0.99960557  0.99839055  0.99893298 -0.22639014  0.99950305  0.99855802\n",
      "  0.99674112  0.27247909  0.50113077  0.98122694  1.04821859]\n",
      "Loss: 23839.627511604194\n",
      "\n",
      "Gradient: [ 1.75090800e+02  6.97642377e+02  4.69920069e+02  5.29613116e+05\n",
      "  2.53369018e+02  6.18184080e+02  1.46370545e+03  3.71260954e+05\n",
      "  1.58352162e+05  8.33131192e+03 -2.13994594e+04]\n",
      "Old weights: [ 0.99960557  0.99839055  0.99893298 -0.22639014  0.99950305  0.99855802\n",
      "  0.99674112  0.27247909  0.50113077  0.98122694  1.04821859]\n",
      "New weights: [ 0.99958806  0.99832079  0.99888598 -0.27935145  0.99947771  0.9984962\n",
      "  0.99659475  0.235353    0.48529556  0.98039381  1.05035854]\n",
      "Loss: 18974.410788726185\n",
      "\n",
      "Gradient: [-1.89093524e+02 -6.15778803e+02 -3.97247578e+02 -3.77475680e+05\n",
      " -3.26510232e+02 -6.14866894e+02 -1.47283105e+03 -4.05637467e+05\n",
      "  2.81617877e+04 -8.99042463e+03  2.31025204e+04]\n",
      "Old weights: [ 0.99958806  0.99832079  0.99888598 -0.27935145  0.99947771  0.9984962\n",
      "  0.99659475  0.235353    0.48529556  0.98039381  1.05035854]\n",
      "New weights: [ 0.99960697  0.99838237  0.99892571 -0.24160388  0.99951036  0.99855769\n",
      "  0.99674204  0.27591674  0.48247938  0.98129285  1.04804829]\n",
      "Loss: 15378.894419660033\n",
      "\n",
      "Gradient: [ 1.13075663e+02  4.71059419e+02  3.21073523e+02  3.72085184e+05\n",
      "  1.58898846e+02  4.05238431e+02  9.64981473e+02  2.44781018e+05\n",
      "  1.27304166e+05  5.38136026e+03 -1.38207438e+04]\n",
      "Old weights: [ 0.99960697  0.99838237  0.99892571 -0.24160388  0.99951036  0.99855769\n",
      "  0.99674204  0.27591674  0.48247938  0.98129285  1.04804829]\n",
      "New weights: [ 0.99959567  0.99833526  0.9988936  -0.2788124   0.99949447  0.99851717\n",
      "  0.99664554  0.25143864  0.46974896  0.98075472  1.04943036]\n",
      "Loss: 12684.78846156011\n",
      "\n",
      "Gradient: [-1.37927714e+02 -4.34512912e+02 -2.76733112e+02 -2.53451636e+05\n",
      " -2.40280248e+02 -4.44944302e+02 -1.05879318e+03 -2.90014715e+05\n",
      "  3.65630782e+04 -6.55717556e+03  1.68510210e+04]\n",
      "Old weights: [ 0.99959567  0.99833526  0.9988936  -0.2788124   0.99949447  0.99851717\n",
      "  0.99664554  0.25143864  0.46974896  0.98075472  1.04943036]\n",
      "New weights: [ 0.99960946  0.99837871  0.99892127 -0.25346723  0.9995185   0.99856166\n",
      "  0.99675142  0.28044011  0.46609265  0.98141044  1.04774526]\n",
      "Loss: 10635.40678484878\n",
      "\n",
      "Gradient: [ 7.03032980e+01  3.14132706e+02  2.18147223e+02  2.62752717e+05\n",
      "  9.46837151e+01  2.57711659e+02  6.21291964e+02  1.58823698e+05\n",
      "  1.03929019e+05  3.34668315e+03 -8.59348647e+03]\n",
      "Old weights: [ 0.99960946  0.99837871  0.99892127 -0.25346723  0.9995185   0.99856166\n",
      "  0.99675142  0.28044011  0.46609265  0.98141044  1.04774526]\n",
      "New weights: [ 0.99960243  0.9983473   0.99889946 -0.27974251  0.99950903  0.99853589\n",
      "  0.99668929  0.26455774  0.45569975  0.98107577  1.04860461]\n",
      "Loss: 9051.290154630604\n",
      "\n",
      "Gradient: [-1.02696419e+02 -3.10316715e+02 -1.94003220e+02 -1.68702915e+05\n",
      " -1.80011049e+02 -3.28563546e+02 -7.73429685e+02 -2.09189004e+05\n",
      "  4.04860895e+04 -4.88177770e+03  1.25465649e+04]\n",
      "Old weights: [ 0.99960243  0.9983473   0.99889946 -0.27974251  0.99950903  0.99853589\n",
      "  0.99668929  0.26455774  0.45569975  0.98107577  1.04860461]\n",
      "New weights: [ 0.9996127   0.99837833  0.99891886 -0.26287221  0.99952704  0.99856875\n",
      "  0.99676663  0.28547664  0.45165114  0.98156395  1.04734995]\n",
      "Loss: 7806.48054743103\n",
      "\n",
      "Gradient: [ 4.07971103e+01  2.05297335e+02  1.46906277e+02  1.86718936e+05\n",
      "  5.12234651e+01  1.55357098e+02  3.84451472e+02  1.00661832e+05\n",
      "  8.60571044e+04  1.94301393e+03 -4.98736692e+03]\n",
      "Old weights: [ 0.9996127   0.99837833  0.99891886 -0.26287221  0.99952704  0.99856875\n",
      "  0.99676663  0.28547664  0.45165114  0.98156395  1.04734995]\n",
      "New weights: [ 0.99960862  0.9983578   0.99890417 -0.28154411  0.99952191  0.99855321\n",
      "  0.99672818  0.27541046  0.44304543  0.98136964  1.04784869]\n",
      "Loss: 6812.139720981481\n",
      "\n",
      "Gradient: [-7.84424585e+01 -2.25370159e+02 -1.37277266e+02 -1.10942738e+05\n",
      " -1.37724048e+02 -2.49000037e+02 -5.76741973e+02 -1.52468778e+05\n",
      "  4.15260402e+04 -3.72845847e+03  9.58341667e+03]\n",
      "Old weights: [ 0.99960862  0.9983578   0.99890417 -0.28154411  0.99952191  0.99855321\n",
      "  0.99672818  0.27541046  0.44304543  0.98136964  1.04784869]\n",
      "New weights: [ 0.99961646  0.99838034  0.9989179  -0.27044983  0.99953569  0.99857811\n",
      "  0.99678586  0.29065734  0.43889283  0.98174249  1.04689035]\n",
      "Loss: 6005.232549265174\n",
      "\n",
      "Gradient: [ 2.04376897e+01  1.29681970e+02  9.75361979e+01  1.33708272e+05\n",
      "  2.19812980e+01  8.42117773e+01  2.21251490e+02  6.15385312e+04\n",
      "  7.21697403e+04  9.74419035e+02 -2.49900479e+03]\n",
      "Old weights: [ 0.99961646  0.99838034  0.9989179  -0.27044983  0.99953569  0.99857811\n",
      "  0.99678586  0.29065734  0.43889283  0.98174249  1.04689035]\n",
      "New weights: [ 0.99961442  0.99836737  0.99890814 -0.28382066  0.99953349  0.99856969\n",
      "  0.99676373  0.28450348  0.43167585  0.98164505  1.04714025]\n",
      "Loss: 5340.703800909336\n",
      "\n",
      "Gradient: [-6.17499447e+01 -1.67400985e+02 -9.84402905e+01 -7.17119831e+04\n",
      " -1.07910735e+02 -1.94736605e+02 -4.41164435e+02 -1.12472168e+05\n",
      "  4.07601849e+04 -2.93475284e+03  7.54417877e+03]\n",
      "Old weights: [ 0.99961442  0.99836737  0.99890814 -0.28382066  0.99953349  0.99856969\n",
      "  0.99676373  0.28450348  0.43167585  0.98164505  1.04714025]\n",
      "New weights: [ 0.99962059  0.99838411  0.99891799 -0.27664946  0.99954428  0.99858916\n",
      "  0.99680785  0.2957507   0.42759984  0.98193852  1.04638583]\n",
      "Loss: 4786.065127281899\n",
      "\n",
      "Gradient: [ 6.38560386e+00  7.70293226e+01  6.32693820e+01  9.66309556e+04\n",
      "  2.46154735e+00  3.46443207e+01  1.08805262e+02  3.54315532e+04\n",
      "  6.11994024e+04  3.05844372e+02 -7.81427809e+02]\n",
      "Old weights: [ 0.99962059  0.99838411  0.99891799 -0.27664946  0.99954428  0.99858916\n",
      "  0.99680785  0.2957507   0.42759984  0.98193852  1.04638583]\n",
      "New weights: [ 0.99961995  0.99837641  0.99891166 -0.28631256  0.99954403  0.9985857\n",
      "  0.99679697  0.29220755  0.4214799   0.98190794  1.04646397]\n",
      "Loss: 4317.645439014665\n",
      "\n",
      "Gradient: [-5.02651578e+01 -1.27959367e+02 -7.19031878e+01 -4.51885064e+04\n",
      " -8.67659950e+01 -1.57843916e+02 -3.47699895e+02 -8.40997268e+04\n",
      "  3.89112204e+04 -2.38871487e+03  6.14124465e+03]\n",
      "Old weights: [ 0.99961995  0.99837641  0.99891166 -0.28631256  0.99954403  0.9985857\n",
      "  0.99679697  0.29220755  0.4214799   0.98190794  1.04646397]\n",
      "New weights: [ 0.99962498  0.9983892   0.99891885 -0.28179371  0.99955271  0.99860148\n",
      "  0.99683174  0.30061752  0.41758877  0.98214681  1.04584985]\n",
      "Loss: 3917.989727227521\n",
      "\n",
      "Gradient: [-3.31640435e+00  4.02628983e+01  3.94384612e+01  7.05936774e+04\n",
      " -1.04257743e+01  9.49197732e-03  3.13395463e+01  1.82019106e+04\n",
      "  5.23917668e+04 -1.55805250e+02  4.04537930e+02]\n",
      "Old weights: [ 0.99962498  0.9983892   0.99891885 -0.28179371  0.99955271  0.99860148\n",
      "  0.99683174  0.30061752  0.41758877  0.98214681  1.04584985]\n",
      "New weights: [ 0.99962531  0.99838518  0.99891491 -0.28885308  0.99955375  0.99860148\n",
      "  0.99682861  0.29879733  0.4123496   0.98216239  1.04580939]\n",
      "Loss: 3574.0512172767453\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [-4.23663501e+01 -1.01228937e+02 -5.38172248e+01 -2.73666683e+04\n",
      " -7.16596004e+01 -1.32864405e+02 -2.83256282e+02 -6.38263063e+04\n",
      "  3.64596381e+04 -2.01321277e+03  5.17645054e+03]\n",
      "Old weights: [ 0.99962531  0.99838518  0.99891491 -0.28885308  0.99955375  0.99860148\n",
      "  0.99682861  0.29879733  0.4123496   0.98216239  1.04580939]\n",
      "New weights: [ 0.99962955  0.9983953   0.99892029 -0.28611641  0.99956092  0.99861477\n",
      "  0.99685693  0.30517996  0.40870363  0.98236371  1.04529175]\n",
      "Loss: 3275.9319191875534\n",
      "\n",
      "Gradient: [-1.00175774e+01  1.44991282e+01  2.28238553e+01  5.22176520e+04\n",
      " -1.88028020e+01 -2.42791543e+01 -2.20164212e+01  7.00726631e+03\n",
      "  4.52103857e+04 -4.74705935e+02  1.22376914e+03]\n",
      "Old weights: [ 0.99962955  0.9983953   0.99892029 -0.28611641  0.99956092  0.99861477\n",
      "  0.99685693  0.30517996  0.40870363  0.98236371  1.04529175]\n",
      "New weights: [ 0.99963055  0.99839385  0.99891801 -0.29133817  0.9995628   0.9986172\n",
      "  0.99685913  0.30447923  0.40418259  0.98241118  1.04516937]\n",
      "Loss: 3016.0025915291517\n",
      "\n",
      "Gradient: [-3.69361888e+01 -8.32074721e+01 -4.15327451e+01 -1.54919718e+04\n",
      " -6.07718804e+01 -1.16043213e+02 -2.38811137e+02 -4.92128928e+04\n",
      "  3.37209210e+04 -1.75510575e+03  4.51326909e+03]\n",
      "Old weights: [ 0.99963055  0.99839385  0.99891801 -0.29133817  0.9995628   0.9986172\n",
      "  0.99685913  0.30447923  0.40418259  0.98241118  1.04516937]\n",
      "New weights: [ 0.99963425  0.99840217  0.99892216 -0.28978898  0.99956887  0.9986288\n",
      "  0.99688301  0.30940052  0.4008105   0.98258669  1.04471804]\n",
      "Loss: 2788.285499597464\n",
      "\n",
      "Gradient: [-1.46480916e+01 -3.63352665e+00  1.12040822e+01  3.91685674e+04\n",
      " -2.41254587e+01 -4.13884677e+01 -5.87547180e+01 -1.02204233e+02\n",
      "  3.92707716e+04 -6.95101629e+02  1.78993404e+03]\n",
      "Old weights: [ 0.99963425  0.99840217  0.99892216 -0.28978898  0.99956887  0.9986288\n",
      "  0.99688301  0.30940052  0.4008105   0.98258669  1.04471804]\n",
      "New weights: [ 0.99963571  0.99840254  0.99892104 -0.29370583  0.99957129  0.99863294\n",
      "  0.99688889  0.30941074  0.39688343  0.9826562   1.04453905]\n",
      "Loss: 2588.0194541859687\n",
      "\n",
      "Gradient: [-3.32049347e+01 -7.11423279e+01 -3.32261606e+01 -7.67151205e+03\n",
      " -5.28425394e+01 -1.04798246e+02 -2.08146644e+02 -3.85700497e+04\n",
      "  3.08985376e+04 -1.57778518e+03  4.05764665e+03]\n",
      "Old weights: [ 0.99963571  0.99840254  0.99892104 -0.29370583  0.99957129  0.99863294\n",
      "  0.99688889  0.30941074  0.39688343  0.9826562   1.04453905]\n",
      "New weights: [ 0.99963903  0.99840965  0.99892436 -0.29293868  0.99957657  0.99864342\n",
      "  0.9969097   0.31326775  0.39379357  0.98281398  1.04413329]\n",
      "Loss: 2411.351515676178\n",
      "\n",
      "Gradient: [-1.78492801e+01 -1.64639106e+01  3.04580956e+00  2.98325597e+04\n",
      " -2.73912493e+01 -5.35062972e+01 -8.40388950e+01 -4.46221346e+03\n",
      "  3.42947732e+04 -8.47497875e+02  2.18140499e+03]\n",
      "Old weights: [ 0.99963903  0.99840965  0.99892436 -0.29293868  0.99957657  0.99864342\n",
      "  0.9969097   0.31326775  0.39379357  0.98281398  1.04413329]\n",
      "New weights: [ 0.99964082  0.9984113   0.99892406 -0.29592194  0.99957931  0.99864877\n",
      "  0.99691811  0.31371397  0.39036409  0.98289873  1.04391515]\n",
      "Loss: 2255.116921397763\n",
      "\n",
      "Gradient: [-3.06423680e+01 -6.31414291e+01 -2.76429174e+01 -2.60558007e+03\n",
      " -4.69974657e+01 -9.73550370e+01 -1.86977846e+02 -3.07258582e+04\n",
      "  2.81202781e+04 -1.45603441e+03  3.74479718e+03]\n",
      "Old weights: [ 0.99964082  0.9984113   0.99892406 -0.29592194  0.99957931  0.99864877\n",
      "  0.99691811  0.31371397  0.39036409  0.98289873  1.04391515]\n",
      "New weights: [ 0.99964388  0.99841761  0.99892682 -0.29566138  0.99958401  0.99865851\n",
      "  0.99693681  0.31678655  0.38755207  0.98304434  1.04354067]\n",
      "Loss: 2116.6806418884216\n",
      "\n",
      "Gradient: [-2.00633821e+01 -2.56016971e+01 -2.70978508e+00  2.30928363e+04\n",
      " -2.92823897e+01 -6.21452093e+01 -1.01427722e+02 -6.98612102e+03\n",
      "  3.00789574e+04 -9.52930576e+02  2.45222557e+03]\n",
      "Old weights: [ 0.99964388  0.99841761  0.99892682 -0.29566138  0.99958401  0.99865851\n",
      "  0.99693681  0.31678655  0.38755207  0.98304434  1.04354067]\n",
      "New weights: [ 0.99964589  0.99842017  0.99892709 -0.29797066  0.99958694  0.99866472\n",
      "  0.99694695  0.31748517  0.38454417  0.98313963  1.04329544]\n",
      "Loss: 1993.8221478014536\n",
      "\n",
      "Gradient: [-2.88833111e+01 -5.79052398e+01 -2.39203836e+01  5.97243767e+02\n",
      " -4.26292998e+01 -9.24951605e+01 -1.72351839e+02 -2.48658871e+04\n",
      "  2.54631308e+04 -1.37248638e+03  3.53010152e+03]\n",
      "Old weights: [ 0.99964589  0.99842017  0.99892709 -0.29797066  0.99958694  0.99866472\n",
      "  0.99694695  0.31748517  0.38454417  0.98313963  1.04329544]\n",
      "New weights: [ 0.99964877  0.99842596  0.99892948 -0.29803039  0.9995912   0.99867397\n",
      "  0.99696418  0.31997175  0.38199786  0.98327688  1.04294243]\n",
      "Loss: 1884.6506137834126\n",
      "\n",
      "Gradient: [-2.15953995e+01 -3.21604145e+01 -6.79427704e+00  1.81756692e+04\n",
      " -3.02643767e+01 -6.83518655e+01 -1.13373973e+02 -8.29699366e+03\n",
      "  2.64726628e+04 -1.02590807e+03  2.63966918e+03]\n",
      "Old weights: [ 0.99964877  0.99842596  0.99892948 -0.29803039  0.9995912   0.99867397\n",
      "  0.99696418  0.31997175  0.38199786  0.98327688  1.04294243]\n",
      "New weights: [ 0.99965093  0.99842918  0.99893016 -0.29984795  0.99959423  0.9986808\n",
      "  0.99697552  0.32080145  0.37935059  0.98337947  1.04267847]\n",
      "Loss: 1787.5416844776796\n",
      "\n",
      "Gradient: [-2.76763032e+01 -5.45421044e+01 -2.14658145e+01  2.54746581e+03\n",
      " -3.93150589e+01 -8.93829143e+01 -1.62233719e+02 -2.04228135e+04\n",
      "  2.29702794e+04 -1.31518249e+03  3.38283576e+03]\n",
      "Old weights: [ 0.99965093  0.99842918  0.99893016 -0.29984795  0.99959423  0.9986808\n",
      "  0.99697552  0.32080145  0.37935059  0.98337947  1.04267847]\n",
      "New weights: [ 0.9996537   0.99843463  0.99893231 -0.3001027   0.99959816  0.99868974\n",
      "  0.99699174  0.32284373  0.37705356  0.98351099  1.04234018]\n",
      "Loss: 1701.0896225744912\n",
      "\n",
      "Gradient: [-2.26557298e+01 -3.69112060e+01 -9.71350867e+00  1.45442038e+04\n",
      " -3.06538586e+01 -7.28513764e+01 -1.21568187e+02 -8.81853233e+03\n",
      "  2.33627361e+04 -1.07643887e+03  2.76944858e+03]\n",
      "Old weights: [ 0.9996537   0.99843463  0.99893231 -0.3001027   0.99959816  0.99868974\n",
      "  0.99699174  0.32284373  0.37705356  0.98351099  1.04234018]\n",
      "New weights: [ 0.99965597  0.99843832  0.99893328 -0.30155712  0.99960122  0.99869703\n",
      "  0.9970039   0.32372559  0.37471729  0.98361863  1.04206324]\n",
      "Loss: 1624.0705229306673\n",
      "\n",
      "Gradient: [-2.68482310e+01 -5.24410376e+01 -1.98722735e+01  3.66241205e+03\n",
      " -3.67593077e+01 -8.74459219e+01 -1.55221222e+02 -1.70002729e+04\n",
      "  2.06626849e+04 -1.27589044e+03  3.28184933e+03]\n",
      "Old weights: [ 0.99965597  0.99843832  0.99893328 -0.30155712  0.99960122  0.99869703\n",
      "  0.9970039   0.32372559  0.37471729  0.98361863  1.04206324]\n",
      "New weights: [ 0.99965865  0.99844357  0.99893527 -0.30192336  0.9996049   0.99870577\n",
      "  0.99701942  0.32542562  0.37265102  0.98374622  1.04173505]\n",
      "Loss: 1555.4135676590174\n",
      "\n",
      "Gradient: [-2.33895500e+01 -4.03888455e+01 -1.18175725e+01  1.18252270e+04\n",
      " -3.06653540e+01 -7.61468367e+01 -1.27175568e+02 -8.83766094e+03\n",
      "  2.06628879e+04 -1.11142930e+03  2.85930658e+03]\n",
      "Old weights: [ 0.99965865  0.99844357  0.99893527 -0.30192336  0.9996049   0.99870577\n",
      "  0.99701942  0.32542562  0.37265102  0.98374622  1.04173505]\n",
      "New weights: [ 0.99966099  0.9984476   0.99893645 -0.30310589  0.99960797  0.99871339\n",
      "  0.99703214  0.32630938  0.37058473  0.98385736  1.04144912]\n",
      "Loss: 1494.1781922403272\n",
      "\n",
      "Gradient: [   -26.27995644    -51.18410083    -18.86070713   4226.64588455\n",
      "    -34.75494046    -86.29289567   -150.34807195 -14320.3059676\n",
      "  18546.95185215  -1248.94492803   3212.58682268]\n",
      "Old weights: [ 0.99966099  0.9984476   0.99893645 -0.30310589  0.99960797  0.99871339\n",
      "  0.99703214  0.32630938  0.37058473  0.98385736  1.04144912]\n",
      "New weights: [ 0.99966362  0.99845272  0.99893834 -0.30352855  0.99961144  0.99872202\n",
      "  0.99704718  0.32774141  0.36873004  0.98398226  1.04112786]\n",
      "Loss: 1439.5356545376465\n",
      "\n",
      "Gradient: [-2.38970695e+01 -4.29648465e+01 -1.33490612e+01  9.75865403e+03\n",
      " -3.04434002e+01 -7.85879668e+01 -1.30999261e+02 -8.54759326e+03\n",
      "  1.83062473e+04 -1.13564672e+03  2.92149084e+03]\n",
      "Old weights: [ 0.99966362  0.99845272  0.99893834 -0.30352855  0.99961144  0.99872202\n",
      "  0.99704718  0.32774141  0.36873004  0.98398226  1.04112786]\n",
      "New weights: [ 0.99966601  0.99845702  0.99893967 -0.30450442  0.99961449  0.99872988\n",
      "  0.99706028  0.32859617  0.36689941  0.98409582  1.04083571]\n",
      "Loss: 1390.7539315195388\n",
      "\n",
      "Gradient: [   -25.8895211     -50.48605744    -18.24004347   4433.56595413\n",
      "    -33.15610714    -85.65700349   -146.94844642 -12187.07760437\n",
      "  16620.6435585   -1230.44890334   3165.03571393]\n",
      "Old weights: [ 0.99966601  0.99845702  0.99893967 -0.30450442  0.99961449  0.99872988\n",
      "  0.99706028  0.32859617  0.36689941  0.98409582  1.04083571]\n",
      "New weights: [ 0.9996686   0.99846207  0.9989415  -0.30494777  0.9996178   0.99873844\n",
      "  0.99707497  0.32981488  0.36523735  0.98421887  1.04051921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1347.185170760676\n",
      "\n",
      "Gradient: [-2.42474904e+01 -4.48978846e+01 -1.44763381e+01  8.16267868e+03\n",
      " -3.00846644e+01 -8.04184592e+01 -1.33592879e+02 -8.07745337e+03\n",
      "  1.62401321e+04 -1.15238362e+03  2.96445979e+03]\n",
      "Old weights: [ 0.9996686   0.99846207  0.9989415  -0.30494777  0.9996178   0.99873844\n",
      "  0.99707497  0.32981488  0.36523735  0.98421887  1.04051921]\n",
      "New weights: [ 0.99967102  0.99846656  0.99894294 -0.30576404  0.99962081  0.99874648\n",
      "  0.99708833  0.33062262  0.36361334  0.9843341   1.04022277]\n",
      "Loss: 1308.2551355766198\n",
      "\n",
      "Gradient: [   -25.62057386    -50.15282067    -17.87970989   4414.03811874\n",
      "    -31.85951278    -85.35687632   -144.56357787 -10461.81556056\n",
      "  14875.85367931  -1217.72314011   3132.31231202]\n",
      "Old weights: [ 0.99967102  0.99846656  0.99894294 -0.30576404  0.99962081  0.99874648\n",
      "  0.99708833  0.33062262  0.36361334  0.9843341   1.04022277]\n",
      "New weights: [ 0.99967358  0.99847157  0.99894473 -0.30620544  0.999624    0.99875502\n",
      "  0.99710279  0.33166881  0.36212575  0.98445588  1.03990953]\n",
      "Loss: 1273.454231707219\n",
      "\n",
      "Gradient: [  -24.48862967   -46.36858361   -15.31648164  6909.72095533\n",
      "   -29.65314412   -81.80864351  -135.33805053 -7512.63555075\n",
      " 14422.35650608 -1163.91522208  2994.0584846 ]\n",
      "Old weights: [ 0.99967358  0.99847157  0.99894473 -0.30620544  0.999624    0.99875502\n",
      "  0.99710279  0.33166881  0.36212575  0.98445588  1.03990953]\n",
      "New weights: [ 0.99967603  0.99847621  0.99894626 -0.30689642  0.99962696  0.9987632\n",
      "  0.99711632  0.33242007  0.36068351  0.98457227  1.03961013]\n",
      "Loss: 1242.3298089494137\n",
      "\n",
      "Gradient: [  -25.43439697   -50.0528508    -17.69070741  4256.0957026\n",
      "   -30.7914929    -85.26976911  -142.87738406 -9045.48838233\n",
      " 13301.58408493 -1208.92699583  3109.68740602]\n",
      "Old weights: [ 0.99967603  0.99847621  0.99894626 -0.30689642  0.99962696  0.9987632\n",
      "  0.99711632  0.33242007  0.36068351  0.98457227  1.03961013]\n",
      "New weights: [ 0.99967858  0.99848122  0.99894803 -0.30732203  0.99963004  0.99877173\n",
      "  0.99713061  0.33332462  0.35935336  0.98469316  1.03929916]\n",
      "Loss: 1214.47950630084\n",
      "\n",
      "Gradient: [  -24.65355306   -47.50352023   -15.95111188  5909.82090078\n",
      "   -29.19060682   -82.87803     -136.49786641 -6908.78800391\n",
      " 12818.60890469 -1171.81504288  3014.32918572]\n",
      "Old weights: [ 0.99967858  0.99848122  0.99894803 -0.30732203  0.99963004  0.99877173\n",
      "  0.99713061  0.33332462  0.35935336  0.98469316  1.03929916]\n",
      "New weights: [ 0.99968104  0.99848597  0.99894963 -0.30791301  0.99963296  0.99878001\n",
      "  0.99714426  0.3340155   0.35807149  0.98481034  1.03899773]\n",
      "Loss: 1189.5454631650232\n",
      "\n",
      "Gradient: [  -25.30441233   -50.09747219   -17.61258094  4018.48769757\n",
      "   -29.89907307   -85.313094    -141.67210813 -7866.82234341\n",
      " 11885.31004098 -1202.79712515  3093.91498136]\n",
      "Old weights: [ 0.99968104  0.99848597  0.99894963 -0.30791301  0.99963296  0.99878001\n",
      "  0.99714426  0.3340155   0.35807149  0.98481034  1.03899773]\n",
      "New weights: [ 0.99968357  0.99849098  0.99895139 -0.30831486  0.99963595  0.99878855\n",
      "  0.99715843  0.33480218  0.35688296  0.98493062  1.03868834]\n",
      "Loss: 1167.209258332496\n",
      "\n",
      "Gradient: [  -24.76514777   -48.39179493   -16.4373107   5099.16685931\n",
      "   -28.72375444   -83.71087452  -137.25371659 -6301.40747532\n",
      " 11400.57433463 -1177.17239367  3028.07021501]\n",
      "Old weights: [ 0.99968357  0.99849098  0.99895139 -0.30831486  0.99963595  0.99878855\n",
      "  0.99715843  0.33480218  0.35688296  0.98493062  1.03868834]\n",
      "New weights: [ 0.99968605  0.99849581  0.99895303 -0.30882477  0.99963882  0.99879692\n",
      "  0.99717215  0.33543232  0.35574291  0.98504834  1.03838553]\n",
      "Loss: 1147.1874671825879\n",
      "\n",
      "Gradient: [  -25.21239669   -50.22733466   -17.60445122  3739.99027357\n",
      "   -29.14377824   -85.43172015  -140.7977479  -6874.00281389\n",
      " 10613.99308746 -1198.467477    3082.76976244]\n",
      "Old weights: [ 0.99968605  0.99849581  0.99895303 -0.30882477  0.99963882  0.99879692\n",
      "  0.99717215  0.33543232  0.35574291  0.98504834  1.03838553]\n",
      "New weights: [ 0.99968857  0.99850084  0.99895479 -0.30919877  0.99964174  0.99880546\n",
      "  0.99718623  0.33611972  0.35468151  0.98516819  1.03807725]\n",
      "Loss: 1129.2277489033472\n",
      "\n",
      "Gradient: [  -24.83927551   -49.0964774    -16.81515819  4432.16532912\n",
      "   -28.26913285   -84.36693043  -137.73067628 -5712.41362142\n",
      " 10144.57895054 -1180.74235357  3037.22122993]\n",
      "Old weights: [ 0.99968857  0.99850084  0.99895479 -0.30919877  0.99964174  0.99880546\n",
      "  0.99718623  0.33611972  0.35468151  0.98516819  1.03807725]\n",
      "New weights: [ 0.99969105  0.99850575  0.99895647 -0.30964199  0.99964456  0.9988139\n",
      "  0.9972      0.33669096  0.35366705  0.98528626  1.03777353]\n",
      "Loss: 1113.105391624792\n",
      "\n",
      "Gradient: [  -25.14587454   -50.40310642   -17.63884465  3445.80125328\n",
      "   -28.49733995   -85.58924483  -140.15098828 -6028.91922319\n",
      "  9474.72047646 -1195.34530273  3074.72865597]\n",
      "Old weights: [ 0.99969105  0.99850575  0.99895647 -0.30964199  0.99964456  0.9988139\n",
      "  0.9972      0.33669096  0.35366705  0.98528626  1.03777353]\n",
      "New weights: [ 0.99969357  0.99851079  0.99895824 -0.30998657  0.99964741  0.99882246\n",
      "  0.99721402  0.33729385  0.35271958  0.9854058   1.03746606]\n",
      "Loss: 1098.6202556874116\n",
      "\n",
      "Gradient: [  -24.88694655   -49.66251643   -17.11293665  3875.95423364\n",
      "   -27.83649071   -84.888881    -138.01500292 -5154.64606687\n",
      "  9030.60030051 -1183.04918362  3043.12891018]\n",
      "Old weights: [ 0.99969357  0.99851079  0.99895824 -0.30998657  0.99964741  0.99882246\n",
      "  0.99721402  0.33729385  0.35271958  0.9854058   1.03746606]\n",
      "New weights: [ 0.99969606  0.99851575  0.99895995 -0.31037416  0.9996502   0.99883094\n",
      "  0.99722782  0.33780932  0.35181652  0.9855241   1.03716174]\n",
      "Loss: 1085.5940648609096\n",
      "\n",
      "Gradient: [  -25.09632251   -50.59908085   -17.6974504   3151.92672234\n",
      "   -27.9387147    -85.76199896  -139.66068287 -5303.16690252\n",
      "  8455.09362486 -1193.02575887  3068.75135001]\n",
      "Old weights: [ 0.99969606  0.99851575  0.99895995 -0.31037416  0.9996502   0.99883094\n",
      "  0.99722782  0.33780932  0.35181652  0.9855241   1.03716174]\n",
      "New weights: [ 0.99969857  0.99852081  0.99896172 -0.31068936  0.99965299  0.99883952\n",
      "  0.99724179  0.33833964  0.35097101  0.9856434   1.03685487]\n",
      "Loss: 1073.8680029179852\n",
      "\n",
      "Gradient: [  -24.91581909   -50.12221093   -17.3507258   3406.60245514\n",
      "   -27.43107236   -85.30748165  -138.16619538 -4634.93350581\n",
      "  8041.53596095 -1184.45764865  3046.73019084]\n",
      "Old weights: [ 0.99969857  0.99852081  0.99896172 -0.31068936  0.99965299  0.99883952\n",
      "  0.99724179  0.33833964  0.35097101  0.9856434   1.03685487]\n",
      "New weights: [ 0.99970106  0.99852583  0.99896345 -0.31103002  0.99965573  0.99884805\n",
      "  0.9972556   0.33880313  0.35016685  0.98576185  1.0365502 ]\n",
      "Loss: 1063.300579070347\n",
      "\n",
      "Gradient: [  -25.05793279   -50.79878898   -17.76820443  2868.18608162\n",
      "   -27.45200825   -85.93493466  -139.27784903 -4675.26319943\n",
      "  7543.44928105 -1191.23310237  3064.12923318]\n",
      "Old weights: [ 0.99970106  0.99852583  0.99896345 -0.31103002  0.99965573  0.99884805\n",
      "  0.9972556   0.33880313  0.35016685  0.98576185  1.0365502 ]\n",
      "New weights: [ 0.99970356  0.99853091  0.99896523 -0.31131684  0.99965848  0.99885665\n",
      "  0.99726953  0.33927065  0.34941251  0.98588097  1.03624378]\n",
      "Loss: 1053.7657307573006\n",
      "\n",
      "Gradient: [  -24.93123355   -50.49899798   -17.54288829  3006.47358349\n",
      "   -27.05517836   -85.64512164  -138.2253055  -4156.18197763\n",
      "  7162.65556112 -1185.22221643  3048.67865941]\n",
      "Old weights: [ 0.99970356  0.99853091  0.99896523 -0.31131684  0.99965848  0.99885665\n",
      "  0.99726953  0.33927065  0.34941251  0.98588097  1.03624378]\n",
      "New weights: [ 0.99970606  0.99853596  0.99896699 -0.31161748  0.99966119  0.99886521\n",
      "  0.99728335  0.33968627  0.34869624  0.98599949  1.03593891]\n",
      "Loss: 1045.1511364033213\n",
      "\n",
      "Gradient: [  -25.02676186   -50.99199245   -17.84328745  2600.26693521\n",
      "   -27.02502654   -86.09880791  -138.96877341 -4128.70353486\n",
      "  6728.97047007 -1189.78020729  3060.3813848 ]\n",
      "Old weights: [ 0.99970606  0.99853596  0.99896699 -0.31161748  0.99966119  0.99886521\n",
      "  0.99728335  0.33968627  0.34869624  0.98599949  1.03593891]\n",
      "New weights: [ 0.99970856  0.99854105  0.99896877 -0.31187751  0.99966389  0.99887382\n",
      "  0.99729725  0.34009914  0.34802335  0.98611847  1.03563288]\n",
      "Loss: 1037.35671422212\n",
      "\n",
      "Gradient: [  -24.93692617   -50.81007912   -17.69978967  2662.39386971\n",
      "   -26.70922352   -85.91829454  -138.22066679 -3718.79010891\n",
      "  6381.18397862 -1185.52100534  3049.43177009]\n",
      "Old weights: [ 0.99970856  0.99854105  0.99896877 -0.31187751  0.99966389  0.99887382\n",
      "  0.99729725  0.34009914  0.34802335  0.98611847  1.03563288]\n",
      "New weights: [ 0.99971105  0.99854614  0.99897054 -0.31214375  0.99966656  0.99888241\n",
      "  0.99731107  0.34047102  0.34738523  0.98623702  1.03532793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1030.2932860794706\n",
      "\n",
      "Gradient: [  -25.0001446    -51.17262581   -17.91775161  2351.12690463\n",
      "   -26.64826066   -86.24825192  -138.71026099 -3650.59908766\n",
      "  6001.72599229 -1188.54070358  3057.18299293]\n",
      "Old weights: [ 0.99971105  0.99854614  0.99897054 -0.31214375  0.99966656  0.99888241\n",
      "  0.99731107  0.34047102  0.34738523  0.98623702  1.03532793]\n",
      "New weights: [ 0.99971355  0.99855125  0.99897233 -0.31237886  0.99966922  0.99889104\n",
      "  0.99732494  0.34083608  0.34678506  0.98635588  1.03502221]\n",
      "Loss: 1023.881387939966\n",
      "\n",
      "Gradient: [  -24.93552156   -51.06824428   -17.82898988  2364.37607221\n",
      "   -26.39245048   -86.13931499  -138.17184313 -3321.60312486\n",
      "  5685.97919707 -1185.4792168   3049.31104291]\n",
      "Old weights: [ 0.99971355  0.99855125  0.99897233 -0.31237886  0.99966922  0.99889104\n",
      "  0.99732494  0.34083608  0.34678506  0.98635588  1.03502221]\n",
      "New weights: [ 0.99971605  0.99855636  0.99897411 -0.3126153   0.99967186  0.99889965\n",
      "  0.99733876  0.34116824  0.34621646  0.98647443  1.03471728]\n",
      "Loss: 1018.0502105911778\n",
      "\n",
      "Gradient: [  -24.9762912    -51.33739162   -17.98857997  2121.94692535\n",
      "   -26.31417251   -86.38046269  -138.48636145 -3230.71778191\n",
      "  5352.66470726 -1187.4298102   3054.31611016]\n",
      "Old weights: [ 0.99971605  0.99855636  0.99897411 -0.3126153   0.99967186  0.99889965\n",
      "  0.99733876  0.34116824  0.34621646  0.98647443  1.03471728]\n",
      "New weights: [ 0.99971854  0.99856149  0.99897591 -0.31282749  0.99967449  0.99890829\n",
      "  0.99735261  0.34149131  0.34568119  0.98659317  1.03441185]\n",
      "Loss: 1012.7366562220374\n",
      "\n",
      "Gradient: [  -24.9288728    -51.28314125   -17.93607006  2104.7278854\n",
      "   -26.10340762   -86.31751447  -138.09235049 -2962.55131568\n",
      "  5067.27920108 -1185.18531682  3048.54363459]\n",
      "Old weights: [ 0.99971854  0.99856149  0.99897591 -0.31282749  0.99967449  0.99890829\n",
      "  0.99735261  0.34149131  0.34568119  0.98659317  1.03441185]\n",
      "New weights: [ 0.99972104  0.99856662  0.99897771 -0.31303797  0.9996771   0.99891692\n",
      "  0.99736642  0.34178757  0.34517446  0.98671169  1.034107  ]\n",
      "Loss: 1007.8844980772603\n",
      "\n",
      "Gradient: [  -24.95400995   -51.48480399   -18.05404356  1912.77682993\n",
      "   -26.01668943   -86.49430498  -138.28611349 -2860.80541254\n",
      "  4773.58224248 -1186.39115669  3051.63579433]\n",
      "Old weights: [ 0.99972104  0.99856662  0.99897771 -0.31303797  0.9996771   0.99891692\n",
      "  0.99736642  0.34178757  0.34517446  0.98671169  1.034107  ]\n",
      "New weights: [ 0.99972353  0.99857177  0.99897951 -0.31322924  0.99967971  0.99892557\n",
      "  0.99738025  0.34207365  0.34469711  0.98683033  1.03380183]\n",
      "Loss: 1003.4436318520595\n",
      "\n",
      "Gradient: [  -24.91829646   -51.46216156   -18.02520715  1877.42672734\n",
      "   -25.8402661    -86.46007653  -137.99153322 -2639.07310758\n",
      "  4516.49983492 -1184.70221686  3047.29106219]\n",
      "Old weights: [ 0.99972353  0.99857177  0.99897951 -0.31322924  0.99967971  0.99892557\n",
      "  0.99738025  0.34207365  0.34469711  0.98683033  1.03380183]\n",
      "New weights: [ 0.99972603  0.99857692  0.99898132 -0.31341699  0.99968229  0.99893422\n",
      "  0.99739405  0.34233756  0.34424546  0.9869488   1.0334971 ]\n",
      "Loss: 999.3694087608094\n",
      "\n",
      "Gradient: [  -24.93251682   -51.61453988   -18.1132629   1722.97010473\n",
      "   -25.75084425   -86.58970688  -138.10199071 -2534.10178893\n",
      "  4257.07189366 -1185.38772823  3049.04684362]\n",
      "Old weights: [ 0.99972603  0.99857692  0.99898132 -0.31341699  0.99968229  0.99893422\n",
      "  0.99739405  0.34233756  0.34424546  0.9869488   1.0334971 ]\n",
      "New weights: [ 0.99972852  0.99858208  0.99898313 -0.31358928  0.99968487  0.99894287\n",
      "  0.99740786  0.34259097  0.34381975  0.98706734  1.0331922 ]\n",
      "Loss: 995.6220413340343\n",
      "\n",
      "Gradient: [  -24.90473505   -51.611061     -18.0995744   1677.67926708\n",
      "   -25.60102726   -86.57262256  -137.87585748 -2348.39153365\n",
      "  4026.07080073 -1184.07500534  3045.66906486]\n",
      "Old weights: [ 0.99972852  0.99858208  0.99898313 -0.31358928  0.99968487  0.99894287\n",
      "  0.99740786  0.34259097  0.34381975  0.98706734  1.0331922 ]\n",
      "New weights: [ 0.99973101  0.99858724  0.99898494 -0.31375705  0.99968743  0.99895153\n",
      "  0.99742164  0.34282581  0.34341714  0.98718574  1.03288763]\n",
      "Loss: 992.1660739882926\n",
      "\n",
      "Gradient: [  -24.91130467   -51.72700129   -18.16590998  1551.47445702\n",
      "   -25.51251683   -86.6672519   -137.92883089 -2244.99302089\n",
      "  3796.4674779  -1184.39564963  3046.48782535]\n",
      "Old weights: [ 0.99973101  0.99858724  0.99898494 -0.31375705  0.99968743  0.99895153\n",
      "  0.99742164  0.34282581  0.34341714  0.98718574  1.03288763]\n",
      "New weights: [ 0.9997335   0.99859241  0.99898675 -0.3139122   0.99968998  0.9989602\n",
      "  0.99743544  0.34305031  0.34303749  0.98730418  1.03258298]\n",
      "Loss: 988.9699112917056\n",
      "\n",
      "Gradient: [  -24.88886943   -51.73439596   -18.16162127  1501.60931395\n",
      "   -25.3836558    -86.6596243   -137.74980295 -2087.69131544\n",
      "  3589.30062938 -1183.33629886  3043.76135077]\n",
      "Old weights: [ 0.9997335   0.99859241  0.99898675 -0.3139122   0.99968998  0.9989602\n",
      "  0.99743544  0.34305031  0.34303749  0.98730418  1.03258298]\n",
      "New weights: [ 0.99973599  0.99859759  0.99898857 -0.31406236  0.99969252  0.99896886\n",
      "  0.99744921  0.34325908  0.34267856  0.98742252  1.03227861]\n",
      "Loss: 986.0053976259331\n",
      "\n",
      "Gradient: [  -24.89005355   -51.82302194   -18.21200656  1397.02399625\n",
      "   -25.29824664   -86.72790633  -137.76309848 -1988.75918219\n",
      "  3385.78317845 -1183.39992325  3043.92012467]\n",
      "Old weights: [ 0.99973599  0.99859759  0.99898857 -0.31406236  0.99969252  0.99896886\n",
      "  0.99744921  0.34325908  0.34267856  0.98742252  1.03227861]\n",
      "New weights: [ 0.99973848  0.99860277  0.99899039 -0.31420206  0.99969504  0.99897754\n",
      "  0.99746299  0.34345795  0.34233999  0.98754086  1.03197422]\n",
      "Loss: 983.2474426371377\n",
      "\n",
      "Gradient: [  -24.87119677   -51.83583209   -18.21326979  1346.03510122\n",
      "   -25.18616278   -86.72469611  -137.61647777 -1854.22891302\n",
      "  3200.26401424 -1182.50995082  3041.62912375]\n",
      "Old weights: [ 0.99973848  0.99860277  0.99899039 -0.31420206  0.99969504  0.99897754\n",
      "  0.99746299  0.34345795  0.34233999  0.98754086  1.03197422]\n",
      "New weights: [ 0.99974096  0.99860795  0.99899221 -0.31433667  0.99969756  0.99898621\n",
      "  0.99747675  0.34364337  0.34201996  0.98765911  1.03167005]\n",
      "Loss: 980.6736874823215\n",
      "\n",
      "Gradient: [  -24.86856933   -51.90367244   -18.2517885   1258.26448909\n",
      "   -25.10509532   -86.77283887  -137.60237652 -1761.38908849\n",
      "  3019.65357758 -1182.39151093  3041.32044671]\n",
      "Old weights: [ 0.99974096  0.99860795  0.99899221 -0.31433667  0.99969756  0.99898621\n",
      "  0.99747675  0.34364337  0.34201996  0.98765911  1.03167005]\n",
      "New weights: [ 0.99974345  0.99861314  0.99899404 -0.31446249  0.99970007  0.99899489\n",
      "  0.99749051  0.34381951  0.34171799  0.98777735  1.03136592]\n",
      "Loss: 978.2642074239817\n",
      "\n",
      "Gradient: [  -24.85208459   -51.91836389   -18.25605307  1208.30900737\n",
      "   -25.00665505   -86.77080349  -137.47804257 -1645.39768398\n",
      "  2853.70669135 -1181.61362548  3039.31769547]\n",
      "Old weights: [ 0.99974345  0.99861314  0.99899404 -0.31446249  0.99970007  0.99899489\n",
      "  0.99749051  0.34381951  0.34171799  0.98777735  1.03136592]\n",
      "New weights: [ 0.99974594  0.99861833  0.99899586 -0.31458332  0.99970257  0.99900356\n",
      "  0.99750426  0.34398405  0.34143262  0.98789551  1.03106199]\n",
      "Loss: 976.0012468119127\n",
      "\n",
      "Gradient: [  -24.84674171   -51.97013267   -18.28561514  1133.83327341\n",
      "   -24.93054467   -86.80330294  -137.44501686 -1559.44256087\n",
      "  2693.27583428 -1181.36534033  3038.67569344]\n",
      "Old weights: [ 0.99974594  0.99861833  0.99899586 -0.31458332  0.99970257  0.99900356\n",
      "  0.99750426  0.34398405  0.34143262  0.98789551  1.03106199]\n",
      "New weights: [ 0.99974842  0.99862353  0.99899769 -0.31469671  0.99970507  0.99901224\n",
      "  0.997518    0.34414     0.3411633   0.98801364  1.03075812]\n",
      "Loss: 973.868982923294\n",
      "\n",
      "Gradient: [  -24.83180833   -51.98447239   -18.29121346  1086.20104678\n",
      "   -24.84336233   -86.80041309  -137.33600293 -1458.76326778\n",
      "  2544.96431456 -1180.6605883   3036.86108403]\n",
      "Old weights: [ 0.99974842  0.99862353  0.99899769 -0.31469671  0.99970507  0.99901224\n",
      "  0.997518    0.34414     0.3411633   0.98801364  1.03075812]\n",
      "New weights: [ 0.9997509   0.99862873  0.99899952 -0.31480533  0.99970755  0.99902092\n",
      "  0.99753174  0.34428587  0.3409088   0.98813171  1.03045444]\n",
      "Loss: 971.8533155176066\n",
      "\n",
      "Gradient: [  -24.82451561   -52.02360968   -18.31390951  1022.40862225\n",
      "   -24.77241973   -86.82056159  -137.28989924 -1379.94651346\n",
      "  2402.35513571 -1180.31894626  3035.9794722 ]\n",
      "Old weights: [ 0.9997509   0.99862873  0.99899952 -0.31480533  0.99970755  0.99902092\n",
      "  0.99753174  0.34428587  0.3409088   0.98813171  1.03045444]\n",
      "New weights: [ 0.99975339  0.99863393  0.99900135 -0.31490757  0.99971003  0.99902961\n",
      "  0.99754546  0.34442387  0.34066856  0.98824974  1.03015084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 969.9416793053167\n",
      "\n",
      "Gradient: [  -24.81057759   -52.0362393    -18.3197727    977.81318363\n",
      "   -24.69464971   -86.81560173  -137.1914112  -1292.07948708\n",
      "  2269.89267071 -1179.66095425  3034.28522042]\n",
      "Old weights: [ 0.99975339  0.99863393  0.99900135 -0.31490757  0.99971003  0.99902961\n",
      "  0.99754546  0.34442387  0.34066856  0.98824974  1.03015084]\n",
      "New weights: [ 0.99975587  0.99863914  0.99900318 -0.31500535  0.9997125   0.99903829\n",
      "  0.99755918  0.34455308  0.34044157  0.98836771  1.02984741]\n",
      "Loss: 968.1228768349661\n",
      "\n",
      "Gradient: [  -24.80187169   -52.06528638   -18.33711942   922.73867515\n",
      "   -24.62882993   -86.82584103  -137.13626552 -1220.3153301\n",
      "  2143.05400525 -1179.25154808  3033.2297246 ]\n",
      "Old weights: [ 0.99975587  0.99863914  0.99900318 -0.31500535  0.9997125   0.99903829\n",
      "  0.99755918  0.34455308  0.34044157  0.98836771  1.02984741]\n",
      "New weights: [ 0.99975835  0.99864434  0.99900502 -0.31509762  0.99971496  0.99904697\n",
      "  0.9975729   0.34467511  0.34022727  0.98848563  1.02954409]\n",
      "Loss: 966.3869295750122\n",
      "\n",
      "Gradient: [  -24.78855441   -52.07543065   -18.34258249   881.51547543\n",
      "   -24.55902076   -86.81813651  -137.04500574 -1143.29174529\n",
      "  2024.80722072 -1178.62256118  3031.61019132]\n",
      "Old weights: [ 0.99975835  0.99864434  0.99900502 -0.31509762  0.99971496  0.99904697\n",
      "  0.9975729   0.34467511  0.34022727  0.98848563  1.02954409]\n",
      "New weights: [ 0.99976083  0.99864955  0.99900685 -0.31518578  0.99971742  0.99905565\n",
      "  0.9975866   0.34478944  0.34002479  0.9886035   1.02924093]\n",
      "Loss: 964.7249452092908\n",
      "\n",
      "Gradient: [  -24.77881326   -52.09629078   -18.35569277   833.65684193\n",
      "   -24.49812331   -86.82030336  -136.9836056  -1078.28885242\n",
      "  1911.94569434 -1178.16342603  3030.42712365]\n",
      "Old weights: [ 0.99976083  0.99864955  0.99900685 -0.31518578  0.99971742  0.99905565\n",
      "  0.9975866   0.34478944  0.34002479  0.9886035   1.02924093]\n",
      "New weights: [ 0.99976331  0.99865476  0.99900869 -0.31526914  0.99971987  0.99906433\n",
      "  0.9976003   0.34489727  0.33983359  0.98872131  1.02893788]\n",
      "Loss: 963.1289993804186\n",
      "\n",
      "Gradient: [  -24.76586616   -52.1035588    -18.36036126   795.89778378\n",
      "   -24.4351147    -86.80953456  -136.89730713 -1010.5326303\n",
      "  1806.43041408 -1177.55158309  3028.85181431]\n",
      "Old weights: [ 0.99976331  0.99865476  0.99900869 -0.31526914  0.99971987  0.99906433\n",
      "  0.9976003   0.34489727  0.33983359  0.98872131  1.02893788]\n",
      "New weights: [ 0.99976578  0.99865997  0.99901052 -0.31534873  0.99972231  0.99907302\n",
      "  0.99761399  0.34499832  0.33965295  0.98883907  1.028635  ]\n",
      "Loss: 961.5920303082382\n",
      "\n",
      "Gradient: [  -24.7553574    -52.11767904   -18.37006233   754.08837729\n",
      "   -24.37885041   -86.80503196  -136.83157895  -951.88328232\n",
      "  1705.97165961 -1177.05550239  3027.57399689]\n",
      "Old weights: [ 0.99976578  0.99865997  0.99901052 -0.31534873  0.99972231  0.99907302\n",
      "  0.99761399  0.34499832  0.33965295  0.98883907  1.028635  ]\n",
      "New weights: [ 0.99976826  0.99866518  0.99901236 -0.31542414  0.99972475  0.9990817\n",
      "  0.99762767  0.34509351  0.33948235  0.98895677  1.02833224]\n",
      "Loss: 960.1077448813301\n",
      "\n",
      "Gradient: [  -24.74261457   -52.12192921   -18.37372129   719.73267983\n",
      "   -24.32170013   -86.79110811  -136.74868451  -892.11288024\n",
      "  1611.84556007 -1176.45296323  3026.02274972]\n",
      "Old weights: [ 0.99976826  0.99866518  0.99901236 -0.31542414  0.99972475  0.9990817\n",
      "  0.99762767  0.34509351  0.33948235  0.98895677  1.02833224]\n",
      "New weights: [ 0.99977073  0.99867039  0.9990142  -0.31549611  0.99972718  0.99909037\n",
      "  0.99764135  0.34518272  0.33932117  0.98907442  1.02802964]\n",
      "Loss: 958.6705349724252\n",
      "\n",
      "Gradient: [  -24.73152909   -52.13042744   -18.38063696   683.05131065\n",
      "   -24.26973511   -86.78102522  -136.67996072  -839.3516772\n",
      "  1522.40298785 -1175.92906245  3024.67360907]\n",
      "Old weights: [ 0.99977073  0.99867039  0.9990142  -0.31549611  0.99972718  0.99909037\n",
      "  0.99764135  0.34518272  0.33932117  0.98907442  1.02802964]\n",
      "New weights: [ 0.9997732   0.99867561  0.99901604 -0.31556442  0.99972961  0.99909905\n",
      "  0.99765502  0.34526665  0.33916893  0.98919201  1.02772717]\n",
      "Loss: 957.2754028647267\n",
      "\n",
      "Gradient: [  -24.71888225   -52.13167602   -18.38318893   651.9464801\n",
      "   -24.21766678   -86.76399915  -136.5994016   -786.50980671\n",
      "  1438.45628681 -1175.33072164  3023.13329034]\n",
      "Old weights: [ 0.9997732   0.99867561  0.99901604 -0.31556442  0.99972961  0.99909905\n",
      "  0.99765502  0.34526665  0.33916893  0.98919201  1.02772717]\n",
      "New weights: [ 0.99977568  0.99868082  0.99901787 -0.31562961  0.99973203  0.99910773\n",
      "  0.99766868  0.3453453   0.33902508  0.98930954  1.02742486]\n",
      "Loss: 955.9178947969273\n",
      "\n",
      "Gradient: [  -24.70735727   -52.13543014   -18.38779697   619.65387768\n",
      "   -24.16965099   -86.74919554  -136.52860471  -739.15167322\n",
      "  1358.80555091 -1174.78557091  3021.72968994]\n",
      "Old weights: [ 0.99977568  0.99868082  0.99901787 -0.31562961  0.99973203  0.99910773\n",
      "  0.99766868  0.3453453   0.33902508  0.98930954  1.02742486]\n",
      "New weights: [ 0.99977815  0.99868603  0.99901971 -0.31569158  0.99973445  0.9991164\n",
      "  0.99768233  0.34541922  0.3388892   0.98942702  1.02712268]\n",
      "Loss: 954.5940417434118\n",
      "\n",
      "Gradient: [  -24.69473719   -52.13378982   -18.38921996   591.59625567\n",
      "   -24.12201631   -86.72920648  -136.44964837  -692.35454824\n",
      "  1283.95080391 -1174.18817526  3020.19192648]\n",
      "Old weights: [ 0.99977815  0.99868603  0.99901971 -0.31569158  0.99973445  0.9991164\n",
      "  0.99768233  0.34541922  0.3388892   0.98942702  1.02712268]\n",
      "New weights: [ 0.99978062  0.99869125  0.99902155 -0.31575074  0.99973686  0.99912508\n",
      "  0.99769597  0.34548845  0.33876081  0.98954444  1.02682067]\n",
      "Loss: 953.3003066415284\n",
      "\n",
      "Gradient: [  -24.68287239   -52.13350065   -18.3918923    563.08989047\n",
      "   -24.07760164   -86.7103716   -136.37741793  -649.91874003\n",
      "  1213.0086305  -1173.62655304  3018.74612844]\n",
      "Old weights: [ 0.99978062  0.99869125  0.99902155 -0.31575074  0.99973686  0.99912508\n",
      "  0.99769597  0.34548845  0.33876081  0.98954444  1.02682067]\n",
      "New weights: [ 0.99978309  0.99869646  0.99902339 -0.31580705  0.99973927  0.99913375\n",
      "  0.99770961  0.34555345  0.33863951  0.9896618   1.02651879]\n",
      "Loss: 952.0335373642696\n",
      "\n",
      "Gradient: [  -24.67023615   -52.1291395    -18.39221142   537.85128786\n",
      "   -24.03385294   -86.68760716  -136.2995631   -608.41903664\n",
      "  1146.2703245  -1173.02809697  3017.20575405]\n",
      "Old weights: [ 0.99978309  0.99869646  0.99902339 -0.31580705  0.99973927  0.99913375\n",
      "  0.99770961  0.34555345  0.33863951  0.9896618   1.02651879]\n",
      "New weights: [ 0.99978555  0.99870167  0.99902523 -0.31586083  0.99974167  0.99914242\n",
      "  0.99772324  0.34561429  0.33852488  0.98977911  1.02621707]\n",
      "Loss: 950.7909248128207\n",
      "\n",
      "Gradient: [  -24.65810476   -52.12537538   -18.39324236   512.6329982\n",
      "   -23.99270424   -86.66530267  -136.22634325  -570.44373744\n",
      "  1083.07673564 -1172.4535196   3015.72677957]\n",
      "Old weights: [ 0.99978555  0.99870167  0.99902523 -0.31586083  0.99974167  0.99914242\n",
      "  0.99772324  0.34561429  0.33852488  0.98977911  1.02621707]\n",
      "New weights: [ 0.99978802  0.99870688  0.99902707 -0.31591209  0.99974407  0.99915108\n",
      "  0.99773686  0.34567133  0.33841657  0.98989635  1.0259155 ]\n",
      "Loss: 949.5699655715443\n",
      "\n",
      "Gradient: [  -24.64542705   -52.11848981   -18.39251078   489.97788222\n",
      "   -23.95237416   -86.6399738   -136.14924764  -533.60322811\n",
      "  1023.58111033 -1171.85283159  3014.18077232]\n",
      "Old weights: [ 0.99978802  0.99870688  0.99902707 -0.31591209  0.99974407  0.99915108\n",
      "  0.99773686  0.34567133  0.33841657  0.98989635  1.0259155 ]\n",
      "New weights: [ 0.99979048  0.9987121   0.99902891 -0.31596109  0.99974646  0.99915975\n",
      "  0.99775048  0.34572469  0.33831421  0.99001354  1.02561408]\n",
      "Loss: 948.3684286286436\n",
      "\n",
      "Gradient: [  -24.63308358   -52.11171849   -18.39213702   467.63046078\n",
      "   -23.91417558   -86.61466385  -136.07534746  -499.65387377\n",
      "   967.28433455 -1171.26792131  3012.67534686]\n",
      "Old weights: [ 0.99979048  0.9987121   0.99902891 -0.31596109  0.99974646  0.99915975\n",
      "  0.99775048  0.34572469  0.33831421  0.99001354  1.02561408]\n",
      "New weights: [ 0.99979295  0.99871731  0.99903075 -0.31600786  0.99974886  0.99916841\n",
      "  0.99776409  0.34577466  0.33821749  0.99013066  1.02531281]\n",
      "Loss: 947.1843257199512\n",
      "\n",
      "Gradient: [  -24.62035079   -52.10251566   -18.39042337   447.32675889\n",
      "   -23.87686193   -86.58698868  -135.998778    -466.92292908\n",
      "   914.24968798 -1170.66438169  3011.12210391]\n",
      "Old weights: [ 0.99979295  0.99871731  0.99903075 -0.31600786  0.99974886  0.99916841\n",
      "  0.99776409  0.34577466  0.33821749  0.99013066  1.02531281]\n",
      "New weights: [ 0.99979541  0.99872252  0.99903259 -0.31605259  0.99975124  0.99917707\n",
      "  0.99777769  0.34582135  0.33812606  0.99024773  1.0250117 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 946.0158849014125\n",
      "\n",
      "Gradient: [  -24.60783638   -52.09312727   -18.38883821   427.49683401\n",
      "   -23.84132016   -86.55906186  -135.92441317  -436.59639755\n",
      "   864.09323156 -1170.07112286  3009.59531537]\n",
      "Old weights: [ 0.99979541  0.99872252  0.99903259 -0.31605259  0.99975124  0.99917707\n",
      "  0.99777769  0.34582135  0.33812606  0.99024773  1.0250117 ]\n",
      "New weights: [ 0.99979787  0.99872773  0.99903443 -0.31609534  0.99975363  0.99918572\n",
      "  0.99779128  0.34586501  0.33803965  0.99036474  1.02471074]\n",
      "Loss: 944.8615269986343\n",
      "\n",
      "Gradient: [  -24.59504251   -52.08181393   -18.38621825   409.32245196\n",
      "   -23.80667433   -86.52925558  -135.84821176  -407.49839918\n",
      "   816.82085114 -1169.4644718   3008.03415959]\n",
      "Old weights: [ 0.99979787  0.99872773  0.99903443 -0.31609534  0.99975363  0.99918572\n",
      "  0.99779128  0.34586501  0.33803965  0.99036474  1.02471074]\n",
      "New weights: [ 0.99980033  0.99873294  0.99903626 -0.31613627  0.99975601  0.99919438\n",
      "  0.99780486  0.34590576  0.33795797  0.99048168  1.02440994]\n",
      "Loss: 943.7198446203788\n",
      "\n",
      "Gradient: [  -24.58238875   -52.07013774   -18.3835818    391.70781414\n",
      "   -23.77351983   -86.49904088  -135.7735333   -380.42451978\n",
      "   772.13233392 -1168.86438984  3006.48991793]\n",
      "Old weights: [ 0.99980033  0.99873294  0.99903626 -0.31613627  0.99975601  0.99919438\n",
      "  0.99780486  0.34590576  0.33795797  0.99048168  1.02440994]\n",
      "New weights: [ 0.99980279  0.99873814  0.9990381  -0.31617544  0.99975839  0.99920303\n",
      "  0.99781844  0.3459438   0.33788076  0.99059857  1.02410929]\n",
      "Loss: 942.5895834568751\n",
      "\n",
      "Gradient: [  -24.5695327    -52.05691341   -18.38013315   375.45430128\n",
      "   -23.74123797   -86.46730956  -135.69759321  -354.54381719\n",
      "   729.99811847 -1168.2545973   3004.92076353]\n",
      "Old weights: [ 0.99980279  0.99873814  0.9990381  -0.31617544  0.99975839  0.99920303\n",
      "  0.99781844  0.3459438   0.33788076  0.99059857  1.02410929]\n",
      "New weights: [ 0.99980524  0.99874335  0.99903994 -0.31621299  0.99976076  0.99921167\n",
      "  0.99783201  0.34597926  0.33780776  0.9907154   1.0238088 ]\n",
      "Loss: 941.4696256141272\n",
      "\n",
      "Gradient: [  -24.5567642    -52.04323017   -18.37657972   359.79438852\n",
      "   -23.71022491   -86.43508828  -135.62270729  -330.38518248\n",
      "   690.179571   -1167.64888414  3003.3621229 ]\n",
      "Old weights: [ 0.99980524  0.99874335  0.99903994 -0.31621299  0.99976076  0.99921167\n",
      "  0.99783201  0.34597926  0.33780776  0.9907154   1.0238088 ]\n",
      "New weights: [ 0.9998077   0.99874855  0.99904178 -0.31624897  0.99976313  0.99922032\n",
      "  0.99784557  0.3460123   0.33773874  0.99083216  1.02350846]\n",
      "Loss: 940.3589747626107\n",
      "\n",
      "Gradient: [  -24.5438479    -52.02828314   -18.37237847   345.2687272\n",
      "   -23.68004091   -86.40162544  -135.54695697  -307.3576335\n",
      "   652.6263607  -1167.03606216  3001.7852501 ]\n",
      "Old weights: [ 0.9998077   0.99874855  0.99904178 -0.31624897  0.99976313  0.99922032\n",
      "  0.99784557  0.3460123   0.33773874  0.99083216  1.02350846]\n",
      "New weights: [ 0.99981015  0.99875376  0.99904362 -0.31628349  0.9997655   0.99922896\n",
      "  0.99785913  0.34604303  0.33767348  0.99094886  1.02320828]\n",
      "Loss: 939.2567429027986\n",
      "\n",
      "Gradient: [  -24.53098416   -52.0128343    -18.36802198   331.33737231\n",
      "   -23.65094636   -86.36764003  -135.47193867  -285.80837503\n",
      "   617.14574735 -1166.42566456  3000.21463566]\n",
      "Old weights: [ 0.99981015  0.99875376  0.99904362 -0.31628349  0.9997655   0.99922896\n",
      "  0.99785913  0.34604303  0.33767348  0.99094886  1.02320828]\n",
      "New weights: [ 0.99981261  0.99875896  0.99904545 -0.31631663  0.99976786  0.99923759\n",
      "  0.99787267  0.34607161  0.33761176  0.99106551  1.02290826]\n",
      "Loss: 938.1621385715608\n",
      "\n",
      "Gradient: [  -24.51801135   -51.99633962   -18.36314075   318.36255455\n",
      "   -23.62262628   -86.33262489  -135.39633057  -265.31379337\n",
      "   583.67634792 -1165.81000863  2998.63053991]\n",
      "Old weights: [ 0.99981261  0.99875896  0.99904545 -0.31631663  0.99976786  0.99923759\n",
      "  0.99787267  0.34607161  0.33761176  0.99106551  1.02290826]\n",
      "New weights: [ 0.99981506  0.99876416  0.99904729 -0.31634846  0.99977023  0.99924623\n",
      "  0.99788621  0.34609814  0.33755339  0.99118209  1.0226084 ]\n",
      "Loss: 937.0744563324906\n",
      "\n",
      "Gradient: [  -24.50506809   -51.97933431   -18.35807873   305.96236708\n",
      "   -23.59524889   -86.29708576  -135.32123328  -246.09776259\n",
      "   552.06012967 -1165.19569073  2997.04990837]\n",
      "Old weights: [ 0.99981506  0.99876416  0.99904729 -0.31634846  0.99977023  0.99924623\n",
      "  0.99788621  0.34609814  0.33755339  0.99118209  1.0226084 ]\n",
      "New weights: [ 0.99981751  0.99876935  0.99904912 -0.31637906  0.99977259  0.99925486\n",
      "  0.99789975  0.34612275  0.33749819  0.99129861  1.02230869]\n",
      "Loss: 935.9930674104004\n",
      "\n",
      "Gradient: [  -24.49204349   -51.96145296   -18.35258551   294.37720697\n",
      "   -23.5685865    -86.26068274  -135.24573622  -227.85379211\n",
      "   522.23099908 -1164.57744096  2995.45920048]\n",
      "Old weights: [ 0.99981751  0.99876935  0.99904912 -0.31637906  0.99977259  0.99925486\n",
      "  0.99789975  0.34612275  0.33749819  0.99129861  1.02230869]\n",
      "New weights: [ 0.99981996  0.99877455  0.99905096 -0.3164085   0.99977494  0.99926348\n",
      "  0.99791327  0.34614554  0.33744596  0.99141507  1.02200915]\n",
      "Loss: 934.9174113454302\n",
      "\n",
      "Gradient: [  -24.47903355   -51.94307341   -18.34690214   283.33514906\n",
      "   -23.542745     -86.22377348  -135.17059823  -210.72243745\n",
      "   494.05758651 -1163.95982882  2993.87015458]\n",
      "Old weights: [ 0.99981996  0.99877455  0.99905096 -0.3164085   0.99977494  0.99926348\n",
      "  0.99791327  0.34614554  0.33744596  0.99141507  1.02200915]\n",
      "New weights: [ 0.99982241  0.99877974  0.99905279 -0.31643683  0.9997773   0.9992721\n",
      "  0.99792679  0.34616661  0.33739656  0.99153146  1.02170976]\n",
      "Loss: 933.8469885557939\n",
      "\n",
      "Gradient: [  -24.46596231   -51.92395229   -18.34085981   272.99363216\n",
      "   -23.51755803   -86.18613232  -135.09519211  -194.47950991\n",
      "   467.47314208 -1163.33924456  2992.27349545]\n",
      "Old weights: [ 0.99982241  0.99877974  0.99905279 -0.31643683  0.9997773   0.9992721\n",
      "  0.99792679  0.34616661  0.33739656  0.99153146  1.02170976]\n",
      "New weights: [ 0.99982485  0.99878494  0.99905463 -0.31646413  0.99977965  0.99928072\n",
      "  0.9979403   0.34618606  0.33734981  0.9916478   1.02141053]\n",
      "Loss: 932.7813537102138\n",
      "\n",
      "Gradient: [  -24.45289634   -51.904358     -18.33462821   263.15747746\n",
      "   -23.49308964   -86.14801386  -135.02004114  -179.20963914\n",
      "   442.3671166  -1162.71885826  2990.67736623]\n",
      "Old weights: [ 0.99982485  0.99878494  0.99905463 -0.31646413  0.99977965  0.99928072\n",
      "  0.9979403   0.34618606  0.33734981  0.9916478   1.02141053]\n",
      "New weights: [ 0.9998273   0.99879013  0.99905646 -0.31649045  0.999782    0.99928934\n",
      "  0.9979538   0.34620398  0.33730557  0.99176407  1.02111146]\n",
      "Loss: 931.7201098218274\n",
      "\n",
      "Gradient: [  -24.43978371   -51.88413038   -18.32809437   253.92784784\n",
      "   -23.46921672   -86.10927022  -134.94471335  -164.74676806\n",
      "   418.67461589 -1162.09620189  2989.0754251 ]\n",
      "Old weights: [ 0.9998273   0.99879013  0.99905646 -0.31649045  0.999782    0.99928934\n",
      "  0.9979538   0.34620398  0.33730557  0.99176407  1.02111146]\n",
      "New weights: [ 0.99982974  0.99879532  0.99905829 -0.31651584  0.99978434  0.99929795\n",
      "  0.99796729  0.34622045  0.33726371  0.99188028  1.02081256]\n",
      "Loss: 930.6629029851565\n",
      "\n",
      "Gradient: [  -24.42667067   -51.86346154   -18.32137839   245.16330249\n",
      "   -23.44597547   -86.07008409  -134.86956966  -151.13831593\n",
      "   396.30161842 -1161.47347878  2987.47333166]\n",
      "Old weights: [ 0.99982974  0.99879532  0.99905829 -0.31651584  0.99978434  0.99929795\n",
      "  0.99796729  0.34622045  0.33726371  0.99188028  1.02081256]\n",
      "New weights: [ 0.99983218  0.9988005   0.99906013 -0.31654035  0.99978669  0.99930656\n",
      "  0.99798078  0.34623557  0.33722408  0.99199642  1.02051381]\n",
      "Loss: 929.6094176860495\n",
      "\n",
      "Gradient: [  -24.41352178   -51.84224782   -18.31440543   236.9270195\n",
      "   -23.42327353   -86.03036039  -134.79431264  -138.25954614\n",
      "   375.18656564 -1160.8490056   2985.86676018]\n",
      "Old weights: [ 0.99983218  0.9988005   0.99906013 -0.31654035  0.99978669  0.99930656\n",
      "  0.99798078  0.34623557  0.33722408  0.99199642  1.02051381]\n",
      "New weights: [ 0.99983463  0.99880569  0.99906196 -0.31656405  0.99978903  0.99931516\n",
      "  0.99799426  0.34624939  0.33718656  0.99211251  1.02021522]\n",
      "Loss: 928.559372622172\n",
      "\n",
      "Gradient: [  -24.40036926   -51.82062797   -18.3072611    229.11534721\n",
      "   -23.40112875   -85.99023145  -134.71919125  -126.1334208\n",
      "   355.24876801 -1160.22431741  2984.25965347]\n",
      "Old weights: [ 0.99983463  0.99880569  0.99906196 -0.31656405  0.99978903  0.99931516\n",
      "  0.99799426  0.34624939  0.33718656  0.99211251  1.02021522]\n",
      "New weights: [ 0.99983707  0.99881087  0.99906379 -0.31658696  0.99979137  0.99932376\n",
      "  0.99800773  0.34626201  0.33715103  0.99222853  1.0199168 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 927.5125169785682\n",
      "\n",
      "Gradient: [  -24.38718899   -51.79853655   -18.29989646   221.76599766\n",
      "   -23.37947079   -85.94963782  -134.64400075  -114.66480099\n",
      "   336.43079865 -1159.59826974  2982.64907034]\n",
      "Old weights: [ 0.99983707  0.99881087  0.99906379 -0.31658696  0.99979137  0.99932376\n",
      "  0.99800773  0.34626201  0.33715103  0.99222853  1.0199168 ]\n",
      "New weights: [ 0.9998395   0.99881605  0.99906562 -0.31660914  0.99979371  0.99933235\n",
      "  0.9980212   0.34627347  0.33711739  0.99234449  1.01961853]\n",
      "Loss: 926.4686271086886\n",
      "\n",
      "Gradient: [  -24.37400349   -51.77607486   -18.29237305   214.80203466\n",
      "   -23.35830562   -85.90867645  -134.56891294  -103.8608516\n",
      "   318.66288626 -1158.97193529  2981.03776583]\n",
      "Old weights: [ 0.9998395   0.99881605  0.99906562 -0.31660914  0.99979371  0.99933235\n",
      "  0.9980212   0.34627347  0.33711739  0.99234449  1.01961853]\n",
      "New weights: [ 0.99984194  0.99882123  0.99906745 -0.31663062  0.99979605  0.99934094\n",
      "  0.99803465  0.34628386  0.33708552  0.99246039  1.01932043]\n",
      "Loss: 925.4275035767317\n",
      "\n",
      "Gradient: [  -24.36079641   -51.75320314   -18.28465953   208.24425429\n",
      "   -23.33757881   -85.86731178  -134.49378691   -93.6478304\n",
      "   301.89208469 -1158.34453924  2979.42374856]\n",
      "Old weights: [ 0.99984194  0.99882123  0.99906745 -0.31663062  0.99979605  0.99934094\n",
      "  0.99803465  0.34628386  0.33708552  0.99246039  1.01932043]\n",
      "New weights: [ 0.99984438  0.9988264   0.99906928 -0.31665144  0.99979838  0.99934953\n",
      "  0.9980481   0.34629323  0.33705533  0.99257622  1.01902248]\n",
      "Loss: 924.388968522002\n",
      "\n",
      "Gradient: [  -24.34758354   -51.72999624   -18.27680054   202.03473099\n",
      "   -23.31728883   -85.82561572  -134.41874127   -84.02295806\n",
      "   286.05768905 -1157.71683402  2977.80895083]\n",
      "Old weights: [ 0.99984438  0.9988264   0.99906928 -0.31665144  0.99979838  0.99934953\n",
      "  0.9980481   0.34629323  0.33705533  0.99257622  1.01902248]\n",
      "New weights: [ 0.99984681  0.99883157  0.9990711  -0.31667164  0.99980071  0.99935811\n",
      "  0.99806155  0.34630163  0.33702673  0.99269199  1.0187247 ]\n",
      "Loss: 923.3528633102161\n",
      "\n",
      "Gradient: [  -24.33435387   -51.70643152   -18.26877661   196.18316826\n",
      "   -23.29739285   -85.7835686   -134.34367908   -74.92812838\n",
      "   271.11129663 -1157.08829807  2976.19203197]\n",
      "Old weights: [ 0.99984681  0.99883157  0.9990711  -0.31667164  0.99980071  0.99935811\n",
      "  0.99806155  0.34630163  0.33702673  0.99269199  1.0187247 ]\n",
      "New weights: [ 0.99984925  0.99883675  0.99907293 -0.31669126  0.99980304  0.99936669\n",
      "  0.99807498  0.34630912  0.33699962  0.9928077   1.01842708]\n",
      "Loss: 922.319046440454\n",
      "\n",
      "Gradient: [  -24.32111852   -51.68256506   -18.26062051   190.64527596\n",
      "   -23.27788481   -85.74122454  -134.26868225   -66.35454889\n",
      "   256.99982485 -1156.45946164  2974.57435361]\n",
      "Old weights: [ 0.99984925  0.99883675  0.99907293 -0.31669126  0.99980304  0.99936669\n",
      "  0.99807498  0.34630912  0.33699962  0.9928077   1.01842708]\n",
      "New weights: [ 0.99985168  0.99884191  0.99907476 -0.31671033  0.99980537  0.99937527\n",
      "  0.99808841  0.34631576  0.33697392  0.99292335  1.01812963]\n",
      "Loss: 921.2873916800742\n",
      "\n",
      "Gradient: [  -24.30787011   -51.65838551   -18.25232071   185.42361681\n",
      "   -23.25873043   -85.69857428  -134.19368415   -58.25568245\n",
      "   243.67929926 -1155.82997634  2972.95502004]\n",
      "Old weights: [ 0.99985168  0.99884191  0.99907476 -0.31671033  0.99980537  0.99937527\n",
      "  0.99808841  0.34631576  0.33697392  0.99292335  1.01812963]\n",
      "New weights: [ 0.99985411  0.99884708  0.99907658 -0.31672887  0.99980769  0.99938384\n",
      "  0.99810183  0.34632158  0.33694955  0.99303893  1.01783233]\n",
      "Loss: 920.2577864025569\n",
      "\n",
      "Gradient: [  -24.29461653   -51.63393553   -18.24390158   180.48377353\n",
      "   -23.23992108   -85.65565917  -134.11874138   -50.6193412\n",
      "   231.10311473 -1155.20021805  2971.33499632]\n",
      "Old weights: [ 0.99985411  0.99884708  0.99907658 -0.31672887  0.99980769  0.99938384\n",
      "  0.99810183  0.34632158  0.33694955  0.99303893  1.01783233]\n",
      "New weights: [ 0.99985654  0.99885224  0.99907841 -0.31674692  0.99981002  0.9993924\n",
      "  0.99811524  0.34632664  0.33692644  0.99315445  1.0175352 ]\n",
      "Loss: 919.2301301064252\n",
      "\n",
      "Gradient: [  -24.2813529    -51.60921107   -18.23535685   175.82383663\n",
      "   -23.22142891   -85.61247666  -134.04380815   -43.40766785\n",
      "   219.23150448 -1154.5699565   2969.71369031]\n",
      "Old weights: [ 0.99985654  0.99885224  0.99907841 -0.31674692  0.99981002  0.9993924\n",
      "  0.99811524  0.34632664  0.33692644  0.99315445  1.0175352 ]\n",
      "New weights: [ 0.99985897  0.9988574   0.99908023 -0.3167645   0.99981234  0.99940096\n",
      "  0.99812864  0.34633098  0.33690452  0.99326991  1.01723823]\n",
      "Loss: 918.2043330953553\n",
      "\n",
      "Gradient: [  -24.26808482   -51.58424509   -18.2267049    171.41661686\n",
      "   -23.20324398   -85.56905886  -133.9689236    -36.60680171\n",
      "   208.02341856 -1153.93946002  2968.09179085]\n",
      "Old weights: [ 0.99985897  0.9988574   0.99908023 -0.3167645   0.99981234  0.99940096\n",
      "  0.99812864  0.34633098  0.33690452  0.99326991  1.01723823]\n",
      "New weights: [ 0.99986139  0.99886256  0.99908205 -0.31678164  0.99981466  0.99940952\n",
      "  0.99814204  0.34633464  0.33688371  0.9933853   1.01694142]\n",
      "Loss: 917.1803153019182\n",
      "\n",
      "Gradient: [  -24.25480913   -51.55903818   -18.21794293   167.25752277\n",
      "   -23.18534333   -85.52540744  -133.89405632   -30.18549722\n",
      "   197.44301999 -1153.30857869  2966.46891226]\n",
      "Old weights: [ 0.99986139  0.99886256  0.99908205 -0.31678164  0.99981466  0.99940952\n",
      "  0.99814204  0.34633464  0.33688371  0.9933853   1.01694142]\n",
      "New weights: [ 0.99986382  0.99886772  0.99908387 -0.31679837  0.99981698  0.99941807\n",
      "  0.99815543  0.34633766  0.33686397  0.99350063  1.01664477]\n",
      "Loss: 916.1580052395228\n",
      "\n",
      "Gradient: [  -24.24152986   -51.53361624   -18.20908501   163.32472421\n",
      "   -23.16771655   -85.48154775  -133.8192334    -24.12933538\n",
      "   187.4540596  -1152.6775057   2964.8455504 ]\n",
      "Old weights: [ 0.99986382  0.99886772  0.99908387 -0.31679837  0.99981698  0.99941807\n",
      "  0.99815543  0.34633766  0.33686397  0.99350063  1.01664477]\n",
      "New weights: [ 0.99986624  0.99887287  0.9990857  -0.3168147   0.9998193   0.99942662\n",
      "  0.99816881  0.34634008  0.33684522  0.9936159   1.01634829]\n",
      "Loss: 915.1373390683171\n",
      "\n",
      "Gradient: [  -24.22824497   -51.50798267   -18.20013054   159.61213802\n",
      "   -23.15034449   -85.43748397  -133.74443328   -18.41218853\n",
      "   178.02432655 -1152.04614553  2963.22145941]\n",
      "Old weights: [ 0.99986624  0.99887287  0.9990857  -0.3168147   0.9998193   0.99942662\n",
      "  0.99816881  0.34634008  0.33684522  0.9936159   1.01634829]\n",
      "New weights: [ 0.99986867  0.99887802  0.99908752 -0.31683066  0.99982161  0.99943516\n",
      "  0.99818218  0.34634192  0.33682742  0.99373111  1.01605196]\n",
      "Loss: 914.1182597628999\n",
      "\n",
      "Gradient: [  -24.21495739   -51.48215815   -18.1910905    156.101964\n",
      "   -23.1332168    -85.39323644  -133.66967476   -13.01978271\n",
      "   169.12174671 -1151.4146387   2961.59699987]\n",
      "Old weights: [ 0.99986867  0.99887802  0.99908752 -0.31683066  0.99982161  0.99943516\n",
      "  0.99818218  0.34634192  0.33682742  0.99373111  1.01605196]\n",
      "New weights: [ 0.99987109  0.99888317  0.99908933 -0.31684627  0.99982392  0.9994437\n",
      "  0.99819555  0.34634322  0.33681051  0.99384625  1.0157558 ]\n",
      "Loss: 913.100716370533\n",
      "\n",
      "Gradient: [  -24.20166587   -51.45614772   -18.18196565   152.78740872\n",
      "   -23.11631727   -85.34881077  -133.59494309    -7.93001751\n",
      "   160.71742623 -1150.78292631  2959.97202008]\n",
      "Old weights: [ 0.99987109  0.99888317  0.99908933 -0.31684627  0.99982392  0.9994437\n",
      "  0.99819555  0.34634322  0.33681051  0.99384625  1.0157558 ]\n",
      "New weights: [ 0.99987351  0.99888832  0.99909115 -0.31686155  0.99982624  0.99945224\n",
      "  0.99820891  0.34634401  0.33679444  0.99396133  1.01545981]\n",
      "Loss: 912.084663350211\n",
      "\n",
      "Gradient: [  -24.18837255   -51.42996814   -18.17276468   149.65374931\n",
      "   -23.09963596   -85.30422351  -133.52025129    -3.12919108\n",
      "   152.78294039 -1150.15111177  2958.34678529]\n",
      "Old weights: [ 0.99987351  0.99888832  0.99909115 -0.31686155  0.99982624  0.99945224\n",
      "  0.99820891  0.34634401  0.33679444  0.99396133  1.01545981]\n",
      "New weights: [ 0.99987593  0.99889346  0.99909297 -0.31687651  0.99982855  0.99946077\n",
      "  0.99822226  0.34634433  0.33677916  0.99407634  1.01516397]\n",
      "Loss: 911.0700599836125\n",
      "\n",
      "Gradient: [-2.41750767e+01 -5.14036253e+01 -1.81634892e+01  1.46693986e+02\n",
      " -2.30831590e+01 -8.52594810e+01 -1.33445589e+02  1.40157579e+00\n",
      "  1.45292410e+02 -1.14951916e+03  2.95672121e+03]\n",
      "Old weights: [ 0.99987593  0.99889346  0.99909297 -0.31687651  0.99982855  0.99946077\n",
      "  0.99822226  0.34634433  0.33677916  0.99407634  1.01516397]\n",
      "New weights: [ 0.99987834  0.9988986   0.99909479 -0.31689118  0.99983085  0.99946929\n",
      "  0.99823561  0.34634419  0.33676463  0.99419129  1.0148683 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 910.0568698502499\n",
      "\n",
      "Gradient: [  -24.16177992   -51.3771329    -18.15414616   143.89578406\n",
      "   -23.06687715   -85.2145968   -133.37096616     5.67516997\n",
      "   138.2206141  -1148.8871501   2955.09548225]\n",
      "Old weights: [ 0.99987834  0.9988986   0.99909479 -0.31689118  0.99983085  0.99946929\n",
      "  0.99823561  0.34634419  0.33676463  0.99419129  1.0148683 ]\n",
      "New weights: [ 0.99988076  0.99890374  0.9990966  -0.31690557  0.99983316  0.99947782\n",
      "  0.99824894  0.34634362  0.33675081  0.99430618  1.01457279]\n",
      "Loss: 909.0450603596631\n",
      "\n",
      "Gradient: [  -24.1484818    -51.3504972    -18.14473764   141.25225387\n",
      "   -23.05077838   -85.16957751  -133.29637504     9.70785351\n",
      "   131.54440036 -1148.25506187  2953.46956463]\n",
      "Old weights: [ 0.99988076  0.99890374  0.9990966  -0.31690557  0.99983316  0.99947782\n",
      "  0.99824894  0.34634362  0.33675081  0.99430618  1.01457279]\n",
      "New weights: [ 0.99988318  0.99890887  0.99909842 -0.3169197   0.99983547  0.99948633\n",
      "  0.99826227  0.34634265  0.33673765  0.99442101  1.01427744]\n",
      "Loss: 908.0346023346318\n",
      "\n",
      "Gradient: [  -24.13518355   -51.32372969   -18.13526933   138.75294478\n",
      "   -23.03485403   -85.12443456  -133.22182223    13.51157666\n",
      "   125.24136812 -1147.62295429  2951.84360346]\n",
      "Old weights: [ 0.99988318  0.99890887  0.99909842 -0.3169197   0.99983547  0.99948633\n",
      "  0.99826227  0.34634265  0.33673765  0.99442101  1.01427744]\n",
      "New weights: [ 0.99988559  0.998914    0.99910023 -0.31693357  0.99983777  0.99949485\n",
      "  0.9982756   0.3463413   0.33672513  0.99453577  1.01398226]\n",
      "Loss: 907.0254696397373\n",
      "\n",
      "Gradient: [  -24.12188501   -51.29683651   -18.1257435    136.39126792\n",
      "   -23.01909377   -85.07917434  -133.14730309    17.10044389\n",
      "   119.29082403 -1146.99081972  2950.2175788 ]\n",
      "Old weights: [ 0.99988559  0.998914    0.99910023 -0.31693357  0.99983777  0.99949485\n",
      "  0.9982756   0.3463413   0.33672513  0.99453577  1.01398226]\n",
      "New weights: [ 0.999888    0.99891913  0.99910204 -0.31694721  0.99984007  0.99950335\n",
      "  0.99828891  0.34633959  0.3367132   0.99465047  1.01368724]\n",
      "Loss: 906.0176388503269\n",
      "\n",
      "Gradient: [  -24.10858709   -51.26982739   -18.11616486   134.15828339\n",
      "   -23.00348963   -85.03380655  -133.07282202    20.48542583\n",
      "   113.67285755 -1146.35870298  2948.59160553]\n",
      "Old weights: [ 0.999888    0.99891913  0.99910204 -0.31694721  0.99984007  0.99950335\n",
      "  0.99828891  0.34633959  0.3367132   0.99465047  1.01368724]\n",
      "New weights: [ 0.99989041  0.99892426  0.99910385 -0.31696063  0.99984237  0.99951186\n",
      "  0.99830222  0.34633754  0.33670183  0.99476511  1.01339238]\n",
      "Loss: 905.0110889575641\n",
      "\n",
      "Gradient: [  -24.09528976   -51.24270822   -18.10653572   132.04781001\n",
      "   -22.9880326    -84.9883373   -132.99837587    23.67882461\n",
      "   108.3689854  -1145.72660315  2946.96568106]\n",
      "Old weights: [ 0.99989041  0.99892426  0.99910385 -0.31696063  0.99984237  0.99951186\n",
      "  0.99830222  0.34633754  0.33670183  0.99476511  1.01339238]\n",
      "New weights: [ 0.99989282  0.99892939  0.99910567 -0.31697383  0.99984467  0.99952036\n",
      "  0.99831552  0.34633517  0.336691    0.99487968  1.01309768]\n",
      "Loss: 904.0058011055826\n",
      "\n",
      "Gradient: [  -24.08199375   -51.21548732   -18.09686006   130.05213818\n",
      "   -22.97271539   -84.94277489  -132.92396775    26.69063797\n",
      "   103.36150021 -1145.09455524  2945.33989504]\n",
      "Old weights: [ 0.99989282  0.99892939  0.99910567 -0.31697383  0.99984467  0.99952036\n",
      "  0.99831552  0.34633517  0.336691    0.99487968  1.01309768]\n",
      "New weights: [ 0.99989523  0.99893451  0.99910747 -0.31698684  0.99984697  0.99952885\n",
      "  0.99832881  0.3463325   0.33668066  0.99499419  1.01280315]\n",
      "Loss: 903.0017583571848\n",
      "\n",
      "Gradient: [  -24.06869911   -51.18817025   -18.08714011   128.16554629\n",
      "   -22.95753013   -84.89712505  -132.84959554    29.53164484\n",
      "    98.63390144 -1144.46256249  2943.71425562]\n",
      "Old weights: [ 0.99989523  0.99893451  0.99910747 -0.31698684  0.99984697  0.99952885\n",
      "  0.99832881  0.3463325   0.33668066  0.99499419  1.01280315]\n",
      "New weights: [ 0.99989764  0.99893963  0.99910928 -0.31699966  0.99984926  0.99953734\n",
      "  0.9983421   0.34632955  0.3366708   0.99510863  1.01250878]\n",
      "Loss: 901.9989454851918\n",
      "\n",
      "Gradient: [  -24.05540642   -51.16076416   -18.07737926   126.38134122\n",
      "   -22.9424702    -84.85139495  -132.77526141    32.2109072\n",
      "    94.17043402 -1143.83065267  2942.08883393]\n",
      "Old weights: [ 0.99989764  0.99893963  0.99910928 -0.31699966  0.99984926  0.99953734\n",
      "  0.9983421   0.34632955  0.3366708   0.99510863  1.01250878]\n",
      "New weights: [ 0.99990004  0.99894474  0.99911109 -0.31701229  0.99985156  0.99954582\n",
      "  0.99835537  0.34632633  0.33666138  0.99522302  1.01221457]\n",
      "Loss: 900.9973487863626\n",
      "\n",
      "Gradient: [  -24.04211578   -51.13327423   -18.06757966   124.6942772\n",
      "   -22.92752867   -84.80558988  -132.70096397    34.73790372\n",
      "    89.95637348 -1143.19883149  2940.46364439]\n",
      "Old weights: [ 0.99990004  0.99894474  0.99911109 -0.31701229  0.99985156  0.99954582\n",
      "  0.99835537  0.34632633  0.33666138  0.99522302  1.01221457]\n",
      "New weights: [ 0.99990245  0.99894986  0.9991129  -0.31702476  0.99985385  0.99955431\n",
      "  0.99836864  0.34632285  0.33665238  0.99533734  1.01192052]\n",
      "Loss: 899.9969559156447\n",
      "\n",
      "Gradient: [  -24.02882765   -51.10570663   -18.05774418   123.09851186\n",
      "   -22.91269959   -84.75971605  -132.62670475    37.12081502\n",
      "    85.97769684 -1142.56712133  2938.83874435]\n",
      "Old weights: [ 0.99990245  0.99894986  0.9991129  -0.31702476  0.99985385  0.99955431\n",
      "  0.99836864  0.34632285  0.33665238  0.99533734  1.01192052]\n",
      "New weights: [ 0.99990485  0.99895497  0.9991147  -0.31703707  0.99985614  0.99956278\n",
      "  0.99838191  0.34631914  0.33664379  0.99545159  1.01162664]\n",
      "Loss: 898.9977557384958\n",
      "\n",
      "Gradient: [  -24.01554217   -51.07806612   -18.04787483   121.58926919\n",
      "   -22.89797683   -84.71377833  -132.55248282    39.36800052\n",
      "    82.22126867 -1141.93552918  2937.21415157]\n",
      "Old weights: [ 0.99990485  0.99895497  0.9991147  -0.31703707  0.99985614  0.99956278\n",
      "  0.99838191  0.34631914  0.33664379  0.99545159  1.01162664]\n",
      "New weights: [ 0.99990725  0.99896007  0.99911651 -0.31704923  0.99985843  0.99957125\n",
      "  0.99839516  0.3463152   0.33663556  0.99556579  1.01133292]\n",
      "Loss: 897.9997381991664\n",
      "\n",
      "Gradient: [  -24.00225972   -51.0503581    -18.03797412   120.16142701\n",
      "   -22.88335503   -84.66778213  -132.47829929    41.48682271\n",
      "    78.6746043  -1141.30407337  2935.58991298]\n",
      "Old weights: [ 0.99990725  0.99896007  0.99911651 -0.31704923  0.99985843  0.99957125\n",
      "  0.99839516  0.3463152   0.33663556  0.99556579  1.01133292]\n",
      "New weights: [ 0.99990965  0.99896518  0.99911831 -0.31706125  0.99986072  0.99957972\n",
      "  0.99840841  0.34631105  0.3366277   0.99567992  1.01103936]\n",
      "Loss: 897.0028942034975\n",
      "\n",
      "Gradient: [  -23.98898044   -51.02258691   -18.02804389   118.81065941\n",
      "   -22.86882878   -84.62173188  -132.40415355    43.48467074\n",
      "    75.32598868 -1140.67276143  2933.96604778]\n",
      "Old weights: [ 0.99990965  0.99896518  0.99911831 -0.31706125  0.99986072  0.99957972\n",
      "  0.99840841  0.34631105  0.3366277   0.99567992  1.01103936]\n",
      "New weights: [ 0.99991205  0.99897028  0.99912012 -0.31707313  0.99986301  0.99958818\n",
      "  0.99842165  0.34630671  0.33662016  0.99579398  1.01074596]\n",
      "Loss: 896.0072155143048\n",
      "\n",
      "Gradient: [  -23.97570465   -50.99475726   -18.01808631   117.53245985\n",
      "   -22.85439326   -84.57563234  -132.3300464     45.36815551\n",
      "    72.16430434 -1140.04160858  2932.34259488]\n",
      "Old weights: [ 0.99991205  0.99897028  0.99912012 -0.31707313  0.99986301  0.99958818\n",
      "  0.99842165  0.34630671  0.33662016  0.99579398  1.01074596]\n",
      "New weights: [ 0.99991445  0.99897538  0.99912192 -0.31708488  0.99986529  0.99959664\n",
      "  0.99843488  0.34630217  0.33661295  0.99590799  1.01045273]\n",
      "Loss: 895.0126946582815\n",
      "\n",
      "Gradient: [  -23.9624325    -50.9668731    -18.00810308   116.32292523\n",
      "   -22.84004369   -84.5294875   -132.25597744    47.14382025\n",
      "    69.17910499 -1139.41062243  2930.7195737 ]\n",
      "Old weights: [ 0.99991445  0.99897538  0.99912192 -0.31708488  0.99986529  0.99959664\n",
      "  0.99843488  0.34630217  0.33661295  0.99590799  1.01045273]\n",
      "New weights: [ 0.99991684  0.99898048  0.99912372 -0.31709651  0.99986758  0.99960509\n",
      "  0.99844811  0.34629746  0.33660603  0.99602193  1.01015965]\n",
      "Loss: 894.0193248429028\n",
      "\n",
      "Gradient: [  -23.94916425   -50.93893856   -17.9980961    115.1780796\n",
      "   -22.82577575   -84.48330155  -132.18194726    48.81759057\n",
      "    66.36048903 -1138.77981573  2929.09701691]\n",
      "Old weights: [ 0.99991684  0.99898048  0.99912372 -0.31709651  0.99986758  0.99960509\n",
      "  0.99844811  0.34629746  0.33660603  0.99602193  1.01015965]\n",
      "New weights: [ 0.99991924  0.99898557  0.99912552 -0.31710803  0.99986986  0.99961354\n",
      "  0.99846133  0.34629257  0.33659939  0.99613581  1.00986674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 893.0270998826039\n",
      "\n",
      "Gradient: [  -23.93590004   -50.9109572    -17.9880669    114.09441161\n",
      "   -22.81158518   -84.43707809  -132.1079556     50.39526875\n",
      "    63.69914286 -1138.14919591  2927.47494339]\n",
      "Old weights: [ 0.99991924  0.99898557  0.99912552 -0.31710803  0.99986986  0.99961354\n",
      "  0.99846133  0.34629257  0.33659939  0.99613581  1.00986674]\n",
      "New weights: [ 0.99992163  0.99899066  0.99912732 -0.31711944  0.99987214  0.99962198\n",
      "  0.99847454  0.34628753  0.33659302  0.99624962  1.009574  ]\n",
      "Loss: 892.0360141326819\n",
      "\n",
      "Gradient: [  -23.92264011   -50.88293268   -17.97801714   113.06840569\n",
      "   -22.79746813   -84.39082081  -132.0340029     51.88215903\n",
      "    61.18624667 -1137.51877376  2925.85338083]\n",
      "Old weights: [ 0.99992163  0.99899066  0.99912732 -0.31711944  0.99987214  0.99962198\n",
      "  0.99847454  0.34628753  0.33659302  0.99624962  1.009574  ]\n",
      "New weights: [ 0.99992402  0.99899575  0.99912911 -0.31713075  0.99987442  0.99963042\n",
      "  0.99848774  0.34628235  0.33658691  0.99636337  1.00928141]\n",
      "Loss: 891.0460624306659\n",
      "\n",
      "Gradient: [  -23.90938459   -50.8548682    -17.96794824   112.09690992\n",
      "   -22.78342081   -84.34453297  -131.960089      53.28341227\n",
      "    58.81349765 -1136.88855633  2924.23234713]\n",
      "Old weights: [ 0.99992402  0.99899575  0.99912911 -0.31713075  0.99987442  0.99963042\n",
      "  0.99848774  0.34628235  0.33658691  0.99636337  1.00928141]\n",
      "New weights: [ 0.99992642  0.99900084  0.99913091 -0.31714196  0.9998767   0.99963886\n",
      "  0.99850094  0.34627702  0.33658102  0.99647706  1.00898899]\n",
      "Loss: 890.0572400439321\n",
      "\n",
      "Gradient: [  -23.89613367   -50.82676697   -17.95786164   111.17681045\n",
      "   -22.76943978   -84.29821781  -131.88621423    54.60377162\n",
      "    56.57303884 -1136.25855285  2922.61186599]\n",
      "Old weights: [ 0.99992642  0.99900084  0.99913091 -0.31714196  0.9998767   0.99963886\n",
      "  0.99850094  0.34627702  0.33658102  0.99647706  1.00898899]\n",
      "New weights: [ 0.9999288   0.99900592  0.99913271 -0.31715308  0.99987898  0.99964729\n",
      "  0.99851413  0.34627156  0.33657537  0.99659069  1.00869673]\n",
      "Loss: 889.0695426230601\n",
      "\n",
      "Gradient: [  -23.88288748   -50.79863188   -17.9477586    110.30528274\n",
      "   -22.75552168   -84.25187825  -131.8123785     55.84781359\n",
      "    54.45746915 -1135.62876987  2920.99195407]\n",
      "Old weights: [ 0.9999288   0.99900592  0.99913271 -0.31715308  0.99987898  0.99964729\n",
      "  0.99851413  0.34627156  0.33657537  0.99659069  1.00869673]\n",
      "New weights: [ 0.99993119  0.999011    0.9991345  -0.31716411  0.99988125  0.99965571\n",
      "  0.99852731  0.34626597  0.33656992  0.99670425  1.00840463]\n",
      "Loss: 888.0829661602517\n",
      "\n",
      "Gradient: [  -23.86964618   -50.77046579   -17.93764042   109.47956501\n",
      "   -22.74166342   -84.20551718  -131.73858205    57.0197762\n",
      "    52.45978881 -1134.99921536  2919.37263179]\n",
      "Old weights: [ 0.99993119  0.999011    0.9991345  -0.31716411  0.99988125  0.99965571\n",
      "  0.99852731  0.34626597  0.33656992  0.99670425  1.00840463]\n",
      "New weights: [ 0.99993358  0.99901608  0.9991363  -0.31717505  0.99988353  0.99966413\n",
      "  0.99854048  0.34626027  0.33656467  0.99681775  1.00811269]\n",
      "Loss: 887.0975069522907\n",
      "\n",
      "Gradient: [  -23.8564099    -50.74227126   -17.92750823   108.69712938\n",
      "   -22.72786203   -84.1591372   -131.66482483    58.12372838\n",
      "    50.57340099 -1134.36989533  2917.75391448]\n",
      "Old weights: [ 0.99993358  0.99901608  0.9991363  -0.31717505  0.99988353  0.99966413\n",
      "  0.99854048  0.34626027  0.33656467  0.99681775  1.00811269]\n",
      "New weights: [ 0.99993597  0.99902115  0.99913809 -0.31718592  0.9998858   0.99967255\n",
      "  0.99855365  0.34625446  0.33655962  0.99693119  1.00782092]\n",
      "Loss: 886.1131615675382\n",
      "\n",
      "Gradient: [  -23.84317878   -50.71405083   -17.91736316   107.95552375\n",
      "   -22.71411473   -84.11274087  -131.59110702    59.16345453\n",
      "    48.79206922 -1133.7408167   2916.13581984]\n",
      "Old weights: [ 0.99993597  0.99902115  0.99913809 -0.31718592  0.9998858   0.99967255\n",
      "  0.99855365  0.34625446  0.33655962  0.99693119  1.00782092]\n",
      "New weights: [ 0.99993835  0.99902622  0.99913988 -0.31719672  0.99988807  0.99968096\n",
      "  0.99856681  0.34624854  0.33655474  0.99704456  1.0075293 ]\n",
      "Loss: 885.1299268164763\n",
      "\n",
      "Gradient: [  -23.82995292   -50.68580681   -17.90720625   107.25248795\n",
      "   -22.70041889   -84.06633053  -131.51742862    60.14257414\n",
      "    47.10991381 -1133.11198495  2914.51836185]\n",
      "Old weights: [ 0.99993835  0.99902622  0.99913988 -0.31719672  0.99988807  0.99968096\n",
      "  0.99856681  0.34624854  0.33655474  0.99704456  1.0075293 ]\n",
      "New weights: [ 0.99994073  0.99903129  0.99914167 -0.31720745  0.99989034  0.99968937\n",
      "  0.99857996  0.34624253  0.33655003  0.99715787  1.00723785]\n",
      "Loss: 884.1477997255645\n",
      "\n",
      "Gradient: [  -23.81673245   -50.65754143   -17.89703848   106.58584307\n",
      "   -22.68677203   -84.01990843  -131.44378975    61.06446495\n",
      "    45.52137812 -1132.4834061   2912.90155592]\n",
      "Old weights: [ 0.99994073  0.99903129  0.99914167 -0.31720745  0.99989034  0.99968937\n",
      "  0.99857996  0.34624253  0.33655003  0.99715787  1.00723785]\n",
      "New weights: [ 0.99994311  0.99903636  0.99914346 -0.3172181   0.99989261  0.99969777\n",
      "  0.9985931   0.34623642  0.33654548  0.99727112  1.00694656]\n",
      "Loss: 883.1667775138012\n",
      "\n",
      "Gradient: [  -23.80351747   -50.62925677   -17.88686078   105.95356984\n",
      "   -22.6731718    -83.97347668  -131.37019042    61.93234796\n",
      "    44.02122188 -1131.8550851   2911.2854147 ]\n",
      "Old weights: [ 0.99994311  0.99903636  0.99914346 -0.3172181   0.99989261  0.99969777\n",
      "  0.9985931   0.34623642  0.33654548  0.99727112  1.00694656]\n",
      "New weights: [ 0.99994549  0.99904142  0.99914525 -0.3172287   0.99989488  0.99970617\n",
      "  0.99860624  0.34623023  0.33654107  0.99738431  1.00665543]\n",
      "Loss: 882.1868575719496\n",
      "\n",
      "Gradient: [  -23.79030809   -50.60095481   -17.87667404   105.35373083\n",
      "   -22.659616     -83.92703727  -131.29663074    62.74923688\n",
      "    42.60449396 -1131.22702723  2909.66995167]\n",
      "Old weights: [ 0.99994549  0.99904142  0.99914525 -0.3172287   0.99989488  0.99970617\n",
      "  0.99860624  0.34623023  0.33654107  0.99738431  1.00665543]\n",
      "New weights: [ 0.99994787  0.99904648  0.99914704 -0.31723923  0.99989714  0.99971456\n",
      "  0.99861937  0.34622395  0.33653681  0.99749743  1.00636446]\n",
      "Loss: 881.2080374439647\n",
      "\n",
      "Gradient: [  -23.77710439   -50.57263739   -17.86647907   104.78452306\n",
      "   -22.64610253   -83.88059207  -131.2231107     63.51799896\n",
      "    41.2665241  -1130.59923694  2908.05517821]\n",
      "Old weights: [ 0.99994787  0.99904648  0.99914704 -0.31723923  0.99989714  0.99971456\n",
      "  0.99861937  0.34622395  0.33653681  0.99749743  1.00636446]\n",
      "New weights: [ 0.99995025  0.99905154  0.99914882 -0.31724971  0.99989941  0.99972295\n",
      "  0.99863249  0.3462176   0.33653269  0.99761049  1.00607366]\n",
      "Loss: 880.2303148104677\n",
      "\n",
      "Gradient: [  -23.76390648   -50.54430629   -17.85627667   104.24422298\n",
      "   -22.63262942   -83.83414287  -131.14963041    64.24132236\n",
      "    40.00290062 -1129.97171885  2906.44110614]\n",
      "Old weights: [ 0.99995025  0.99905154  0.99914882 -0.31724971  0.99989941  0.99972295\n",
      "  0.99863249  0.3462176   0.33653269  0.99761049  1.00607366]\n",
      "New weights: [ 0.99995263  0.99905659  0.99915061 -0.31726014  0.99990167  0.99973133\n",
      "  0.99864561  0.34621118  0.33652869  0.99772349  1.00578301]\n",
      "Loss: 879.253687474017\n",
      "\n",
      "Gradient: [  -23.75071443   -50.51596315   -17.84606757   103.73122144\n",
      "   -22.61919481   -83.78769134  -131.07618986    64.92176007\n",
      "    38.80946137 -1129.34447697  2904.82774567]\n",
      "Old weights: [ 0.99995263  0.99905659  0.99915061 -0.31726014  0.99990167  0.99973133\n",
      "  0.99864561  0.34621118  0.33652869  0.99772349  1.00578301]\n",
      "New weights: [ 0.999955    0.99906164  0.99915239 -0.31727051  0.99990393  0.99973971\n",
      "  0.99865871  0.34620468  0.3365248   0.99783642  1.00549253]\n",
      "Loss: 878.2781533459722\n",
      "\n",
      "Gradient: [  -23.73752832   -50.48760954   -17.83585247   103.24398457\n",
      "   -22.60579695   -83.74123905  -131.00278913    65.56170933\n",
      "    37.68227523 -1128.71751535  2903.21510718]\n",
      "Old weights: [ 0.999955    0.99906164  0.99915239 -0.31727051  0.99990393  0.99973971\n",
      "  0.99865871  0.34620468  0.3365248   0.99783642  1.00549253]\n",
      "New weights: [ 0.99995738  0.99906669  0.99915418 -0.31728084  0.99990619  0.99974808\n",
      "  0.99867181  0.34619813  0.33652104  0.99794929  1.00520221]\n",
      "Loss: 877.3037104347967\n",
      "\n",
      "Gradient: [  -23.72434823   -50.45924693   -17.82563202   102.7810767\n",
      "   -22.59243416   -83.69478749  -130.92942822    66.16344374\n",
      "    36.61763296 -1128.09083757  2901.6031998 ]\n",
      "Old weights: [ 0.99995738  0.99906669  0.99915418 -0.31728084  0.99990619  0.99974808\n",
      "  0.99867181  0.34619813  0.33652104  0.99794929  1.00520221]\n",
      "New weights: [ 0.99995975  0.99907174  0.99915596 -0.31729111  0.99990845  0.99975645\n",
      "  0.99868491  0.34619151  0.33651737  0.9980621   1.00491205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 876.3303568356322\n",
      "\n",
      "Gradient: [  -23.71117422   -50.43087672   -17.81540685   102.34113236\n",
      "   -22.57910489   -83.64833808  -130.8561072     66.72910076\n",
      "    35.6120316  -1127.46444719  2899.99203264]\n",
      "Old weights: [ 0.99995975  0.99907174  0.99915596 -0.31729111  0.99990845  0.99975645\n",
      "  0.99868491  0.34619151  0.33651737  0.9980621   1.00491205]\n",
      "New weights: [ 0.99996212  0.99907678  0.99915774 -0.31730135  0.99991071  0.99976482\n",
      "  0.99869799  0.34618484  0.33651381  0.99817485  1.00462205]\n",
      "Loss: 875.3580907209925\n",
      "\n",
      "Gradient: [  -23.69800637   -50.40250021   -17.80517754   101.92287103\n",
      "   -22.56580766   -83.60189213  -130.78282608    67.26070545\n",
      "    34.66216557 -1126.83834741  2898.38161385]\n",
      "Old weights: [ 0.99996212  0.99907678  0.99915774 -0.31730135  0.99991071  0.99976482\n",
      "  0.99869799  0.34618484  0.33651381  0.99817485  1.00462205]\n",
      "New weights: [ 0.99996449  0.99908182  0.99915952 -0.31731154  0.99991297  0.99977318\n",
      "  0.99871107  0.34617811  0.33651035  0.99828753  1.00433221]\n",
      "Loss: 874.3869103325129\n",
      "\n",
      "Gradient: [  -23.68484474   -50.37411865   -17.79494464   101.52507684\n",
      "   -22.55254106   -83.5554509   -130.70958489    67.76016342\n",
      "    33.76491342 -1126.21254135  2896.77195145]\n",
      "Old weights: [ 0.99996449  0.99908182  0.99915952 -0.31731154  0.99991297  0.99977318\n",
      "  0.99871107  0.34617811  0.33651035  0.99828753  1.00433221]\n",
      "New weights: [ 0.99996686  0.99908686  0.9991613  -0.31732169  0.99991522  0.99978153\n",
      "  0.99872414  0.34617134  0.33650697  0.99840015  1.00404254]\n",
      "Loss: 873.4168139735455\n",
      "\n",
      "Gradient: [  -23.67168938   -50.34573321   -17.78470868   101.14660788\n",
      "   -22.53930379   -83.50901556  -130.63638364    68.22927856\n",
      "    32.91732932 -1125.58703186  2895.16305269]\n",
      "Old weights: [ 0.99996686  0.99908686  0.9991613  -0.31732169  0.99991522  0.99978153\n",
      "  0.99872414  0.34617134  0.33650697  0.99840015  1.00404254]\n",
      "New weights: [ 0.99996923  0.99909189  0.99916308 -0.31733181  0.99991747  0.99978988\n",
      "  0.99873721  0.34616451  0.33650368  0.99851271  1.00375302]\n",
      "Loss: 872.4478000025861\n",
      "\n",
      "Gradient: [  -23.65854036   -50.317345     -17.77447014   100.78638132\n",
      "   -22.52609459   -83.46258724  -130.56322238    68.66974957\n",
      "    32.11663175 -1124.9618217   2893.55492463]\n",
      "Old weights: [ 0.99996923  0.99909189  0.99916308 -0.31733181  0.99991747  0.99978988\n",
      "  0.99873721  0.34616451  0.33650368  0.99851271  1.00375302]\n",
      "New weights: [ 0.99997159  0.99909692  0.99916486 -0.31734189  0.99991973  0.99979823\n",
      "  0.99875026  0.34615765  0.33650047  0.99862521  1.00346366]\n",
      "Loss: 871.4798668274409\n",
      "\n",
      "Gradient: [  -23.64539771   -50.28895504   -17.7642295    100.44337902\n",
      "   -22.51291229   -83.41616697  -130.49010111    69.08318351\n",
      "    31.36019551 -1124.33691339  2891.9475737 ]\n",
      "Old weights: [ 0.99997159  0.99909692  0.99916486 -0.31734189  0.99991973  0.99979823\n",
      "  0.99875026  0.34615765  0.33650047  0.99862521  1.00346366]\n",
      "New weights: [ 0.99997396  0.99910195  0.99916663 -0.31735193  0.99992198  0.99980657\n",
      "  0.99876331  0.34615074  0.33649733  0.99873764  1.00317447]\n",
      "Loss: 870.5130128999555\n",
      "\n",
      "Gradient: [  -23.63226149   -50.26056433   -17.75398718   100.11663649\n",
      "   -22.49975578   -83.36975576  -130.41701985    69.47109453\n",
      "    30.64554196 -1123.71230936  2890.34100611]\n",
      "Old weights: [ 0.99997396  0.99910195  0.99916663 -0.31735193  0.99992198  0.99980657\n",
      "  0.99876331  0.34615074  0.33649733  0.99873764  1.00317447]\n",
      "New weights: [ 0.99997632  0.99910698  0.99916841 -0.31736194  0.99992423  0.99981491\n",
      "  0.99877635  0.34614379  0.33649427  0.99885001  1.00288543]\n",
      "Loss: 869.5472367114099\n",
      "\n",
      "Gradient: [  -23.61913175   -50.2321738    -17.74374361    99.80524603\n",
      "   -22.48662402   -83.32335453  -130.34397862    69.83491438\n",
      "    29.97033166 -1123.08801184  2888.73522756]\n",
      "Old weights: [ 0.99997632  0.99910698  0.99916841 -0.31736194  0.99992423  0.99981491\n",
      "  0.99877635  0.34614379  0.33649427  0.99885001  1.00288543]\n",
      "New weights: [ 0.99997868  0.999112    0.99917018 -0.31737192  0.99992648  0.99982324\n",
      "  0.99878939  0.34613681  0.33649127  0.99896232  1.00259656]\n",
      "Loss: 868.5825367883449\n",
      "\n",
      "Gradient: [  -23.60600853   -50.20378431   -17.73349918    99.50834849\n",
      "   -22.473516     -83.27696416  -130.27097743    70.17599265\n",
      "    29.33235585 -1122.46402297  2887.13024352]\n",
      "Old weights: [ 0.99997868  0.999112    0.99917018 -0.31737192  0.99992648  0.99982324\n",
      "  0.99878939  0.34613681  0.33649127  0.99896232  1.00259656]\n",
      "New weights: [ 0.99998104  0.99911702  0.99917196 -0.31738187  0.99992872  0.99983157\n",
      "  0.99880241  0.34612979  0.33648834  0.99907457  1.00230785]\n",
      "Loss: 867.6189116888742\n",
      "\n",
      "Gradient: [  -23.59289187   -50.17539669   -17.72325424    99.22513473\n",
      "   -22.4604308    -83.23058549  -130.19801628    70.49560499\n",
      "    28.72952974 -1121.84034472  2885.52605902]\n",
      "Old weights: [ 0.99998104  0.99911702  0.99917196 -0.31738187  0.99992872  0.99983157\n",
      "  0.99880241  0.34612979  0.33648834  0.99907457  1.00230785]\n",
      "New weights: [ 0.9999834   0.99912204  0.99917373 -0.3173918   0.99993097  0.99983989\n",
      "  0.99881543  0.34612274  0.33648546  0.99918675  1.0020193 ]\n",
      "Loss: 866.6563599993734\n",
      "\n",
      "Gradient: [  -23.57978181   -50.14701172   -17.71300915    98.95483934\n",
      "   -22.44736754   -83.1842193   -130.12509519    70.79495425\n",
      "    28.15988509 -1121.21697897  2883.92267887]\n",
      "Old weights: [ 0.9999834   0.99912204  0.99917373 -0.3173918   0.99993097  0.99983989\n",
      "  0.99881543  0.34612274  0.33648546  0.99918675  1.0020193 ]\n",
      "New weights: [ 0.99998576  0.99912705  0.9991755  -0.31740169  0.99993322  0.99984821\n",
      "  0.99882845  0.34611566  0.33648265  0.99929888  1.0017309 ]\n",
      "Loss: 865.6948803315739\n",
      "\n",
      "Gradient: [  -23.56667838   -50.11863014   -17.70276424    98.69674109\n",
      "   -22.43432538   -83.13786632  -130.05221417    71.07517704\n",
      "    27.62156405 -1120.59392747  2882.32010752]\n",
      "Old weights: [ 0.99998576  0.99912705  0.9991755  -0.31740169  0.99993322  0.99984821\n",
      "  0.99882845  0.34611566  0.33648265  0.99929888  1.0017309 ]\n",
      "New weights: [ 0.99998812  0.99913207  0.99917727 -0.31741156  0.99993546  0.99985652\n",
      "  0.99884145  0.34610855  0.33647989  0.99941093  1.00144267]\n",
      "Loss: 864.7344713199378\n",
      "\n",
      "Gradient: [  -23.55358161   -50.09025263   -17.69251982    98.45015804\n",
      "   -22.42130354   -83.09152726  -129.97937322    71.33734539\n",
      "    27.11281265 -1119.97119187  2880.71834919]\n",
      "Old weights: [ 0.99998812  0.99913207  0.99917727 -0.31741156  0.99993546  0.99985652\n",
      "  0.99884145  0.34610855  0.33647989  0.99941093  1.00144267]\n",
      "New weights: [ 0.99999047  0.99913708  0.99917904 -0.31742141  0.9999377   0.99986483\n",
      "  0.99885445  0.34610142  0.33647717  0.99952293  1.0011546 ]\n",
      "Loss: 863.7751316193009\n",
      "\n",
      "Gradient: [  -23.54049154   -50.06187985   -17.68227617    98.21444737\n",
      "   -22.40830127   -83.04520277  -129.90657234    71.58247206\n",
      "    26.63197531 -1119.34877369  2879.1174078 ]\n",
      "Old weights: [ 0.99999047  0.99913708  0.99917904 -0.31742141  0.9999377   0.99986483\n",
      "  0.99885445  0.34610142  0.33647717  0.99952293  1.0011546 ]\n",
      "New weights: [ 0.99999283  0.99914208  0.99918081 -0.31743123  0.99993994  0.99987314\n",
      "  0.99886744  0.34609426  0.33647451  0.99963487  1.00086669]\n",
      "Loss: 862.8168599028411\n",
      "\n",
      "Gradient: [  -23.5274082    -50.03351243   -17.67203358    97.98900147\n",
      "   -22.39531787   -82.99889348  -129.83381153    71.81151244\n",
      "    26.17748903 -1118.7266744   2877.51728703]\n",
      "Old weights: [ 0.99999283  0.99914208  0.99918081 -0.31743123  0.99993994  0.99987314\n",
      "  0.99886744  0.34609426  0.33647451  0.99963487  1.00086669]\n",
      "New weights: [ 0.99999518  0.99914708  0.99918258 -0.31744103  0.99994218  0.99988144\n",
      "  0.99888042  0.34608708  0.33647189  0.99974674  1.00057894]\n",
      "Loss: 861.8596548601721\n",
      "\n",
      "Gradient: [  -23.51433161   -50.00515093   -17.66179229    97.77324741\n",
      "   -22.38235268   -82.95259995  -129.76109081    72.02536897\n",
      "    25.74787845 -1118.10489532  2875.91799035]\n",
      "Old weights: [ 0.99999518  0.99914708  0.99918258 -0.31744103  0.99994218  0.99988144\n",
      "  0.99888042  0.34608708  0.33647189  0.99974674  1.00057894]\n",
      "New weights: [ 0.99999753  0.99915209  0.99918434 -0.3174508   0.99994442  0.99988973\n",
      "  0.9988934   0.34607988  0.33646932  0.99985855  1.00029134]\n",
      "Loss: 860.9035151957511\n",
      "\n",
      "Gradient: [  -23.5012618    -49.9767959    -17.65155257    97.56664382\n",
      "   -22.36940507   -82.90632275  -129.68841016    72.22489309\n",
      "    25.34175074 -1117.48343775  2874.31952098]\n",
      "Old weights: [ 0.99999753  0.99915209  0.99918434 -0.3174508   0.99994442  0.99988973\n",
      "  0.9988934   0.34607988  0.33646932  0.99985855  1.00029134]\n",
      "New weights: [ 0.99999988  0.99915708  0.99918611 -0.31746056  0.99994666  0.99989802\n",
      "  0.99890637  0.34607266  0.33646678  0.9999703   1.00000391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 859.9484396273634\n",
      "\n",
      "Gradient: [  -23.48819879   -49.94844787   -17.64131463    97.36868008\n",
      "   -22.35647446   -82.8600624   -129.61576959    72.41088893\n",
      "    24.95779115 -1116.86230285  2872.72188194]\n",
      "Old weights: [ 0.99999988  0.99915708  0.99918611 -0.31746056  0.99994666  0.99989802\n",
      "  0.99890637  0.34607266  0.33646678  0.9999703   1.00000391]\n",
      "New weights: [ 1.00000223  0.99916208  0.99918787 -0.3174703   0.99994889  0.99990631\n",
      "  0.99891933  0.34606541  0.33646429  1.00008198  0.99971664]\n",
      "Loss: 858.9944268848154\n",
      "\n",
      "Gradient: [  -23.47514261   -49.92010733   -17.63107869    97.17887379\n",
      "   -22.34356028   -82.81381938  -129.54316909    72.58411529\n",
      "    24.5947585  -1116.24149174  2871.12507607]\n",
      "Old weights: [ 1.00000223  0.99916208  0.99918787 -0.3174703   0.99994889  0.99990631\n",
      "  0.99891933  0.34606541  0.33646429  1.00008198  0.99971664]\n",
      "New weights: [ 1.00000458  0.99916707  0.99918963 -0.31748001  0.99995113  0.99991459\n",
      "  0.99893228  0.34605816  0.33646183  1.00019361  0.99942953]\n",
      "Loss: 858.041475708808\n",
      "\n",
      "Gradient: [  -23.46209327   -49.89177472   -17.62084497    96.99676989\n",
      "   -22.33066202   -82.76759415  -129.47060865    72.74528871\n",
      "    24.25148118 -1115.62100544  2869.52910601]\n",
      "Old weights: [ 1.00000458  0.99916707  0.99918963 -0.31748001  0.99995113  0.99991459\n",
      "  0.99893228  0.34605816  0.33646183  1.00019361  0.99942953]\n",
      "New weights: [ 1.00000692  0.99917206  0.9991914  -0.31748971  0.99995336  0.99992287\n",
      "  0.99894523  0.34605088  0.3364594   1.00030517  0.99914257]\n",
      "Loss: 857.0895848498516\n",
      "\n",
      "Gradient: [  -23.4490508    -49.86345049   -17.61061365    96.82193853\n",
      "   -22.31777916   -82.72138714  -129.39808828    72.8950854\n",
      "    23.92685313 -1115.00084493  2867.93397421]\n",
      "Old weights: [ 1.00000692  0.99917206  0.9991914  -0.31748971  0.99995336  0.99992287\n",
      "  0.99894523  0.34605088  0.3364594   1.00030517  0.99914257]\n",
      "New weights: [ 1.00000927  0.99917705  0.99919316 -0.3174994   0.99995559  0.99993114\n",
      "  0.99895817  0.34604359  0.33645701  1.00041667  0.99885578]\n",
      "Loss: 856.1387530673335\n",
      "\n",
      "Gradient: [  -23.43601521   -49.83513504   -17.60038492    96.65397415\n",
      "   -22.30491124   -82.67519878  -129.32560797    73.03414388\n",
      "    23.61983027 -1114.38101111  2866.33968298]\n",
      "Old weights: [ 1.00000927  0.99917705  0.99919316 -0.3174994   0.99995559  0.99993114\n",
      "  0.99895817  0.34604359  0.33645701  1.00041667  0.99885578]\n",
      "New weights: [ 1.00001161  0.99918203  0.99919492 -0.31750906  0.99995782  0.99993941\n",
      "  0.9989711   0.34603629  0.33645465  1.00052811  0.99856915]\n",
      "Loss: 855.188979128748\n",
      "\n",
      "Gradient: [  -23.42298652   -49.80682877   -17.59015895    96.49249374\n",
      "   -22.29205782   -82.62902943  -129.25316771    73.16306676\n",
      "    23.32942698 -1113.76150481  2864.74623447]\n",
      "Old weights: [ 1.00001161  0.99918203  0.99919492 -0.31750906  0.99995782  0.99993941\n",
      "  0.9989711   0.34603629  0.33645465  1.00052811  0.99856915]\n",
      "New weights: [ 1.00001395  0.99918701  0.99919668 -0.31751871  0.99996005  0.99994767\n",
      "  0.99898403  0.34602897  0.33645232  1.00063949  0.99828267]\n",
      "Loss: 854.2402618089006\n",
      "\n",
      "Gradient: [  -23.40996475   -49.77853202   -17.5799359     96.33713585\n",
      "   -22.27921848   -82.58287947  -129.1807675     73.28242301\n",
      "    23.05471284 -1113.14232682  2863.15363066]\n",
      "Old weights: [ 1.00001395  0.99918701  0.99919668 -0.31751871  0.99996005  0.99994767\n",
      "  0.99898403  0.34602897  0.33645232  1.00063949  0.99828267]\n",
      "New weights: [ 1.00001629  0.99919199  0.99919843 -0.31752834  0.99996228  0.99995593\n",
      "  0.99899695  0.34602164  0.33645001  1.0007508   0.99799636]\n",
      "Loss: 853.2925998892562\n",
      "\n",
      "Gradient: [  -23.39694991   -49.75024515   -17.56971593    96.18755919\n",
      "   -22.26639282   -82.53674923  -129.10840732    73.39274966\n",
      "    22.79480954 -1112.52347787  2861.56187342]\n",
      "Old weights: [ 1.00001629  0.99919199  0.99919843 -0.31752834  0.99996228  0.99995593\n",
      "  0.99899695  0.34602164  0.33645001  1.0007508   0.99799636]\n",
      "New weights: [ 1.00001863  0.99919696  0.99920019 -0.31753796  0.99996451  0.99996418\n",
      "  0.99900986  0.3460143   0.33644773  1.00086205  0.9977102 ]\n",
      "Loss: 852.3459921573944\n",
      "\n",
      "Gradient: [  -23.38394202   -49.72196848   -17.55949918    96.04344166\n",
      "   -22.25358047   -82.49063904  -129.03608716    73.49455372\n",
      "    22.54888795 -1111.90495862  2859.97096448]\n",
      "Old weights: [ 1.00001863  0.99919696  0.99920019 -0.31753796  0.99996451  0.99996418\n",
      "  0.99900986  0.3460143   0.33644773  1.00086205  0.9977102 ]\n",
      "New weights: [ 1.00002097  0.99920193  0.99920195 -0.31754757  0.99996673  0.99997243\n",
      "  0.99902276  0.34600696  0.33644548  1.00097324  0.9974242 ]\n",
      "Loss: 851.4004374064565\n",
      "\n",
      "Gradient: [  -23.37094108   -49.69370231   -17.54928578    95.90447913\n",
      "   -22.24078108   -82.44454921  -128.96380703    73.58831376\n",
      "    22.31616537 -1111.28676972  2858.38090543]\n",
      "Old weights: [ 1.00002097  0.99920193  0.99920195 -0.31754757  0.99996673  0.99997243\n",
      "  0.99902276  0.34600696  0.33644548  1.00097324  0.9974242 ]\n",
      "New weights: [ 1.00002331  0.9992069   0.9992037  -0.31755716  0.99996896  0.99998068\n",
      "  0.99903566  0.3459996   0.33644324  1.00108437  0.99713837]\n",
      "Loss: 850.4559344346716\n",
      "\n",
      "Gradient: [  -23.35794711   -49.66544693   -17.53907587    95.77038453\n",
      "   -22.2279943    -82.39848     -128.89156689    73.67448159\n",
      "    22.09590294 -1110.66891174  2856.79169776]\n",
      "Old weights: [ 1.00002331  0.9992069   0.9992037  -0.31755716  0.99996896  0.99998068\n",
      "  0.99903566  0.3459996   0.33644324  1.00108437  0.99713837]\n",
      "New weights: [ 1.00002565  0.99921187  0.99920546 -0.31756674  0.99997118  0.99998892\n",
      "  0.99904855  0.34599223  0.33644104  1.00119544  0.99685269]\n",
      "Loss: 849.51248204496\n",
      "\n",
      "Gradient: [  -23.34496012   -49.63720261   -17.52886957    95.64088687\n",
      "   -22.21521984   -82.3524317   -128.81936675    73.75348368\n",
      "    21.88740319 -1110.05138523  2855.20334285]\n",
      "Old weights: [ 1.00002565  0.99921187  0.99920546 -0.31756674  0.99997118  0.99998892\n",
      "  0.99904855  0.34599223  0.33644104  1.00119544  0.99685269]\n",
      "New weights: [ 1.00002798  0.99921683  0.99920721 -0.3175763   0.9999734   0.99999715\n",
      "  0.99906143  0.34598485  0.33643885  1.00130644  0.99656717]\n",
      "Loss: 848.5700790445682\n",
      "\n",
      "Gradient: [  -23.33198012   -49.6089696    -17.51866698    95.51573033\n",
      "   -22.20245738   -82.30640456  -128.7472066     73.82572265\n",
      "    21.69000768 -1109.43419067  2853.61584196]\n",
      "Old weights: [ 1.00002798  0.99921683  0.99920721 -0.3175763   0.9999734   0.99999715\n",
      "  0.99906143  0.34598485  0.33643885  1.00130644  0.99656717]\n",
      "New weights: [ 1.00003031  0.99922179  0.99920896 -0.31758585  0.99997562  1.00000538\n",
      "  0.9990743   0.34597747  0.33643668  1.00141739  0.9962818 ]\n",
      "Loss: 847.6287242447193\n",
      "\n",
      "Gradient: [  -23.31900712   -49.58074815   -17.50846822    95.39467341\n",
      "   -22.18970665   -82.26039882  -128.67508641    73.89157853\n",
      "    21.50309487 -1108.81732853  2852.02919627]\n",
      "Old weights: [ 1.00003031  0.99922179  0.99920896 -0.31758585  0.99997562  1.00000538\n",
      "  0.9990743   0.34597747  0.33643668  1.00141739  0.9962818 ]\n",
      "New weights: [ 1.00003264  0.99922675  0.99921071 -0.31759539  0.99997784  1.00001361\n",
      "  0.99908717  0.34597008  0.33643453  1.00152827  0.9959966 ]\n",
      "Loss: 846.6884164603301\n",
      "\n",
      "Gradient: [  -23.30604113   -49.55253847   -17.49827339    95.27748814\n",
      "   -22.17696738   -82.21441469  -128.60300618    73.95141012\n",
      "    21.32607802 -1108.20079923  2850.44340685]\n",
      "Old weights: [ 1.00003264  0.99922675  0.99921071 -0.31759539  0.99997784  1.00001361\n",
      "  0.99908717  0.34597008  0.33643453  1.00152827  0.9959966 ]\n",
      "New weights: [ 1.00003498  0.99923171  0.99921246 -0.31760492  0.99998006  1.00002183\n",
      "  0.99910003  0.34596269  0.33643239  1.00163909  0.99571156]\n",
      "Loss: 845.7491545097636\n",
      "\n",
      "Gradient: [  -23.29308215   -49.52434079   -17.48808259    95.16395931\n",
      "   -22.16423932   -82.16845239  -128.5309659     74.00555607\n",
      "    21.15840324 -1107.58460317  2848.8584747 ]\n",
      "Old weights: [ 1.00003498  0.99923171  0.99921246 -0.31760492  0.99998006  1.00002183\n",
      "  0.99910003  0.34596269  0.33643239  1.00163909  0.99571156]\n",
      "New weights: [ 1.0000373   0.99923666  0.99921421 -0.31761443  0.99998227  1.00003005\n",
      "  0.99911288  0.34595529  0.33643028  1.00174985  0.99542667]\n",
      "Loss: 844.8109372145501\n",
      "\n",
      "Gradient: [  -23.28013019   -49.4961553    -17.4778959     95.05388378\n",
      "   -22.15152223   -82.12251211  -128.45896554    74.0543361\n",
      "    20.99954768 -1106.96874069  2847.27440072]\n",
      "Old weights: [ 1.0000373   0.99923666  0.99921421 -0.31761443  0.99998227  1.00003005\n",
      "  0.99911288  0.34595529  0.33643028  1.00174985  0.99542667]\n",
      "New weights: [ 1.00003963  0.99924161  0.99921596 -0.31762394  0.99998449  1.00003826\n",
      "  0.99912573  0.34594788  0.33642818  1.00186054  0.99514194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 843.8737633992267\n",
      "\n",
      "Gradient: [  -23.26718526   -49.4679822    -17.4677134     94.94706976\n",
      "   -22.1388159    -82.07659405  -128.3870051     74.09805201\n",
      "    20.84901775 -1106.35321212  2845.69118576]\n",
      "Old weights: [ 1.00003963  0.99924161  0.99921596 -0.31762394  0.99998449  1.00003826\n",
      "  0.99912573  0.34594788  0.33642818  1.00186054  0.99514194]\n",
      "New weights: [ 1.00004196  0.99924656  0.9992177  -0.31763343  0.9999867   1.00004647\n",
      "  0.99913857  0.34594047  0.33642609  1.00197118  0.99485738]\n",
      "Loss: 842.9376318911279\n",
      "\n",
      "Gradient: [  -23.25424737   -49.43982165   -17.45753519    94.84333621\n",
      "   -22.1261201    -82.03069838  -128.31508455    74.13698871\n",
      "    20.7063475  -1105.73801777  2844.10883055]\n",
      "Old weights: [ 1.00004196  0.99924656  0.9992177  -0.31763343  0.9999867   1.00004647\n",
      "  0.99913857  0.34594047  0.33642609  1.00197118  0.99485738]\n",
      "New weights: [ 1.00004428  0.9992515   0.99921945 -0.31764292  0.99998891  1.00005467\n",
      "  0.9991514   0.34593306  0.33642402  1.00208175  0.99457296]\n",
      "Loss: 842.0025415202307\n",
      "\n",
      "Gradient: [  -23.24131651   -49.41167383   -17.44736133    94.74251224\n",
      "   -22.11343463   -81.98482525  -128.2432039     74.17141514\n",
      "    20.5710971  -1105.1231579   2842.52733579]\n",
      "Old weights: [ 1.00004428  0.9992515   0.99921945 -0.31764292  0.99998891  1.00005467\n",
      "  0.9991514   0.34593306  0.33642402  1.00208175  0.99457296]\n",
      "New weights: [ 1.00004661  0.99925644  0.9992212  -0.31765239  0.99999113  1.00006287\n",
      "  0.99916422  0.34592564  0.33642197  1.00219226  0.99428871]\n",
      "Loss: 841.0684911189909\n",
      "\n",
      "Gradient: [  -23.22839271   -49.38353889   -17.4371919     94.64443651\n",
      "   -22.10075932   -81.93897484  -128.17136311    74.20158518\n",
      "    20.44285133 -1104.50863275  2840.9467021 ]\n",
      "Old weights: [ 1.00004661  0.99925644  0.9992212  -0.31765239  0.99999113  1.00006287\n",
      "  0.99916422  0.34592564  0.33642197  1.00219226  0.99428871]\n",
      "New weights: [ 1.00004893  0.99926138  0.99922294 -0.31766186  0.99999334  1.00007106\n",
      "  0.99917704  0.34591822  0.33641992  1.00230272  0.99400462]\n",
      "Loss: 840.1354795222302\n",
      "\n",
      "Gradient: [  -23.21547595   -49.35541699   -17.42702695    94.54895672\n",
      "   -22.08809397   -81.89314727  -128.09956218    74.2277385\n",
      "    20.32121822 -1103.89444255  2839.36693005]\n",
      "Old weights: [ 1.00004893  0.99926138  0.99922294 -0.31766186  0.99999334  1.00007106\n",
      "  0.99917704  0.34591822  0.33641992  1.00230272  0.99400462]\n",
      "New weights: [ 1.00005125  0.99926632  0.99922468 -0.31767131  0.99999554  1.00007925\n",
      "  0.99918985  0.3459108   0.33641789  1.00241311  0.99372068]\n",
      "Loss: 839.2035055670057\n",
      "\n",
      "Gradient: [  -23.20256624   -49.32730826   -17.41686656    94.45592911\n",
      "   -22.07543843   -81.84734268  -128.02780108    74.25010133\n",
      "    20.20582778 -1103.2805875   2837.78802013]\n",
      "Old weights: [ 1.00005125  0.99926632  0.99922468 -0.31767131  0.99999554  1.00007925\n",
      "  0.99918985  0.3459108   0.33641789  1.00241311  0.99372068]\n",
      "New weights: [ 1.00005357  0.99927125  0.99922642 -0.31768076  0.99999775  1.00008744\n",
      "  0.99920265  0.34590337  0.33641587  1.00252343  0.9934369 ]\n",
      "Loss: 838.2725680924985\n",
      "\n",
      "Gradient: [  -23.18966359   -49.29921282   -17.40671078    94.36521796\n",
      "   -22.06279252   -81.80156121  -127.9560798     74.26888725\n",
      "    20.0963307  -1102.66706776  2836.20997278]\n",
      "Old weights: [ 1.00005357  0.99927125  0.99922642 -0.31768076  0.99999775  1.00008744\n",
      "  0.99920265  0.34590337  0.33641587  1.00252343  0.9934369 ]\n",
      "New weights: [ 1.00005589  0.99927618  0.99922816 -0.31769019  0.99999996  1.00009562\n",
      "  0.99921545  0.34589595  0.33641386  1.0026337   0.99315328]\n",
      "Loss: 837.3426659399431\n",
      "\n",
      "Gradient: [  -23.176768     -49.27113082   -17.39655968    94.27669514\n",
      "   -22.05015611   -81.75580297  -127.88439833    74.28429788\n",
      "    19.99239726 -1102.05388349  2834.63278841]\n",
      "Old weights: [ 1.00005589  0.99927618  0.99922816 -0.31769019  0.99999996  1.00009562\n",
      "  0.99921545  0.34589595  0.33641386  1.0026337   0.99315328]\n",
      "New weights: [ 1.00005821  0.99928111  0.9992299  -0.31769962  1.00000216  1.00010379\n",
      "  0.99922824  0.34588852  0.33641186  1.00274391  0.99286982]\n",
      "Loss: 836.4137979525146\n",
      "\n",
      "Gradient: [  -23.16387947   -49.24306235   -17.38641329    94.19023972\n",
      "   -22.03752904   -81.71006808  -127.81275664    74.29652352\n",
      "    19.89371621 -1101.44103484  2833.05646735]\n",
      "Old weights: [ 1.00005821  0.99928111  0.9992299  -0.31769962  1.00000216  1.00010379\n",
      "  0.99922824  0.34588852  0.33641186  1.00274391  0.99286982]\n",
      "New weights: [ 1.00006053  0.99928603  0.99923164 -0.31770904  1.00000437  1.00011196\n",
      "  0.99924102  0.34588109  0.33640987  1.00285405  0.99258651]\n",
      "Loss: 835.4859629752789\n",
      "\n",
      "Gradient: [  -23.150998     -49.21500754   -17.37627167    94.10573753\n",
      "   -22.02491118   -81.66435664  -127.74115472    74.30574383\n",
      "    19.7999937  -1100.82852191  2831.4810099 ]\n",
      "Old weights: [ 1.00006053  0.99928603  0.99923164 -0.31770904  1.00000437  1.00011196\n",
      "  0.99924102  0.34588109  0.33640987  1.00285405  0.99258651]\n",
      "New weights: [ 1.00006284  0.99929095  0.99923338 -0.31771845  1.00000657  1.00012013\n",
      "  0.99925379  0.34587366  0.33640789  1.00296413  0.99230336]\n",
      "Loss: 834.5591598551291\n",
      "\n",
      "Gradient: [  -23.1381236    -49.18696648   -17.36613487    94.02308077\n",
      "   -22.0123024    -81.61866874  -127.66959255    74.31212842\n",
      "    19.71095236 -1100.21634481  2829.90641633]\n",
      "Old weights: [ 1.00006284  0.99929095  0.99923338 -0.31771845  1.00000657  1.00012013\n",
      "  0.99925379  0.34587366  0.33640789  1.00296413  0.99230336]\n",
      "New weights: [ 1.00006516  0.99929587  0.99923512 -0.31772785  1.00000877  1.00012829\n",
      "  0.99926656  0.34586623  0.33640592  1.00307415  0.99202037]\n",
      "Loss: 833.6333874406795\n",
      "\n",
      "Gradient: [  -23.12525627   -49.15893926   -17.35600293    93.94216772\n",
      "   -21.99970259   -81.57300448  -127.59807012    74.31583737\n",
      "    19.62633034 -1099.60450363  2828.33268684]\n",
      "Old weights: [ 1.00006516  0.99929587  0.99923512 -0.31772785  1.00000877  1.00012829\n",
      "  0.99926656  0.34586623  0.33640592  1.00307415  0.99202037]\n",
      "New weights: [ 1.00006747  0.99930079  0.99923685 -0.31773725  1.00001097  1.00013645\n",
      "  0.99927932  0.34585879  0.33640396  1.00318411  0.99173754]\n",
      "Loss: 832.7086445822785\n",
      "\n",
      "Gradient: [  -23.11239601   -49.13092599   -17.34587589    93.86290232\n",
      "   -21.98711163   -81.52736395  -127.52658739    74.31702183\n",
      "    19.54588048 -1098.99299842  2826.7598216 ]\n",
      "Old weights: [ 1.00006747  0.99930079  0.99923685 -0.31773725  1.00001097  1.00013645\n",
      "  0.99927932  0.34585879  0.33640396  1.00318411  0.99173754]\n",
      "New weights: [ 1.00006978  0.9993057   0.99923859 -0.31774663  1.00001317  1.0001446\n",
      "  0.99929207  0.34585136  0.336402    1.00329401  0.99145486]\n",
      "Loss: 831.7849301319123\n",
      "\n",
      "Gradient: [  -23.09954282   -49.10292674   -17.33575379    93.7851939\n",
      "   -21.9745294    -81.48174723  -127.45514437    74.31582446\n",
      "    19.46936944 -1098.38182926  2825.18782076]\n",
      "Old weights: [ 1.00006978  0.9993057   0.99923859 -0.31774663  1.00001317  1.0001446\n",
      "  0.99929207  0.34585136  0.336402    1.00329401  0.99145486]\n",
      "New weights: [ 1.00007209  0.99931061  0.99924032 -0.31775601  1.00001537  1.00015275\n",
      "  0.99930482  0.34584393  0.33640006  1.00340385  0.99117234]\n",
      "Loss: 830.8622429431784\n",
      "\n",
      "Gradient: [  -23.0866967    -49.07494159   -17.32563666    93.70895687\n",
      "   -21.96195582   -81.43615438  -127.38374102    74.31237994\n",
      "    19.39657693 -1097.77099617  2823.61668442]\n",
      "Old weights: [ 1.00007209  0.99931061  0.99924032 -0.31775601  1.00001537  1.00015275\n",
      "  0.99930482  0.34584393  0.33640006  1.00340385  0.99117234]\n",
      "New weights: [ 1.0000744   0.99931552  0.99924205 -0.31776538  1.00001756  1.00016089\n",
      "  0.99931756  0.3458365   0.33639812  1.00351363  0.99088998]\n",
      "Loss: 829.9405818712415\n",
      "\n",
      "Gradient: [  -23.07385765   -49.04697061   -17.31552455    93.63411044\n",
      "   -21.94939078   -81.39058549  -127.31237733    74.3068154\n",
      "    19.32729504 -1097.16049919  2822.04641264]\n",
      "Old weights: [ 1.0000744   0.99931552  0.99924205 -0.31776538  1.00001756  1.00016089\n",
      "  0.99931756  0.3458365   0.33639812  1.00351363  0.99088998]\n",
      "New weights: [ 1.00007671  0.99932042  0.99924378 -0.31777475  1.00001976  1.00016903\n",
      "  0.99933029  0.34582907  0.33639618  1.00362334  0.99060778]\n",
      "Loss: 829.0199457728143\n",
      "\n",
      "Gradient: [  -23.06102567   -49.01901389   -17.30541747    93.56057832\n",
      "   -21.93683419   -81.34504062  -127.24105329    74.29925085\n",
      "    19.26132747 -1096.55033834  2820.47700548]\n",
      "Old weights: [ 1.00007671  0.99932042  0.99924378 -0.31777475  1.00001976  1.00016903\n",
      "  0.99933029  0.34582907  0.33639618  1.00362334  0.99060778]\n",
      "New weights: [ 1.00007901  0.99932532  0.99924551 -0.3177841   1.00002195  1.00017717\n",
      "  0.99934301  0.34582164  0.33639426  1.003733    0.99032573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 828.1003335061147\n",
      "\n",
      "Gradient: [  -23.04820076   -48.99107148   -17.29531547    93.48828851\n",
      "   -21.92428596   -81.29951982  -127.16976886    74.28979958\n",
      "    19.19848893 -1095.94051361  2818.90846292]\n",
      "Old weights: [ 1.00007901  0.99932532  0.99924551 -0.3177841   1.00002195  1.00017717\n",
      "  0.99934301  0.34582164  0.33639426  1.003733    0.99032573]\n",
      "New weights: [ 1.00008132  0.99933022  0.99924724 -0.31779345  1.00002414  1.0001853\n",
      "  0.99935573  0.34581421  0.33639234  1.00384259  0.99004384]\n",
      "Loss: 827.1817439308517\n",
      "\n",
      "Gradient: [  -23.03538292   -48.96314344   -17.28521856    93.41717304\n",
      "   -21.91174601   -81.25402316  -127.09852403    74.27856855\n",
      "    19.13860449 -1095.33102501  2817.34078497]\n",
      "Old weights: [ 1.00008132  0.99933022  0.99924724 -0.31779345  1.00002414  1.0001853\n",
      "  0.99935573  0.34581421  0.33639234  1.00384259  0.99004384]\n",
      "New weights: [ 1.00008362  0.99933512  0.99924897 -0.31780279  1.00002634  1.00019342\n",
      "  0.99936844  0.34580678  0.33639042  1.00395213  0.98976211]\n",
      "Loss: 826.2641759081623\n",
      "\n",
      "Gradient: [  -23.02257216   -48.93522983   -17.27512678    93.34716774\n",
      "   -21.89921427   -81.20855068  -127.02731879    74.2656587\n",
      "    19.08150904 -1094.72187252  2815.77397157]\n",
      "Old weights: [ 1.00008362  0.99933512  0.99924897 -0.31780279  1.00002634  1.00019342\n",
      "  0.99936844  0.34580678  0.33639042  1.00395213  0.98976211]\n",
      "New weights: [ 1.00008592  0.99934001  0.9992507  -0.31781213  1.00002853  1.00020154\n",
      "  0.99938114  0.34579936  0.33638852  1.0040616   0.98948053]\n",
      "Loss: 825.3476283006422\n",
      "\n",
      "Gradient: [  -23.00976846   -48.9073307    -17.26504015    93.27821203\n",
      "   -21.88669066   -81.16310244  -126.9561531     74.25116534\n",
      "    19.02704669 -1094.1130561   2814.20802266]\n",
      "Old weights: [ 1.00008592  0.99934001  0.9992507  -0.31781213  1.00002853  1.00020154\n",
      "  0.99938114  0.34579936  0.33638852  1.0040616   0.98948053]\n",
      "New weights: [ 1.00008822  0.9993449   0.99925243 -0.31782146  1.00003071  1.00020966\n",
      "  0.99939384  0.34579193  0.33638661  1.00417101  0.98919911]\n",
      "Loss: 824.4320999722861\n",
      "\n",
      "Gradient: [  -22.99697183   -48.87944612   -17.25495869    93.21024873\n",
      "   -21.87417511   -81.11767849  -126.88502697    74.23517843\n",
      "    18.9750703  -1093.50457574  2812.64293815]\n",
      "Old weights: [ 1.00008822  0.9993449   0.99925243 -0.31782146  1.00003071  1.00020966\n",
      "  0.99939384  0.34579193  0.33638661  1.00417101  0.98919911]\n",
      "New weights: [ 1.00009052  0.99934979  0.99925415 -0.31783078  1.0000329   1.00021777\n",
      "  0.99940653  0.34578451  0.33638472  1.00428036  0.98891784]\n",
      "Loss: 823.5175897885026\n",
      "\n",
      "Gradient: [  -22.98418227   -48.85157611   -17.24488242    93.14322387\n",
      "   -21.86166756   -81.07227886  -126.81394035    74.21778291\n",
      "    18.92544096 -1092.89643138  2811.07871792]\n",
      "Old weights: [ 1.00009052  0.99934979  0.99925415 -0.31783078  1.0000329   1.00021777\n",
      "  0.99940653  0.34578451  0.33638472  1.00428036  0.98891784]\n",
      "New weights: [ 1.00009282  0.99935468  0.99925588 -0.31784009  1.00003509  1.00022588\n",
      "  0.99941921  0.34577709  0.33638282  1.00438965  0.98863674]\n",
      "Loss: 822.6040966160668\n",
      "\n",
      "Gradient: [  -22.97139978   -48.82372073   -17.23481137    93.07708648\n",
      "   -21.84916794   -81.02690359  -126.74289324    74.19905892\n",
      "    18.87802755 -1092.28862298  2809.51536184]\n",
      "Old weights: [ 1.00009282  0.99935468  0.99925588 -0.31784009  1.00003509  1.00022588\n",
      "  0.99941921  0.34577709  0.33638282  1.00438965  0.98863674]\n",
      "New weights: [ 1.00009512  0.99935956  0.9992576  -0.3178494   1.00003727  1.00023398\n",
      "  0.99943188  0.34576967  0.33638093  1.00449888  0.98835578]\n",
      "Loss: 821.6916193231242\n",
      "\n",
      "Gradient: [  -22.95862436   -48.79588002   -17.22474556    93.01178845\n",
      "   -21.83667619   -80.98155273  -126.67188561    74.17908216\n",
      "    18.83270629 -1091.68115047  2807.95286976]\n",
      "Old weights: [ 1.00009512  0.99935956  0.9992576  -0.3178494   1.00003727  1.00023398\n",
      "  0.99943188  0.34576967  0.33638093  1.00449888  0.98835578]\n",
      "New weights: [ 1.00009741  0.99936444  0.99925932 -0.3178587   1.00003946  1.00024208\n",
      "  0.99944455  0.34576225  0.33637905  1.00460805  0.98807499]\n",
      "Loss: 820.7801567791525\n",
      "\n",
      "Gradient: [  -22.94585599   -48.76805401   -17.21468499    92.94728437\n",
      "   -21.82419227   -80.93622629  -126.60091744    74.15792406\n",
      "    18.7893603  -1091.07401379  2806.3912415 ]\n",
      "Old weights: [ 1.00009741  0.99936444  0.99925932 -0.3178587   1.00003946  1.00024208\n",
      "  0.99944455  0.34576225  0.33637905  1.00460805  0.98807499]\n",
      "New weights: [ 1.00009971  0.99936932  0.99926104 -0.317868    1.00004164  1.00025017\n",
      "  0.99945721  0.34575483  0.33637717  1.00471716  0.98779435]\n",
      "Loss: 819.869707855016\n",
      "\n",
      "Gradient: [  -22.93309469   -48.74024274   -17.2046297     92.88353137\n",
      "   -21.8117161    -80.89092432  -126.52998871    74.13565206\n",
      "    18.7478793  -1090.46721287  2804.83047689]\n",
      "Old weights: [ 1.00009971  0.99936932  0.99926104 -0.317868    1.00004164  1.00025017\n",
      "  0.99945721  0.34575483  0.33637717  1.00471716  0.98779435]\n",
      "New weights: [ 1.000102    0.99937419  0.99926276 -0.31787728  1.00004382  1.00025826\n",
      "  0.99946986  0.34574742  0.3363753   1.0048262   0.98751387]\n",
      "Loss: 818.9602714228548\n",
      "\n",
      "Gradient: [  -22.92034046   -48.71244624   -17.19457968    92.82048897\n",
      "   -21.79924765   -80.84564684  -126.45909941    74.11232982\n",
      "    18.70815915 -1089.86074762  2803.27057571]\n",
      "Old weights: [ 1.000102    0.99937419  0.99926276 -0.31787728  1.00004382  1.00025826\n",
      "  0.99946986  0.34574742  0.3363753   1.0048262   0.98751387]\n",
      "New weights: [ 1.00010429  0.99937906  0.99926448 -0.31788657  1.000046    1.00026635\n",
      "  0.99948251  0.34574001  0.33637343  1.00493519  0.98723354]\n",
      "Loss: 818.0518463561859\n",
      "\n",
      "Gradient: [  -22.90759328   -48.68466455   -17.18453497    92.75811898\n",
      "   -21.78678687   -80.80039389  -126.38824951    74.08801743\n",
      "    18.67010155 -1089.25461797  2801.71153775]\n",
      "Old weights: [ 1.00010429  0.99937906  0.99926448 -0.31788657  1.000046    1.00026635\n",
      "  0.99948251  0.34574001  0.33637343  1.00493519  0.98723354]\n",
      "New weights: [ 1.00010658  0.99938393  0.9992662  -0.31789584  1.00004818  1.00027443\n",
      "  0.99949515  0.3457326   0.33637156  1.00504411  0.98695337]\n",
      "Loss: 817.1444315297933\n",
      "\n",
      "Gradient: [  -22.89485315   -48.65689769   -17.17449556    92.69638533\n",
      "   -21.77433371   -80.75516547  -126.31743899    74.06277163\n",
      "    18.6336137  -1088.64882383  2800.15336276]\n",
      "Old weights: [ 1.00010658  0.99938393  0.9992662  -0.31789584  1.00004818  1.00027443\n",
      "  0.99949515  0.3457326   0.33637156  1.00504411  0.98695337]\n",
      "New weights: [ 1.00010887  0.99938879  0.99926792 -0.31790511  1.00005036  1.0002825\n",
      "  0.99950778  0.34572519  0.3363697   1.00515298  0.98667335]\n",
      "Loss: 816.2380258197868\n",
      "\n",
      "Gradient: [  -22.88212008   -48.62914569   -17.16446148    92.63525397\n",
      "   -21.76188813   -80.70996161  -126.24666783    74.03664595\n",
      "    18.59860801 -1088.04336509  2798.59605051]\n",
      "Old weights: [ 1.00010887  0.99938879  0.99926792 -0.31790511  1.00005036  1.0002825\n",
      "  0.99950778  0.34572519  0.3363697   1.00515298  0.98667335]\n",
      "New weights: [ 1.00011116  0.99939366  0.99926964 -0.31791438  1.00005253  1.00029057\n",
      "  0.9995204   0.34571779  0.33636784  1.00526178  0.98639349]\n",
      "Loss: 815.332628103557\n",
      "\n",
      "Gradient: [  -22.86939407   -48.60140858   -17.15443274    92.57469275\n",
      "   -21.74945009   -80.66478234  -126.17593602    74.00969096\n",
      "    18.56500179 -1087.43824166  2797.03960073]\n",
      "Old weights: [ 1.00011116  0.99939366  0.99926964 -0.31791438  1.00005253  1.00029057\n",
      "  0.9995204   0.34571779  0.33636784  1.00526178  0.98639349]\n",
      "New weights: [ 1.00011345  0.99939852  0.99927135 -0.31792363  1.00005471  1.00029864\n",
      "  0.99953302  0.34571039  0.33636598  1.00537053  0.98611379]\n",
      "Loss: 814.4282372598144\n",
      "\n",
      "Gradient: [  -22.8566751    -48.57368637   -17.14440934    92.51467133\n",
      "   -21.73701954   -80.61962767  -126.10524352    73.98195437\n",
      "    18.53271696 -1086.83345342  2795.48401315]\n",
      "Old weights: [ 1.00011345  0.99939852  0.99927135 -0.31792363  1.00005471  1.00029864\n",
      "  0.99953302  0.34571039  0.33636598  1.00537053  0.98611379]\n",
      "New weights: [ 1.00011574  0.99940338  0.99927307 -0.31793288  1.00005688  1.0003067\n",
      "  0.99954563  0.34570299  0.33636413  1.00547921  0.98583424]\n",
      "Loss: 813.5248521685199\n",
      "\n",
      "Gradient: [  -22.84396319   -48.54597909   -17.13439129    92.45516108\n",
      "   -21.72459646   -80.57449761  -126.03459032    73.95348123\n",
      "    18.50167985 -1086.22900028  2793.92928748]\n",
      "Old weights: [ 1.00011574  0.99940338  0.99927307 -0.31793288  1.00005688  1.0003067\n",
      "  0.99954563  0.34570299  0.33636413  1.00547921  0.98583424]\n",
      "New weights: [ 1.00011802  0.99940823  0.99927478 -0.31794213  1.00005905  1.00031476\n",
      "  0.99955824  0.34569559  0.33636228  1.00558783  0.98555485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 812.6224717109417\n",
      "\n",
      "Gradient: [  -22.83125831   -48.51828675   -17.12437861    92.39613494\n",
      "   -21.71218081   -80.52939219  -125.9639764     73.92431406\n",
      "    18.47182089 -1085.62488212  2792.37542343]\n",
      "Old weights: [ 1.00011802  0.99940823  0.99927478 -0.31794213  1.00005905  1.00031476\n",
      "  0.99955824  0.34569559  0.33636228  1.00558783  0.98555485]\n",
      "New weights: [ 1.0001203   0.99941308  0.99927649 -0.31795137  1.00006122  1.00032281\n",
      "  0.99957083  0.3456882   0.33636043  1.0056964   0.98527561]\n",
      "Loss: 811.7210947695879\n",
      "\n",
      "Gradient: [  -22.81856049   -48.49060938   -17.11437131    92.3375674\n",
      "   -21.69977255   -80.4843114   -125.89340174    73.89449299\n",
      "    18.44307441 -1085.02109881  2790.8224207 ]\n",
      "Old weights: [ 1.0001203   0.99941308  0.99927649 -0.31795137  1.00006122  1.00032281\n",
      "  0.99957083  0.3456882   0.33636043  1.0056964   0.98527561]\n",
      "New weights: [ 1.00012258  0.99941793  0.9992782  -0.3179606   1.00006339  1.00033086\n",
      "  0.99958342  0.34568081  0.33635859  1.0058049   0.98499653]\n",
      "Loss: 810.8207202282836\n",
      "\n",
      "Gradient: [  -22.8058697    -48.46294698   -17.10436939    92.27943435\n",
      "   -21.68737165   -80.43925527  -125.82286631    73.86405593\n",
      "    18.41537841 -1084.41765025  2789.27027897]\n",
      "Old weights: [ 1.00012258  0.99941793  0.9992782  -0.3179606   1.00006339  1.00033086\n",
      "  0.99958342  0.34568081  0.33635859  1.0058049   0.98499653]\n",
      "New weights: [ 1.00012486  0.99942278  0.99927991 -0.31796983  1.00006556  1.0003389\n",
      "  0.999596    0.34567343  0.33635674  1.00591334  0.9847176 ]\n",
      "Loss: 809.9213469720692\n",
      "\n",
      "Gradient: [  -22.79318595   -48.43529959   -17.09437285    92.22171304\n",
      "   -21.67497809   -80.39422381  -125.7523701     73.83303864\n",
      "    18.38867439 -1083.81453629  2787.71899791]\n",
      "Old weights: [ 1.00012486  0.99942278  0.99927991 -0.31796983  1.00006556  1.0003389\n",
      "  0.999596    0.34567343  0.33635674  1.00591334  0.9847176 ]\n",
      "New weights: [ 1.00012714  0.99942762  0.99928162 -0.31797905  1.00006773  1.00034694\n",
      "  0.99960858  0.34566604  0.3363549   1.00602172  0.98443883]\n",
      "Loss: 809.0229738872816\n",
      "\n",
      "Gradient: [  -22.78050923   -48.4076672    -17.08438171    92.16438199\n",
      "   -21.66259183   -80.34921702  -125.68191309    73.80147489\n",
      "    18.3629071  -1083.21175681  2786.1685772 ]\n",
      "Old weights: [ 1.00012714  0.99942762  0.99928162 -0.31797905  1.00006773  1.00034694\n",
      "  0.99960858  0.34566604  0.3363549   1.00602172  0.98443883]\n",
      "New weights: [ 1.00012942  0.99943246  0.99928333 -0.31798827  1.0000699   1.00035498\n",
      "  0.99962115  0.34565866  0.33635307  1.00613004  0.98416021]\n",
      "Loss: 808.1255998615009\n",
      "\n",
      "Gradient: [  -22.76783955   -48.38004983   -17.07439598    92.10742094\n",
      "   -21.65021285   -80.3042349   -125.61149525    73.76939655\n",
      "    18.33802439 -1082.60931169  2784.61901649]\n",
      "Old weights: [ 1.00012942  0.99943246  0.99928333 -0.31798827  1.0000699   1.00035498\n",
      "  0.99962115  0.34565866  0.33635307  1.00613004  0.98416021]\n",
      "New weights: [ 1.0001317   0.9994373   0.99928504 -0.31799748  1.00007206  1.00036301\n",
      "  0.99963371  0.34565128  0.33635123  1.0062383   0.98388175]\n",
      "Loss: 807.2292237835572\n",
      "\n",
      "Gradient: [  -22.7551769    -48.3524475    -17.06441565    92.05081073\n",
      "   -21.63784112   -80.25927748  -125.54111656    73.7368337\n",
      "    18.31397703 -1082.00720078  2783.07031543]\n",
      "Old weights: [ 1.0001317   0.9994373   0.99928504 -0.31799748  1.00007206  1.00036301\n",
      "  0.99963371  0.34565128  0.33635123  1.0062383   0.98388175]\n",
      "New weights: [ 1.00013397  0.99944213  0.99928675 -0.31800669  1.00007423  1.00037103\n",
      "  0.99964626  0.34564391  0.3363494   1.0063465   0.98360344]\n",
      "Loss: 806.3338445435398\n",
      "\n",
      "Gradient: [  -22.74252127   -48.3248602    -17.05444073    91.9945333\n",
      "   -21.62547662   -80.21434474  -125.470777      73.70381476\n",
      "    18.29071855 -1081.40542394  2781.52247367]\n",
      "Old weights: [ 1.00013397  0.99944213  0.99928675 -0.31800669  1.00007423  1.00037103\n",
      "  0.99964626  0.34564391  0.3363494   1.0063465   0.98360344]\n",
      "New weights: [ 1.00013625  0.99944697  0.99928845 -0.31801588  1.00007639  1.00037906\n",
      "  0.99965881  0.34563654  0.33634757  1.00645464  0.98332529]\n",
      "Loss: 805.4394610327771\n",
      "\n",
      "Gradient: [  -22.72987267   -48.29728796   -17.04447124    91.9385716\n",
      "   -21.61311932   -80.1694367   -125.40047656    73.67036652\n",
      "    18.26820508 -1080.80398105  2779.97549085]\n",
      "Old weights: [ 1.00013625  0.99944697  0.99928845 -0.31801588  1.00007639  1.00037906\n",
      "  0.99965881  0.34563654  0.33634757  1.00645464  0.98332529]\n",
      "New weights: [ 1.00013852  0.9994518   0.99929016 -0.31802508  1.00007855  1.00038707\n",
      "  0.99967135  0.34562917  0.33634575  1.00656272  0.98304729]\n",
      "Loss: 804.5460721438524\n",
      "\n",
      "Gradient: [  -22.71723108   -48.26973078   -17.03450716    91.88290952\n",
      "   -21.60076921   -80.12455336  -125.3302152     73.63651431\n",
      "    18.24639521 -1080.20287195  2778.42936661]\n",
      "Old weights: [ 1.00013852  0.9994518   0.99929016 -0.31802508  1.00007855  1.00038707\n",
      "  0.99967135  0.34562917  0.33634575  1.00656272  0.98304729]\n",
      "New weights: [ 1.00014079  0.99945662  0.99929186 -0.31803427  1.00008071  1.00039508\n",
      "  0.99968388  0.34562181  0.33634392  1.00667074  0.98276945]\n",
      "Loss: 803.6536767705788\n",
      "\n",
      "Gradient: [  -22.70459652   -48.24218866   -17.02454851    91.82753185\n",
      "   -21.58842625   -80.07969471  -125.25999291    73.602282\n",
      "    18.22524984 -1079.60209649  2776.88410055]\n",
      "Old weights: [ 1.00014079  0.99945662  0.99929186 -0.31803427  1.00008071  1.00039508\n",
      "  0.99968388  0.34562181  0.33634392  1.00667074  0.98276945]\n",
      "New weights: [ 1.00014306  0.99946145  0.99929356 -0.31804345  1.00008287  1.00040309\n",
      "  0.99969641  0.34561445  0.3363421   1.0067787   0.98249176]\n",
      "Loss: 802.762273808035\n",
      "\n",
      "Gradient: [  -22.69196896   -48.21466161   -17.01459529    91.77242424\n",
      "   -21.57609043   -80.03486077  -125.18980967    73.56769216\n",
      "    18.20473208 -1079.00165454  2775.33969232]\n",
      "Old weights: [ 1.00014306  0.99946145  0.99929356 -0.31804345  1.00008287  1.00040309\n",
      "  0.99969641  0.34561445  0.3363421   1.0067787   0.98249176]\n",
      "New weights: [ 1.00014533  0.99946627  0.99929526 -0.31805263  1.00008503  1.0004111\n",
      "  0.99970893  0.34560709  0.33634028  1.0068866   0.98221423]\n",
      "Loss: 801.8718621524896\n",
      "\n",
      "Gradient: [  -22.67934842   -48.18714965   -17.0046475     91.71757314\n",
      "   -21.56376174   -79.99005152  -125.11966546    73.53276606\n",
      "    18.18480708 -1078.40154594  2773.79614152]\n",
      "Old weights: [ 1.00014533  0.99946627  0.99929526 -0.31805263  1.00008503  1.0004111\n",
      "  0.99970893  0.34560709  0.33634028  1.0068866   0.98221423]\n",
      "New weights: [ 1.0001476   0.99947109  0.99929696 -0.3180618   1.00008718  1.0004191\n",
      "  0.99972144  0.34559974  0.33633846  1.00699445  0.98193685]\n",
      "Loss: 800.9824407014909\n",
      "\n",
      "Gradient: [  -22.66673489   -48.15965276   -16.99470515    91.66296577\n",
      "   -21.55144014   -79.94526698  -125.04956025    73.49752381\n",
      "    18.16544196 -1077.80177055  2772.25344776]\n",
      "Old weights: [ 1.0001476   0.99947109  0.99929696 -0.3180618   1.00008718  1.0004191\n",
      "  0.99972144  0.34559974  0.33633846  1.00699445  0.98193685]\n",
      "New weights: [ 1.00014987  0.9994759   0.99929866 -0.31807097  1.00008934  1.00042709\n",
      "  0.99973394  0.34559239  0.33633664  1.00710223  0.98165962]\n",
      "Loss: 800.0940083537926\n",
      "\n",
      "Gradient: [  -22.65412836   -48.13217096   -16.98476823    91.60859004\n",
      "   -21.53912563   -79.90050713  -124.97949402    73.46198438\n",
      "    18.14660566 -1077.2023282   2770.71161064]\n",
      "Old weights: [ 1.00014987  0.9994759   0.99929866 -0.31807097  1.00008934  1.00042709\n",
      "  0.99973394  0.34559239  0.33633664  1.00710223  0.98165962]\n",
      "New weights: [ 1.00015213  0.99948072  0.99930036 -0.31808013  1.00009149  1.00043508\n",
      "  0.99974644  0.34558504  0.33633483  1.00720995  0.98138255]\n",
      "Loss: 799.206564009413\n",
      "\n",
      "Gradient: [  -22.64152883   -48.10470426   -16.97483676    91.55443456\n",
      "   -21.52681819   -79.85577198  -124.90946676    73.42616566\n",
      "    18.1282689  -1076.60321874  2769.17062978]\n",
      "Old weights: [ 1.00015213  0.99948072  0.99930036 -0.31808013  1.00009149  1.00043508\n",
      "  0.99974644  0.34558504  0.33633483  1.00720995  0.98138255]\n",
      "New weights: [ 1.0001544   0.99948553  0.99930206 -0.31808928  1.00009364  1.00044307\n",
      "  0.99975893  0.3455777   0.33633302  1.00731761  0.98110564]\n",
      "Loss: 798.3201065695403\n",
      "\n",
      "Gradient: [  -22.6289363    -48.07725264   -16.96491072    91.50048856\n",
      "   -21.5145178    -79.81106153  -124.83947844    73.39008457\n",
      "    18.11040398 -1076.00444202  2767.63050476]\n",
      "Old weights: [ 1.0001544   0.99948553  0.99930206 -0.31808928  1.00009364  1.00044307\n",
      "  0.99975893  0.3455777   0.33633302  1.00731761  0.98110564]\n",
      "New weights: [ 1.00015666  0.99949034  0.99930376 -0.31809843  1.0000958   1.00045105\n",
      "  0.99977142  0.34557036  0.33633121  1.00742521  0.98082887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 797.4346349366432\n",
      "\n",
      "Gradient: [  -22.61635077   -48.04981612   -16.95499013    91.44674188\n",
      "   -21.50222445   -79.76637577  -124.76952903    73.35375706\n",
      "    18.09298482 -1075.40599788  2766.09123518]\n",
      "Old weights: [ 1.00015666  0.99949034  0.99930376 -0.31809843  1.0000958   1.00045105\n",
      "  0.99977142  0.34557036  0.33633121  1.00742521  0.98082887]\n",
      "New weights: [ 1.00015892  0.99949514  0.99930545 -0.31810758  1.00009795  1.00045902\n",
      "  0.99978389  0.34556303  0.3363294   1.00753275  0.98055226]\n",
      "Loss: 796.5501480144056\n",
      "\n",
      "Gradient: [  -22.60377222   -48.02239469   -16.94507499    91.39318492\n",
      "   -21.48993811   -79.72171469  -124.69961853    73.31719819\n",
      "    18.07598673 -1074.80788616  2764.55282063]\n",
      "Old weights: [ 1.00015892  0.99949514  0.99930545 -0.31810758  1.00009795  1.00045902\n",
      "  0.99978389  0.34556303  0.3363294   1.00753275  0.98055226]\n",
      "New weights: [ 1.00016118  0.99949994  0.99930715 -0.31811672  1.00010009  1.000467\n",
      "  0.99979636  0.3455557   0.33632759  1.00764023  0.98027581]\n",
      "Loss: 795.6666447077148\n",
      "\n",
      "Gradient: [  -22.59120067   -47.99498836   -16.93516528    91.33980864\n",
      "   -21.47765878   -79.6770783   -124.62974691    73.28042217\n",
      "    18.05938647 -1074.2101067   2763.0152607 ]\n",
      "Old weights: [ 1.00016118  0.99949994  0.99930715 -0.31811672  1.00010009  1.000467\n",
      "  0.99979636  0.3455557   0.33632759  1.00764023  0.98027581]\n",
      "New weights: [ 1.00016344  0.99950474  0.99930884 -0.31812585  1.00010224  1.00047496\n",
      "  0.99980883  0.34554837  0.33632578  1.00774765  0.97999951]\n",
      "Loss: 794.7841239226789\n",
      "\n",
      "Gradient: [  -22.5786361    -47.96759713   -16.92526103    91.28660448\n",
      "   -21.46538645   -79.63246658  -124.55991414    73.24344243\n",
      "    18.04316204 -1073.61265933  2761.47855496]\n",
      "Old weights: [ 1.00016344  0.99950474  0.99930884 -0.31812585  1.00010224  1.00047496\n",
      "  0.99980883  0.34554837  0.33632578  1.00774765  0.97999951]\n",
      "New weights: [ 1.0001657   0.99950954  0.99931053 -0.31813498  1.00010439  1.00048293\n",
      "  0.99982128  0.34554104  0.33632398  1.00785501  0.97972336]\n",
      "Loss: 793.9025845666508\n",
      "\n",
      "Gradient: [  -22.56607851   -47.94022099   -16.91536222    91.23356436\n",
      "   -21.4531211    -79.58787954  -124.4901202     73.20627164\n",
      "    18.02729272 -1073.0155439   2759.942703  ]\n",
      "Old weights: [ 1.0001657   0.99950954  0.99931053 -0.31813498  1.00010439  1.00048293\n",
      "  0.99982128  0.34554104  0.33632398  1.00785501  0.97972336]\n",
      "New weights: [ 1.00016796  0.99951433  0.99931222 -0.3181441   1.00010653  1.00049089\n",
      "  0.99983373  0.34553372  0.33632218  1.00796231  0.97944736]\n",
      "Loss: 793.0220255481912\n",
      "\n",
      "Gradient: [  -22.55352789   -47.91285995   -16.90546886    91.18068068\n",
      "   -21.44086271   -79.54331716  -124.42036508    73.16892175\n",
      "    18.01175893 -1072.41876023  2758.40770439]\n",
      "Old weights: [ 1.00016796  0.99951433  0.99931222 -0.3181441   1.00010653  1.00049089\n",
      "  0.99983373  0.34553372  0.33632218  1.00796231  0.97944736]\n",
      "New weights: [ 1.00017021  0.99951912  0.99931391 -0.31815322  1.00010868  1.00049884\n",
      "  0.99984617  0.34552641  0.33632038  1.00806955  0.97917152]\n",
      "Loss: 792.1424457770491\n",
      "\n",
      "Gradient: [  -22.54098425   -47.885514     -16.89558095    91.12794622\n",
      "   -21.42861128   -79.49877944  -124.35064875    73.13140405\n",
      "    17.99654217 -1071.82230817  2756.87355872]\n",
      "Old weights: [ 1.00017021  0.99951912  0.99931391 -0.31815322  1.00010868  1.00049884\n",
      "  0.99984617  0.34552641  0.33632038  1.00806955  0.97917152]\n",
      "New weights: [ 1.00017247  0.99952391  0.9993156  -0.31816233  1.00011082  1.00050679\n",
      "  0.99985861  0.34551909  0.33631858  1.00817674  0.97889584]\n",
      "Loss: 791.2638441642334\n",
      "\n",
      "Gradient: [  -22.52844758   -47.85818315   -16.88569849    91.0753542\n",
      "   -21.4163668    -79.45426638  -124.2809712     73.09372919\n",
      "    17.98162501 -1071.22618755  2755.34026554]\n",
      "Old weights: [ 1.00017247  0.99952391  0.9993156  -0.31816233  1.00011082  1.00050679\n",
      "  0.99985861  0.34551909  0.33631858  1.00817674  0.97889584]\n",
      "New weights: [ 1.00017472  0.9995287   0.99931729 -0.31817144  1.00011296  1.00051473\n",
      "  0.99987104  0.34551178  0.33631678  1.00828386  0.9786203 ]\n",
      "Loss: 790.3862196219303\n",
      "\n",
      "Gradient: [  -22.51591788   -47.83086739   -16.87582147    91.02289821\n",
      "   -21.40412924   -79.40977796  -124.21133239    73.05590724\n",
      "    17.96699097 -1070.6303982   2753.80782444]\n",
      "Old weights: [ 1.00017472  0.9995287   0.99931729 -0.31817144  1.00011296  1.00051473\n",
      "  0.99987104  0.34551178  0.33631678  1.00828386  0.9786203 ]\n",
      "New weights: [ 1.00017697  0.99953348  0.99931898 -0.31818054  1.0001151   1.00052268\n",
      "  0.99988346  0.34550448  0.33631498  1.00839092  0.97834492]\n",
      "Loss: 789.5095710635572\n",
      "\n",
      "Gradient: [  -22.50339514   -47.80356671   -16.8659499     90.97057219\n",
      "   -21.39189861   -79.36531419  -124.14173231    73.01794768\n",
      "    17.95262451 -1070.03493994  2752.27623497]\n",
      "Old weights: [ 1.00017697  0.99953348  0.99931898 -0.31818054  1.0001151   1.00052268\n",
      "  0.99988346  0.34550448  0.33631498  1.00839092  0.97834492]\n",
      "New weights: [ 1.00017922  0.99953826  0.99932067 -0.31818964  1.00011724  1.00053061\n",
      "  0.99989587  0.34549718  0.33631319  1.00849792  0.97806969]\n",
      "Loss: 788.6338974037324\n",
      "\n",
      "Gradient: [  -22.49087937   -47.77628113   -16.85608378    90.91837042\n",
      "   -21.37967489   -79.32087504  -124.07217094    72.97985947\n",
      "    17.93851094 -1069.43981262  2750.7454967 ]\n",
      "Old weights: [ 1.00017922  0.99953826  0.99932067 -0.31818964  1.00011724  1.00053061\n",
      "  0.99989587  0.34549718  0.33631319  1.00849792  0.97806969]\n",
      "New weights: [ 1.00018147  0.99954304  0.99932235 -0.31819873  1.00011938  1.00053854\n",
      "  0.99990828  0.34548988  0.33631139  1.00860487  0.97779462]\n",
      "Loss: 787.759197558282\n",
      "\n",
      "Gradient: [  -22.47837054   -47.74901062   -16.8462231     90.8662875\n",
      "   -21.36745808   -79.27646052  -124.00264825    72.94165108\n",
      "    17.92463642 -1068.84501607  2749.2156092 ]\n",
      "Old weights: [ 1.00018147  0.99954304  0.99932235 -0.31819873  1.00011938  1.00053854\n",
      "  0.99990828  0.34548988  0.33631139  1.00860487  0.97779462]\n",
      "New weights: [ 1.00018372  0.99954781  0.99932404 -0.31820782  1.00012152  1.00054647\n",
      "  0.99992068  0.34548258  0.3363096   1.00871175  0.9775197 ]\n",
      "Loss: 786.8854704442477\n",
      "\n",
      "Gradient: [  -22.46586867   -47.7217552    -16.83636787    90.81431835\n",
      "   -21.35524815   -79.23207061  -123.93316423    72.90333049\n",
      "    17.91098785 -1068.2505501   2747.68657203]\n",
      "Old weights: [ 1.00018372  0.99954781  0.99932404 -0.31820782  1.00012152  1.00054647\n",
      "  0.99992068  0.34548258  0.3363096   1.00871175  0.9775197 ]\n",
      "New weights: [ 1.00018596  0.99955259  0.99932572 -0.3182169   1.00012365  1.0005544\n",
      "  0.99993307  0.34547529  0.33630781  1.00881858  0.97724493]\n",
      "Loss: 786.0127149798723\n",
      "\n",
      "Gradient: [  -22.45337375   -47.69451486   -16.82651809    90.76245814\n",
      "   -21.34304511   -79.1877053   -123.86371885    72.86490523\n",
      "    17.8975529  -1067.65641456  2746.15838474]\n",
      "Old weights: [ 1.00018596  0.99955259  0.99932572 -0.3182169   1.00012365  1.0005544\n",
      "  0.99993307  0.34547529  0.33630781  1.00881858  0.97724493]\n",
      "New weights: [ 1.00018821  0.99955736  0.9993274  -0.31822598  1.00012579  1.00056231\n",
      "  0.99994546  0.34546801  0.33630602  1.00892534  0.97697031]\n",
      "Loss: 785.1409300845843\n",
      "\n",
      "Gradient: [  -22.44088578   -47.66728958   -16.81667375    90.71070233\n",
      "   -21.33084894   -79.1433646   -123.7943121     72.82638242\n",
      "    17.88431991 -1067.06260927  2744.6310469 ]\n",
      "Old weights: [ 1.00018821  0.99955736  0.9993274  -0.31822598  1.00012579  1.00056231\n",
      "  0.99994546  0.34546801  0.33630602  1.00892534  0.97697031]\n",
      "New weights: [ 1.00019045  0.99956212  0.99932908 -0.31823505  1.00012792  1.00057023\n",
      "  0.99995784  0.34546072  0.33630423  1.00903205  0.97669585]\n",
      "Loss: 784.2701146790534\n",
      "\n",
      "Gradient: [  -22.42840475   -47.64007938   -16.80683485    90.65904664\n",
      "   -21.31865964   -79.09904848  -123.72494394    72.78776878\n",
      "    17.87127786 -1066.46913406  2743.10455806]\n",
      "Old weights: [ 1.00019045  0.99956212  0.99932908 -0.31823505  1.00012792  1.00057023\n",
      "  0.99995784  0.34546072  0.33630423  1.00903205  0.97669585]\n",
      "New weights: [ 1.0001927   0.99956689  0.99933076 -0.31824411  1.00013005  1.00057814\n",
      "  0.99997021  0.34545344  0.33630244  1.0091387   0.97642154]\n",
      "Loss: 783.4002676851127\n",
      "\n",
      "Gradient: [  -22.41593065   -47.61288424   -16.79700139    90.60748701\n",
      "   -21.30647719   -79.05475694  -123.65561437    72.74907063\n",
      "    17.85841639 -1065.87598875  2741.57891778]\n",
      "Old weights: [ 1.0001927   0.99956689  0.99933076 -0.31824411  1.00013005  1.00057814\n",
      "  0.99997021  0.34545344  0.33630244  1.0091387   0.97642154]\n",
      "New weights: [ 1.00019494  0.99957165  0.99933244 -0.31825317  1.00013218  1.00058604\n",
      "  0.99998258  0.34544617  0.33630066  1.00924528  0.97614738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 782.5313880258176\n",
      "\n",
      "Gradient: [  -22.40346349   -47.58570417   -16.78717337    90.55601962\n",
      "   -21.2943016    -79.01048996  -123.58632335    72.71029396\n",
      "    17.84572567 -1065.28317317  2740.05412561]\n",
      "Old weights: [ 1.00019494  0.99957165  0.99933244 -0.31825317  1.00013218  1.00058604\n",
      "  0.99998258  0.34544617  0.33630066  1.00924528  0.97614738]\n",
      "New weights: [ 1.00019718  0.99957641  0.99933412 -0.31826223  1.00013431  1.00059394\n",
      "  0.99999494  0.3454389   0.33629887  1.00935181  0.97587338]\n",
      "Loss: 781.6634746254136\n",
      "\n",
      "Gradient: [  -22.39100326   -47.55853914   -16.77735079    90.50464086\n",
      "   -21.28213284   -78.96624755  -123.51707087    72.67144442\n",
      "    17.83319645 -1064.69068715  2738.53018111]\n",
      "Old weights: [ 1.00019718  0.99957641  0.99933412 -0.31826223  1.00013431  1.00059394\n",
      "  0.99999494  0.3454389   0.33629887  1.00935181  0.97587338]\n",
      "New weights: [ 1.00019942  0.99958116  0.9993358  -0.31827128  1.00013644  1.00060184\n",
      "  1.00000729  0.34543163  0.33629709  1.00945828  0.97559952]\n",
      "Loss: 780.7965264093434\n",
      "\n",
      "Gradient: [  -22.37854996   -47.53138917   -16.76753365    90.45334733\n",
      "   -21.26997092   -78.92202968  -123.4478569     72.63252734\n",
      "    17.82081999 -1064.09853051  2737.00708382]\n",
      "Old weights: [ 1.00019942  0.99958116  0.9993358  -0.31827128  1.00013644  1.00060184\n",
      "  1.00000729  0.34543163  0.33629709  1.00945828  0.97559952]\n",
      "New weights: [ 1.00020165  0.99958592  0.99933748 -0.31828032  1.00013857  1.00060973\n",
      "  1.00001963  0.34542437  0.33629531  1.00956469  0.97532582]\n",
      "Loss: 779.9305423042484\n",
      "\n",
      "Gradient: [  -22.36610358   -47.50425425   -16.75772195    90.40213578\n",
      "   -21.25781582   -78.87783636  -123.37868143    72.59354774\n",
      "    17.80858804 -1063.50670308  2735.48483329]\n",
      "Old weights: [ 1.00020165  0.99958592  0.99933748 -0.31828032  1.00013857  1.00060973\n",
      "  1.00001963  0.34542437  0.33629531  1.00956469  0.97532582]\n",
      "New weights: [ 1.00020389  0.99959067  0.99933915 -0.31828936  1.00014069  1.00061762\n",
      "  1.00003197  0.34541711  0.33629353  1.00967104  0.97505227]\n",
      "Loss: 779.0655212379487\n",
      "\n",
      "Gradient: [  -22.35366412   -47.47713436   -16.74791568    90.3510032\n",
      "   -21.24566755   -78.83366756  -123.30954443    72.55451038\n",
      "    17.79649282 -1062.91520469  2733.96342909]\n",
      "Old weights: [ 1.00020389  0.99959067  0.99933915 -0.31828936  1.00014069  1.00061762\n",
      "  1.00003197  0.34541711  0.33629353  1.00967104  0.97505227]\n",
      "New weights: [ 1.00020613  0.99959541  0.99934083 -0.3182984   1.00014282  1.0006255\n",
      "  1.0000443   0.34540985  0.33629175  1.00977733  0.97477888]\n",
      "Loss: 778.2014621394928\n",
      "\n",
      "Gradient: [  -22.34123158   -47.45002951   -16.73811484    90.29994671\n",
      "   -21.23352608   -78.78952327  -123.24044588    72.51541974\n",
      "    17.78452697 -1062.32403515  2732.44287074]\n",
      "Old weights: [ 1.00020613  0.99959541  0.99934083 -0.3182984   1.00014282  1.0006255\n",
      "  1.0000443   0.34540985  0.33629175  1.00977733  0.97477888]\n",
      "New weights: [ 1.00020836  0.99960016  0.9993425  -0.31830743  1.00014494  1.00063338\n",
      "  1.00005662  0.3454026   0.33628997  1.00988357  0.97450563]\n",
      "Loss: 777.3383639390727\n",
      "\n",
      "Gradient: [  -22.32880595   -47.42293969   -16.72831943    90.24896359\n",
      "   -21.22139142   -78.74540349  -123.17138577    72.47628004\n",
      "    17.77268355 -1061.7331943   2730.92315781]\n",
      "Old weights: [ 1.00020836  0.99960016  0.9993425  -0.31830743  1.00014494  1.00063338\n",
      "  1.00005662  0.3454026   0.33628997  1.00988357  0.97450563]\n",
      "New weights: [ 1.00021059  0.9996049   0.99934417 -0.31831645  1.00014706  1.00064126\n",
      "  1.00006894  0.34539535  0.33628819  1.00998974  0.97423254]\n",
      "Loss: 776.4762255681197\n",
      "\n",
      "Gradient: [  -22.31638722   -47.39586489   -16.71852945    90.1980513\n",
      "   -21.20926355   -78.70130821  -123.10236406    72.43709528\n",
      "    17.76095601 -1061.14268195  2729.40428984]\n",
      "Old weights: [ 1.00021059  0.9996049   0.99934417 -0.31831645  1.00014706  1.00064126\n",
      "  1.00006894  0.34539535  0.33628819  1.00998974  0.97423254]\n",
      "New weights: [ 1.00021283  0.99960964  0.99934585 -0.31832547  1.00014918  1.00064913\n",
      "  1.00008125  0.34538811  0.33628641  1.01009585  0.9739596 ]\n",
      "Loss: 775.6150459592193\n",
      "\n",
      "Gradient: [  -22.30397541   -47.36880511   -16.70874489    90.1472074\n",
      "   -21.19714248   -78.65723741  -123.03338074    72.39786923\n",
      "    17.74933817 -1060.55249793  2727.88626638]\n",
      "Old weights: [ 1.00021283  0.99960964  0.99934585 -0.31832547  1.00014918  1.00064913\n",
      "  1.00008125  0.34538811  0.33628641  1.01009585  0.9739596 ]\n",
      "New weights: [ 1.00021506  0.99961438  0.99934752 -0.31833449  1.0001513   1.00065699\n",
      "  1.00009356  0.34538087  0.33628464  1.01020191  0.97368681]\n",
      "Loss: 774.7548240461401\n",
      "\n",
      "Gradient: [  -22.29157049   -47.34176034   -16.69896577    90.09642962\n",
      "   -21.1850282    -78.61319108  -122.96443578    72.35860544\n",
      "    17.73782418 -1059.96264207  2726.36908697]\n",
      "Old weights: [ 1.00021506  0.99961438  0.99934752 -0.31833449  1.0001513   1.00065699\n",
      "  1.00009356  0.34538087  0.33628464  1.01020191  0.97368681]\n",
      "New weights: [ 1.00021729  0.99961911  0.99934919 -0.3183435   1.00015342  1.00066486\n",
      "  1.00010585  0.34537364  0.33628287  1.01030791  0.97341417]\n",
      "Loss: 773.895558763879\n",
      "\n",
      "Gradient: [  -22.27917247   -47.31473058   -16.68919206    90.04571581\n",
      "   -21.17292069   -78.56916921  -122.89552918    72.31930727\n",
      "    17.72640854 -1059.37311418  2724.85275115]\n",
      "Old weights: [ 1.00021729  0.99961911  0.99934919 -0.3183435   1.00015342  1.00066486\n",
      "  1.00010585  0.34537364  0.33628287  1.01030791  0.97341417]\n",
      "New weights: [ 1.00021951  0.99962384  0.99935086 -0.3183525   1.00015554  1.00067271\n",
      "  1.00011814  0.3453664   0.33628109  1.01041384  0.97314169]\n",
      "Loss: 773.0372490485715\n",
      "\n",
      "Gradient: [  -22.26678135   -47.28771581   -16.67942378    89.99506392\n",
      "   -21.16081995   -78.5251718   -122.82666089    72.27997788\n",
      "    17.71508603 -1058.7839141   2723.33725848]\n",
      "Old weights: [ 1.00021951  0.99962384  0.99935086 -0.3183525   1.00015554  1.00067271\n",
      "  1.00011814  0.3453664   0.33628109  1.01041384  0.97314169]\n",
      "New weights: [ 1.00022174  0.99962857  0.99935252 -0.3183615   1.00015766  1.00068056\n",
      "  1.00013042  0.34535918  0.33627932  1.01051972  0.97286936]\n",
      "Loss: 772.1798938375472\n",
      "\n",
      "Gradient: [  -22.25439711   -47.26071604   -16.66966091    89.94447203\n",
      "   -21.14872598   -78.48119882  -122.75783091    72.24062027\n",
      "    17.70385176 -1058.19504163  2721.8226085 ]\n",
      "Old weights: [ 1.00022174  0.99962857  0.99935252 -0.3183615   1.00015766  1.00068056\n",
      "  1.00013042  0.34535918  0.33627932  1.01051972  0.97286936]\n",
      "New weights: [ 1.00022397  0.9996333   0.99935419 -0.3183705   1.00015977  1.00068841\n",
      "  1.0001427   0.34535195  0.33627755  1.01062554  0.97259717]\n",
      "Loss: 771.3234920693359\n",
      "\n",
      "Gradient: [  -22.24201977   -47.23373126   -16.65990346    89.89393833\n",
      "   -21.13663878   -78.43725026  -122.68903921    72.20123726\n",
      "    17.69270107 -1057.60649662  2720.30880074]\n",
      "Old weights: [ 1.00022397  0.9996333   0.99935419 -0.3183705   1.00015977  1.00068841\n",
      "  1.0001427   0.34535195  0.33627755  1.01062554  0.97259717]\n",
      "New weights: [ 1.00022619  0.99963802  0.99935586 -0.31837949  1.00016188  1.00069626\n",
      "  1.00015497  0.34534473  0.33627578  1.0107313   0.97232514]\n",
      "Loss: 770.4680426836364\n",
      "\n",
      "Gradient: [  -22.2296493    -47.20676145   -16.65015142    89.8434611\n",
      "   -21.12455832   -78.39332613  -122.62028577    72.1618315\n",
      "    17.6816296  -1057.01827887  2718.79583475]\n",
      "Old weights: [ 1.00022619  0.99963802  0.99935586 -0.31837949  1.00016188  1.00069626\n",
      "  1.00015497  0.34534473  0.33627578  1.0107313   0.97232514]\n",
      "New weights: [ 1.00022841  0.99964274  0.99935752 -0.31838847  1.000164    1.0007041\n",
      "  1.00016723  0.34533751  0.33627401  1.010837    0.97205326]\n",
      "Loss: 769.6135446213262\n",
      "\n",
      "Gradient: [  -22.21728572   -47.17980662   -16.6404048     89.79303872\n",
      "   -21.11248462   -78.34942639  -122.55157057    72.12240552\n",
      "    17.6706332  -1056.43038821  2717.28371008]\n",
      "Old weights: [ 1.00022841  0.99964274  0.99935752 -0.31838847  1.000164    1.0007041\n",
      "  1.00016723  0.34533751  0.33627401  1.010837    0.97205326]\n",
      "New weights: [ 1.00023063  0.99964746  0.99935919 -0.31839745  1.00016611  1.00071193\n",
      "  1.00017949  0.3453303   0.33627225  1.01094265  0.97178153]\n",
      "Loss: 768.7599968244683\n",
      "\n",
      "Gradient: [  -22.204929     -47.15286675   -16.63066359    89.74266967\n",
      "   -21.10041767   -78.30555104  -122.48289358    72.08296168\n",
      "    17.65970799 -1055.84282446  2715.77242627]\n",
      "Old weights: [ 1.00023063  0.99964746  0.99935919 -0.31839745  1.00016611  1.00071193\n",
      "  1.00017949  0.3453303   0.33627225  1.01094265  0.97178153]\n",
      "New weights: [ 1.00023285  0.99965218  0.99936085 -0.31840642  1.00016822  1.00071976\n",
      "  1.00019173  0.34532309  0.33627048  1.01104823  0.97150996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 767.9073982362744\n",
      "\n",
      "Gradient: [  -22.19257916   -47.12594184   -16.62092778    89.69235249\n",
      "   -21.08835745   -78.26170007  -122.4142548     72.04350222\n",
      "    17.64885027 -1055.25558744  2714.26198286]\n",
      "Old weights: [ 1.00023285  0.99965218  0.99936085 -0.31840642  1.00016822  1.00071976\n",
      "  1.00019173  0.34532309  0.33627048  1.01104823  0.97150996]\n",
      "New weights: [ 1.00023507  0.99965689  0.99936251 -0.31841539  1.00017033  1.00072759\n",
      "  1.00020398  0.34531589  0.33626872  1.01115376  0.97123853]\n",
      "Loss: 767.0557478011764\n",
      "\n",
      "Gradient: [  -22.18023619   -47.09903189   -16.61119738    89.64208583\n",
      "   -21.07630396   -78.21787347  -122.34565419    72.00402926\n",
      "    17.63805657 -1054.66867698  2712.75237939]\n",
      "Old weights: [ 1.00023507  0.99965689  0.99936251 -0.31841539  1.00017033  1.00072759\n",
      "  1.00020398  0.34531589  0.33626872  1.01115376  0.97123853]\n",
      "New weights: [ 1.00023729  0.9996616   0.99936417 -0.31842436  1.00017243  1.00073541\n",
      "  1.00021621  0.34530869  0.33626695  1.01125922  0.97096726]\n",
      "Loss: 766.2050444647421\n",
      "\n",
      "Gradient: [  -22.16790008   -47.07213688   -16.60147237    89.59186838\n",
      "   -21.06425721   -78.17407122  -122.27709173    71.96454478\n",
      "    17.6273236  -1054.0820929   2711.2436154 ]\n",
      "Old weights: [ 1.00023729  0.9996616   0.99936417 -0.31842436  1.00017243  1.00073541\n",
      "  1.00021621  0.34530869  0.33626695  1.01125922  0.97096726]\n",
      "New weights: [ 1.00023951  0.99966631  0.99936583 -0.31843332  1.00017454  1.00074323\n",
      "  1.00022844  0.34530149  0.33626519  1.01136463  0.97069613]\n",
      "Loss: 765.3552871737284\n",
      "\n",
      "Gradient: [  -22.15557084   -47.04525681   -16.59175277    89.54169894\n",
      "   -21.05221717   -78.13029332  -122.20856741    71.92505067\n",
      "    17.61664827 -1053.49583502  2709.73569044]\n",
      "Old weights: [ 1.00023951  0.99966631  0.99936583 -0.31843332  1.00017454  1.00074323\n",
      "  1.00022844  0.34530149  0.33626519  1.01136463  0.97069613]\n",
      "New weights: [ 1.00024172  0.99967101  0.99936749 -0.31844227  1.00017665  1.00075104\n",
      "  1.00024066  0.3452943   0.33626343  1.01146998  0.97042516]\n",
      "Loss: 764.5064748760678\n",
      "\n",
      "Gradient: [  -22.14324844   -47.01839167   -16.58203857    89.49157636\n",
      "   -21.04018386   -78.08653974  -122.14008119    71.88554871\n",
      "    17.60602765 -1052.90990316  2708.22860404]\n",
      "Old weights: [ 1.00024172  0.99967101  0.99936749 -0.31844227  1.00017665  1.00075104\n",
      "  1.00024066  0.3452943   0.33626343  1.01146998  0.97042516]\n",
      "New weights: [ 1.00024394  0.99967571  0.99936915 -0.31845122  1.00017875  1.00075885\n",
      "  1.00025287  0.34528711  0.33626167  1.01157527  0.97015434]\n",
      "Loss: 763.6586065208543\n",
      "\n",
      "Gradient: [  -22.13093291   -46.99154145   -16.57232976    89.44149953\n",
      "   -21.02815726   -78.04281048  -122.07163307    71.84604056\n",
      "    17.59545898 -1052.32429714  2706.72235575]\n",
      "Old weights: [ 1.00024394  0.99967571  0.99936915 -0.31845122  1.00017875  1.00075885\n",
      "  1.00025287  0.34528711  0.33626167  1.01157527  0.97015434]\n",
      "New weights: [ 1.00024615  0.99968041  0.99937081 -0.31846016  1.00018085  1.00076665\n",
      "  1.00026508  0.34527993  0.33625991  1.0116805   0.96988366]\n",
      "Loss: 762.8116810583448\n",
      "\n",
      "Gradient: [  -22.11862422   -46.96470615   -16.56262635    89.39146745\n",
      "   -21.01613736   -77.99910552  -122.00322302    71.8065278\n",
      "    17.58493965 -1051.73901679  2705.21694511]\n",
      "Old weights: [ 1.00024615  0.99968041  0.99937081 -0.31846016  1.00018085  1.00076665\n",
      "  1.00026508  0.34527993  0.33625991  1.0116805   0.96988366]\n",
      "New weights: [ 1.00024836  0.99968511  0.99937246 -0.3184691   1.00018295  1.00077445\n",
      "  1.00027728  0.34527275  0.33625815  1.01178568  0.96961314]\n",
      "Loss: 761.9656974399779\n",
      "\n",
      "Gradient: [  -22.10632237   -46.93788576   -16.55292832    89.34147914\n",
      "   -21.00412417   -77.95542486  -121.93485102    71.76701194\n",
      "    17.5744672  -1051.15406192  2703.71237165]\n",
      "Old weights: [ 1.00024836  0.99968511  0.99937246 -0.3184691   1.00018295  1.00077445\n",
      "  1.00027728  0.34527275  0.33625815  1.01178568  0.96961314]\n",
      "New weights: [ 1.00025057  0.9996898   0.99937412 -0.31847804  1.00018505  1.00078225\n",
      "  1.00028947  0.34526557  0.33625639  1.01189079  0.96934277]\n",
      "Loss: 761.120654618358\n",
      "\n",
      "Gradient: [  -22.09402737   -46.91108027   -16.54323568    89.29153367\n",
      "   -20.99211768   -77.91176847  -121.86651705    71.72749437\n",
      "    17.5640393  -1050.56943236  2702.20863493]\n",
      "Old weights: [ 1.00025057  0.9996898   0.99937412 -0.31847804  1.00018505  1.00078225\n",
      "  1.00028947  0.34526557  0.33625639  1.01189079  0.96934277]\n",
      "New weights: [ 1.00025278  0.99969449  0.99937577 -0.31848697  1.00018715  1.00079004\n",
      "  1.00030166  0.3452584   0.33625464  1.01199585  0.96907255]\n",
      "Loss: 760.2765515472441\n",
      "\n",
      "Gradient: [  -22.0817392    -46.88428968   -16.53354842    89.24163019\n",
      "   -20.98011789   -77.86813635  -121.79822108    71.68797642\n",
      "    17.55365377 -1049.98512792  2700.70573447]\n",
      "Old weights: [ 1.00025278  0.99969449  0.99937577 -0.31848697  1.00018715  1.00079004\n",
      "  1.00030166  0.3452584   0.33625464  1.01199585  0.96907255]\n",
      "New weights: [ 1.00025499  0.99969918  0.99937743 -0.31849589  1.00018925  1.00079783\n",
      "  1.00031384  0.34525123  0.33625288  1.01210085  0.96880248]\n",
      "Loss: 759.4333871815434\n",
      "\n",
      "Gradient: [  -22.06945787   -46.85751397   -16.52386655    89.19176788\n",
      "   -20.96812478   -77.82452849  -121.72996309    71.64845935\n",
      "    17.54330853 -1049.40114844  2699.20366982]\n",
      "Old weights: [ 1.00025499  0.99969918  0.99937743 -0.31849589  1.00018925  1.00079783\n",
      "  1.00031384  0.34525123  0.33625288  1.01210085  0.96880248]\n",
      "New weights: [ 1.0002572   0.99970387  0.99937908 -0.31850481  1.00019135  1.00080561\n",
      "  1.00032601  0.34524406  0.33625113  1.01220579  0.96853256]\n",
      "Loss: 758.5911604773788\n",
      "\n",
      "Gradient: [  -22.05718337   -46.83075314   -16.51419006    89.14194595\n",
      "   -20.95613836   -77.78094486  -121.66174307    71.60894433\n",
      "    17.53300163 -1048.81749372  2697.70244052]\n",
      "Old weights: [ 1.0002572   0.99970387  0.99937908 -0.31850481  1.00019135  1.00080561\n",
      "  1.00032601  0.34524406  0.33625113  1.01220579  0.96853256]\n",
      "New weights: [ 1.0002594   0.99970855  0.99938073 -0.31851372  1.00019344  1.00081339\n",
      "  1.00033818  0.3452369   0.33624937  1.01231067  0.96826279]\n",
      "Loss: 757.7498703919773\n",
      "\n",
      "Gradient: [  -22.04491569   -46.80400718   -16.50451894    89.09216369\n",
      "   -20.94415862   -77.73738547  -121.593561      71.56943248\n",
      "    17.52273121 -1048.2341636   2696.20204611]\n",
      "Old weights: [ 1.0002594   0.99970855  0.99938073 -0.31851372  1.00019344  1.00081339\n",
      "  1.00033818  0.3452369   0.33624937  1.01231067  0.96826279]\n",
      "New weights: [ 1.00026161  0.99971323  0.99938238 -0.31852263  1.00019554  1.00082116\n",
      "  1.00035034  0.34522975  0.33624762  1.01241549  0.96799317]\n",
      "Loss: 756.909515883759\n",
      "\n",
      "Gradient: [  -22.03265484   -46.77727609   -16.4948532     89.0424204\n",
      "   -20.93218556   -77.69385029  -121.52541684    71.52992485\n",
      "    17.51249555 -1047.65115789  2694.70248612]\n",
      "Old weights: [ 1.00026161  0.99971323  0.99938238 -0.31852263  1.00019554  1.00082116\n",
      "  1.00035034  0.34522975  0.33624762  1.01241549  0.96799317]\n",
      "New weights: [ 1.00026381  0.99971791  0.99938403 -0.31853154  1.00019763  1.00082893\n",
      "  1.00036249  0.34522259  0.33624587  1.01252026  0.9677237 ]\n",
      "Loss: 756.0700959122883\n",
      "\n",
      "Gradient: [  -22.0204008    -46.75055986   -16.48519282    88.99271542\n",
      "   -20.92021916   -77.65033931  -121.45731059    71.49042244\n",
      "    17.50229298 -1047.06847642  2693.20376011]\n",
      "Old weights: [ 1.00026381  0.99971791  0.99938403 -0.31853154  1.00019763  1.00082893\n",
      "  1.00036249  0.34522259  0.33624587  1.01252026  0.9677237 ]\n",
      "New weights: [ 1.00026601  0.99972258  0.99938568 -0.31854044  1.00019972  1.0008367\n",
      "  1.00037464  0.34521544  0.33624412  1.01262497  0.96745438]\n",
      "Loss: 755.2316094382796\n",
      "\n",
      "Gradient: [  -22.00815358   -46.72385847   -16.47553782    88.94304814\n",
      "   -20.90825944   -77.60685252  -121.38924221    71.45092617\n",
      "    17.49212196 -1046.486119    2691.70586761]\n",
      "Old weights: [ 1.00026601  0.99972258  0.99938568 -0.31854044  1.00019972  1.0008367\n",
      "  1.00037464  0.34521544  0.33624412  1.01262497  0.96745438]\n",
      "New weights: [ 1.00026821  0.99972725  0.99938733 -0.31854933  1.00020182  1.00084446\n",
      "  1.00038678  0.3452083   0.33624237  1.01272961  0.96718521]\n",
      "Loss: 754.3940554236376\n",
      "\n",
      "Gradient: [  -21.99591318   -46.69717193   -16.46588818    88.89341797\n",
      "   -20.89630637   -77.56338991  -121.3212117     71.41143694\n",
      "    17.48198103 -1045.90408546  2690.20880816]\n",
      "Old weights: [ 1.00026821  0.99972725  0.99938733 -0.31854933  1.00020182  1.00084446\n",
      "  1.00038678  0.3452083   0.33624237  1.01272961  0.96718521]\n",
      "New weights: [ 1.00027041  0.99973192  0.99938897 -0.31855822  1.0002039   1.00085221\n",
      "  1.00039891  0.34520116  0.33624062  1.01283421  0.96691619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 753.5574328313759\n",
      "\n",
      "Gradient: [  -21.98367958   -46.67050022   -16.45624391    88.84382437\n",
      "   -20.88435997   -77.51995147  -121.25321902    71.37195557\n",
      "    17.47186879 -1045.32237562  2688.7125813 ]\n",
      "Old weights: [ 1.00027041  0.99973192  0.99938897 -0.31855822  1.0002039   1.00085221\n",
      "  1.00039891  0.34520116  0.33624062  1.01283421  0.96691619]\n",
      "New weights: [ 1.00027261  0.99973659  0.99939062 -0.31856711  1.00020599  1.00085996\n",
      "  1.00041103  0.34519402  0.33623887  1.01293874  0.96664731]\n",
      "Loss: 752.7217406256962\n",
      "\n",
      "Gradient: [  -21.97145278   -46.64384333   -16.44660499    88.7942668\n",
      "   -20.87242022   -77.47653717  -121.18526416    71.33248286\n",
      "    17.46178394 -1044.74098929  2687.21718658]\n",
      "Old weights: [ 1.00027261  0.99973659  0.99939062 -0.31856711  1.00020599  1.00085996\n",
      "  1.00041103  0.34519402  0.33623887  1.01293874  0.96664731]\n",
      "New weights: [ 1.00027481  0.99974126  0.99939226 -0.31857598  1.00020808  1.00086771\n",
      "  1.00042315  0.34518689  0.33623713  1.01304321  0.96637859]\n",
      "Loss: 751.8869777719271\n",
      "\n",
      "Gradient: [  -21.95923278   -46.61720126   -16.43697143    88.74474478\n",
      "   -20.86048712   -77.43314702  -121.11734709    71.29301953\n",
      "    17.45172525 -1044.15992631  2685.72262352]\n",
      "Old weights: [ 1.00027481  0.99974126  0.99939226 -0.31857598  1.00020808  1.00086771\n",
      "  1.00042315  0.34518689  0.33623713  1.01304321  0.96637859]\n",
      "New weights: [ 1.00027701  0.99974592  0.99939391 -0.31858486  1.00021017  1.00087546\n",
      "  1.00043526  0.34517976  0.33623538  1.01314763  0.96611002]\n",
      "Loss: 751.0531432365699\n",
      "\n",
      "Gradient: [  -21.94701958   -46.59057401   -16.42734323    88.69525784\n",
      "   -20.84856066   -77.38978099  -121.0494678     71.25356629\n",
      "    17.44169155 -1043.57918649  2684.22889168]\n",
      "Old weights: [ 1.00027701  0.99974592  0.99939391 -0.31858486  1.00021017  1.00087546\n",
      "  1.00043526  0.34517976  0.33623538  1.01314763  0.96611002]\n",
      "New weights: [ 1.0002792   0.99975058  0.99939555 -0.31859373  1.00021225  1.00088319\n",
      "  1.00044737  0.34517263  0.33623364  1.01325199  0.9658416 ]\n",
      "Loss: 750.2202359872622\n",
      "\n",
      "Gradient: [  -21.93481317   -46.56396155   -16.41772038    88.64580554\n",
      "   -20.83664085   -77.34643907  -120.98162627    71.2141238\n",
      "    17.43168174 -1042.99876966  2682.73599059]\n",
      "Old weights: [ 1.0002792   0.99975058  0.99939555 -0.31859373  1.00021225  1.00088319\n",
      "  1.00044737  0.34517263  0.33623364  1.01325199  0.9658416 ]\n",
      "New weights: [ 1.00028139  0.99975523  0.99939719 -0.31860259  1.00021434  1.00089093\n",
      "  1.00045947  0.34516551  0.3362319   1.01335629  0.96557332]\n",
      "Loss: 749.3882549927906\n",
      "\n",
      "Gradient: [  -21.92261355   -46.5373639    -16.40810288    88.59638746\n",
      "   -20.82472768   -77.30312125  -120.91382246    71.17469268\n",
      "    17.42169478 -1042.41867562  2681.24391979]\n",
      "Old weights: [ 1.00028139  0.99975523  0.99939719 -0.31860259  1.00021434  1.00089093\n",
      "  1.00045947  0.34516551  0.3362319   1.01335629  0.96557332]\n",
      "New weights: [ 1.00028359  0.99975989  0.99939883 -0.31861145  1.00021642  1.00089866\n",
      "  1.00047156  0.34515839  0.33623015  1.01346053  0.9653052 ]\n",
      "Loss: 748.5571992230884\n",
      "\n",
      "Gradient: [  -21.91042072   -46.51078102   -16.39849072    88.5470032\n",
      "   -20.81282113   -77.25982752  -120.84605637    71.13527351\n",
      "    17.41172969 -1041.83890422  2679.75267883]\n",
      "Old weights: [ 1.00028359  0.99975989  0.99939883 -0.31861145  1.00021642  1.00089866\n",
      "  1.00047156  0.34515839  0.33623015  1.01346053  0.9653052 ]\n",
      "New weights: [ 1.00028578  0.99976454  0.99940047 -0.31862031  1.0002185   1.00090638\n",
      "  1.00048364  0.34515128  0.33622841  1.01356471  0.96503722]\n",
      "Loss: 747.7270676492368\n",
      "\n",
      "Gradient: [  -21.89823467   -46.48421293   -16.38888391    88.4976524\n",
      "   -20.80092122   -77.21655786  -120.77832797    71.09586685\n",
      "    17.40178555 -1041.25945526  2678.26226724]\n",
      "Old weights: [ 1.00028578  0.99976454  0.99940047 -0.31862031  1.0002185   1.00090638\n",
      "  1.00048364  0.34515128  0.33622841  1.01356471  0.96503722]\n",
      "New weights: [ 1.00028797  0.99976919  0.99940211 -0.31862916  1.00022058  1.00091411\n",
      "  1.00049572  0.34514417  0.33622667  1.01366884  0.9647694 ]\n",
      "Loss: 746.897859243469\n",
      "\n",
      "Gradient: [  -21.88605539   -46.45765961   -16.37928244    88.44833469\n",
      "   -20.78902793   -77.17331226  -120.71063724    71.05647321\n",
      "    17.39186148 -1040.68032858  2676.77268457]\n",
      "Old weights: [ 1.00028797  0.99976919  0.99940211 -0.31862916  1.00022058  1.00091411\n",
      "  1.00049572  0.34514417  0.33622667  1.01366884  0.9647694 ]\n",
      "New weights: [ 1.00029016  0.99977383  0.99940375 -0.318638    1.00022266  1.00092182\n",
      "  1.00050779  0.34513706  0.33622493  1.01377291  0.96450172]\n",
      "Loss: 746.0695729791336\n",
      "\n",
      "Gradient: [  -21.87388289   -46.43112105   -16.36968631    88.39904974\n",
      "   -20.77714127   -77.13009071  -120.64298415    71.01709309\n",
      "    17.38195665 -1040.10152398  2675.28393035]\n",
      "Old weights: [ 1.00029016  0.99977383  0.99940375 -0.318638    1.00022266  1.00092182\n",
      "  1.00050779  0.34513706  0.33622493  1.01377291  0.96450172]\n",
      "New weights: [ 1.00029234  0.99977848  0.99940539 -0.31864684  1.00022474  1.00092954\n",
      "  1.00051986  0.34512996  0.33622319  1.01387692  0.96423419]\n",
      "Loss: 745.2422078307737\n",
      "\n",
      "Gradient: [  -21.86171716   -46.40459725   -16.36009551    88.34979724\n",
      "   -20.76526122   -77.08689319  -120.5753687     70.97772695\n",
      "    17.37207029 -1039.5230413   2673.79600414]\n",
      "Old weights: [ 1.00029234  0.99977848  0.99940539 -0.31864684  1.00022474  1.00092954\n",
      "  1.00051986  0.34512996  0.33622319  1.01387692  0.96423419]\n",
      "New weights: [ 1.00029453  0.99978312  0.99940702 -0.31865568  1.00022681  1.00093725\n",
      "  1.00053191  0.34512287  0.33622146  1.01398087  0.96396681]\n",
      "Loss: 744.4157627739977\n",
      "\n",
      "Gradient: [  -21.84955819   -46.37808819   -16.35051004    88.30057689\n",
      "   -20.75338778   -77.04371969  -120.50779085    70.93837523\n",
      "    17.36220166 -1038.94488035  2672.30890546]\n",
      "Old weights: [ 1.00029453  0.99978312  0.99940702 -0.31865568  1.00022681  1.00093725\n",
      "  1.00053191  0.34512287  0.33622146  1.01398087  0.96396681]\n",
      "New weights: [ 1.00029671  0.99978775  0.99940866 -0.31866451  1.00022889  1.00094495\n",
      "  1.00054396  0.34511577  0.33621972  1.01408476  0.96369958]\n",
      "Loss: 743.5902367856341\n",
      "\n",
      "Gradient: [  -21.83740599   -46.35159388   -16.3409299     88.2513884\n",
      "   -20.74152095   -77.00057021  -120.44025059    70.89903833\n",
      "    17.35235007 -1038.36704096  2670.82263387]\n",
      "Old weights: [ 1.00029671  0.99978775  0.99940866 -0.31866451  1.00022889  1.00094495\n",
      "  1.00054396  0.34511577  0.33621972  1.01408476  0.96369958]\n",
      "New weights: [ 1.0002989   0.99979239  0.99941029 -0.31867333  1.00023096  1.00095265\n",
      "  1.00055601  0.34510868  0.33621799  1.0141886   0.9634325 ]\n",
      "Loss: 742.765628843583\n",
      "\n",
      "Gradient: [  -21.82526054   -46.32511429   -16.33135509    88.2022315\n",
      "   -20.72966073   -76.95744471  -120.37274789    70.85971665\n",
      "    17.34251485 -1037.78952295  2669.3371889 ]\n",
      "Old weights: [ 1.0002989   0.99979239  0.99941029 -0.31867333  1.00023096  1.00095265\n",
      "  1.00055601  0.34510868  0.33621799  1.0141886   0.9634325 ]\n",
      "New weights: [ 1.00030108  0.99979702  0.99941192 -0.31868215  1.00023303  1.00096035\n",
      "  1.00056805  0.3451016   0.33621625  1.01429238  0.96316557]\n",
      "Loss: 741.941937926919\n",
      "\n",
      "Gradient: [  -21.81312186   -46.29864943   -16.32178561    88.15310594\n",
      "   -20.71780711   -76.9143432   -120.30528274    70.82041055\n",
      "    17.3326954  -1037.21232614  2667.8525701 ]\n",
      "Old weights: [ 1.00030108  0.99979702  0.99941192 -0.31868215  1.00023303  1.00096035\n",
      "  1.00056805  0.3451016   0.33621625  1.01429238  0.96316557]\n",
      "New weights: [ 1.00030326  0.99980165  0.99941356 -0.31869097  1.00023511  1.00096804\n",
      "  1.00058008  0.34509451  0.33621452  1.0143961   0.96289878]\n",
      "Loss: 741.119163015835\n",
      "\n",
      "Gradient: [  -21.80098992   -46.27219929   -16.31222144    88.10401148\n",
      "   -20.70596009   -76.87126566  -120.23785511    70.78112036\n",
      "    17.32289111 -1036.63545034  2666.368777  ]\n",
      "Old weights: [ 1.00030326  0.99980165  0.99941356 -0.31869097  1.00023511  1.00096804\n",
      "  1.00058008  0.34509451  0.33621452  1.0143961   0.96289878]\n",
      "New weights: [ 1.00030544  0.99980628  0.99941519 -0.31869978  1.00023718  1.00097572\n",
      "  1.0005921   0.34508744  0.33621279  1.01449976  0.96263214]\n",
      "Loss: 740.297303091669\n",
      "\n",
      "Gradient: [  -21.78886473   -46.24576385   -16.30266259    88.05494789\n",
      "   -20.69411965   -76.82821207  -120.17046499    70.74184643\n",
      "    17.31310146 -1036.0588954   2664.88580915]\n",
      "Old weights: [ 1.00030544  0.99980628  0.99941519 -0.31869978  1.00023718  1.00097572\n",
      "  1.0005921   0.34508744  0.33621279  1.01449976  0.96263214]\n",
      "New weights: [ 1.00030762  0.9998109   0.99941682 -0.31870858  1.00023925  1.00098341\n",
      "  1.00060412  0.34508036  0.33621105  1.01460337  0.96236566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 739.4763571368931\n",
      "\n",
      "Gradient: [  -21.77674628   -46.21934312   -16.29310906    88.00591494\n",
      "   -20.68228581   -76.78518243  -120.10311234    70.70258904\n",
      "    17.3033259  -1035.48266112  2663.4036661 ]\n",
      "Old weights: [ 1.00030762  0.9998109   0.99941682 -0.31870858  1.00023925  1.00098341\n",
      "  1.00060412  0.34508036  0.33621105  1.01460337  0.96236566]\n",
      "New weights: [ 1.0003098   0.99981552  0.99941845 -0.31871738  1.00024131  1.00099109\n",
      "  1.00061613  0.34507329  0.33620932  1.01470692  0.96209931]\n",
      "Loss: 738.6563241350889\n",
      "\n",
      "Gradient: [  -21.76463457   -46.19293707   -16.28356084    87.95691245\n",
      "   -20.67045856   -76.74217671  -120.03579716    70.6633485\n",
      "    17.29356395 -1034.90674733  2661.92234737]\n",
      "Old weights: [ 1.0003098   0.99981552  0.99941845 -0.31871738  1.00024131  1.00099109\n",
      "  1.00061613  0.34507329  0.33620932  1.01470692  0.96209931]\n",
      "New weights: [ 1.00031197  0.99982014  0.99942008 -0.31872618  1.00024338  1.00099876\n",
      "  1.00062813  0.34506622  0.33620759  1.01481041  0.96183312]\n",
      "Loss: 737.837203071\n",
      "\n",
      "Gradient: [  -21.7525296    -46.16654571   -16.27401793    87.9079402\n",
      "   -20.65863788   -76.69919492  -119.96851942    70.62412506\n",
      "    17.28381514 -1034.33115385  2660.44185253]\n",
      "Old weights: [ 1.00031197  0.99982014  0.99942008 -0.31872618  1.00024338  1.00099876\n",
      "  1.00062813  0.34506622  0.33620759  1.01481041  0.96183312]\n",
      "New weights: [ 1.00031415  0.99982476  0.9994217  -0.31873497  1.00024545  1.00100643\n",
      "  1.00064013  0.34505916  0.33620587  1.01491384  0.96156708]\n",
      "Loss: 737.0189929304725\n",
      "\n",
      "Gradient: [  -21.74043136   -46.14016902   -16.26448032    87.85899802\n",
      "   -20.64682379   -76.65623702  -119.90127909    70.58491898\n",
      "    17.27407904 -1033.7558805   2658.96218111]\n",
      "Old weights: [ 1.00031415  0.99982476  0.9994217  -0.31873497  1.00024545  1.00100643\n",
      "  1.00064013  0.34505916  0.33620587  1.01491384  0.96156708]\n",
      "New weights: [ 1.00031632  0.99982937  0.99942333 -0.31874376  1.00024751  1.0010141\n",
      "  1.00065212  0.3450521   0.33620414  1.01501722  0.96130118]\n",
      "Loss: 736.2016927004918\n",
      "\n",
      "Gradient: [  -21.72833985   -46.11380701   -16.25494802    87.81008574\n",
      "   -20.63501627   -76.61330302  -119.83407617    70.54573051\n",
      "    17.26435522 -1033.18092711  2657.48333265]\n",
      "Old weights: [ 1.00031632  0.99982937  0.99942333 -0.31874376  1.00024751  1.0010141\n",
      "  1.00065212  0.3450521   0.33620414  1.01501722  0.96130118]\n",
      "New weights: [ 1.0003185   0.99983399  0.99942496 -0.31875254  1.00024958  1.00102176\n",
      "  1.0006641   0.34504505  0.33620241  1.01512053  0.96103543]\n",
      "Loss: 735.3853013691756\n",
      "\n",
      "Gradient: [  -21.71625507   -46.08745965   -16.24542102    87.76120317\n",
      "   -20.62321531   -76.57039289  -119.76691062    70.50655987\n",
      "    17.25464331 -1032.60629349  2656.00530669]\n",
      "Old weights: [ 1.0003185   0.99983399  0.99942496 -0.31875254  1.00024958  1.00102176\n",
      "  1.0006641   0.34504505  0.33620241  1.01512053  0.96103543]\n",
      "New weights: [ 1.00032067  0.9998386   0.99942658 -0.31876131  1.00025164  1.00102941\n",
      "  1.00067608  0.345038    0.33620069  1.01522379  0.96076983]\n",
      "Loss: 734.5698179257555\n",
      "\n",
      "Gradient: [  -21.70417701   -46.06112694   -16.23589932    87.71235018\n",
      "   -20.61142093   -76.52750662  -119.69978243    70.46740727\n",
      "    17.24494292 -1032.03197948  2654.52810279]\n",
      "Old weights: [ 1.00032067  0.9998386   0.99942658 -0.31876131  1.00025164  1.00102941\n",
      "  1.00067608  0.345038    0.33620069  1.01522379  0.96076983]\n",
      "New weights: [ 1.00032284  0.9998432   0.9994282  -0.31877009  1.0002537   1.00103707\n",
      "  1.00068805  0.34503095  0.33619896  1.015327    0.96050438]\n",
      "Loss: 733.7552413605988\n",
      "\n",
      "Gradient: [  -21.69210566   -46.03480888   -16.22638291    87.66352661\n",
      "   -20.59963311   -76.48464421  -119.63269157    70.4282729\n",
      "    17.2352537  -1031.45798488  2653.05172048]\n",
      "Old weights: [ 1.00032284  0.9998432   0.9994282  -0.31877009  1.0002537   1.00103707\n",
      "  1.00068805  0.34503095  0.33619896  1.015327    0.96050438]\n",
      "New weights: [ 1.00032501  0.9998478   0.99942983 -0.31877885  1.00025576  1.00104472\n",
      "  1.00070001  0.34502391  0.33619724  1.01543014  0.96023908]\n",
      "Loss: 732.9415706651821\n",
      "\n",
      "Gradient: [  -21.68004103   -46.00850545   -16.2168718     87.61473231\n",
      "   -20.58785184   -76.44180564  -119.56563803    70.38915697\n",
      "    17.22557534 -1030.88430954  2651.5761593 ]\n",
      "Old weights: [ 1.00032501  0.9998478   0.99942983 -0.31877885  1.00025576  1.00104472\n",
      "  1.00070001  0.34502391  0.33619724  1.01543014  0.96023908]\n",
      "New weights: [ 1.00032718  0.99985241  0.99943145 -0.31878761  1.00025782  1.00105236\n",
      "  1.00071197  0.34501687  0.33619552  1.01553323  0.95997392]\n",
      "Loss: 732.1288048321188\n",
      "\n",
      "Gradient: [  -21.66798311   -45.98221666   -16.20736597    87.56596715\n",
      "   -20.57607713   -76.39899088  -119.49862179    70.35005964\n",
      "    17.21590751 -1030.31095326  2650.10141881]\n",
      "Old weights: [ 1.00032718  0.99985241  0.99943145 -0.31878761  1.00025782  1.00105236\n",
      "  1.00071197  0.34501687  0.33619552  1.01553323  0.95997392]\n",
      "New weights: [ 1.00032934  0.999857    0.99943307 -0.31879637  1.00025988  1.00106\n",
      "  1.00072392  0.34500984  0.3361938   1.01563626  0.95970891]\n",
      "Loss: 731.3169428551176\n",
      "\n",
      "Gradient: [  -21.65593189   -45.95594248   -16.19786543    87.51723101\n",
      "   -20.56430897   -76.35619994  -119.43164282    70.31098109\n",
      "    17.20624992 -1029.73791587  2648.62749855]\n",
      "Old weights: [ 1.00032934  0.999857    0.99943307 -0.31879637  1.00025988  1.00106\n",
      "  1.00072392  0.34500984  0.3361938   1.01563626  0.95970891]\n",
      "New weights: [ 1.00033151  0.9998616   0.99943469 -0.31880512  1.00026193  1.00106763\n",
      "  1.00073586  0.3450028   0.33619207  1.01573924  0.95944404]\n",
      "Loss: 730.5059837290369\n",
      "\n",
      "Gradient: [  -21.64388738   -45.92968291   -16.18837017    87.46852375\n",
      "   -20.55254736   -76.3134328   -119.3647011     70.27192145\n",
      "    17.1966023  -1029.1651972   2647.15439805]\n",
      "Old weights: [ 1.00033151  0.9998616   0.99943469 -0.31880512  1.00026193  1.00106763\n",
      "  1.00073586  0.3450028   0.33619207  1.01573924  0.95944404]\n",
      "New weights: [ 1.00033367  0.99986619  0.99943631 -0.31881387  1.00026399  1.00107527\n",
      "  1.0007478   0.34499578  0.33619035  1.01584215  0.95917933]\n",
      "Loss: 729.6959264498372\n",
      "\n",
      "Gradient: [  -21.63184957   -45.90343795   -16.17888019    87.41984527\n",
      "   -20.5407923    -76.27068944  -119.29779662    70.2328809\n",
      "    17.18696437 -1028.59279707  2645.68211687]\n",
      "Old weights: [ 1.00033367  0.99986619  0.99943631 -0.31881387  1.00026399  1.00107527\n",
      "  1.0007478   0.34499578  0.33619035  1.01584215  0.95917933]\n",
      "New weights: [ 1.00033584  0.99987078  0.99943792 -0.31882261  1.00026604  1.00108289\n",
      "  1.00075973  0.34498875  0.33618864  1.01594501  0.95891476]\n",
      "Loss: 728.8867700145919\n",
      "\n",
      "Gradient: [  -21.61981845   -45.87720758   -16.16939549    87.37119546\n",
      "   -20.52904377   -76.22796986  -119.23092934    70.19385956\n",
      "    17.1773359  -1028.02071529  2644.21065455]\n",
      "Old weights: [ 1.00033584  0.99987078  0.99943792 -0.31882261  1.00026604  1.00108289\n",
      "  1.00075973  0.34498875  0.33618864  1.01594501  0.95891476]\n",
      "New weights: [ 1.000338    0.99987537  0.99943954 -0.31883135  1.00026809  1.00109052\n",
      "  1.00077165  0.34498173  0.33618692  1.01604781  0.95865034]\n",
      "Loss: 728.0785134215054\n",
      "\n",
      "Gradient: [  -21.60779402   -45.8509918    -16.15991607    87.3225742\n",
      "   -20.51730178   -76.18527403  -119.16409926    70.15485755\n",
      "    17.16771665 -1027.4489517   2642.74001064]\n",
      "Old weights: [ 1.000338    0.99987537  0.99943954 -0.31883135  1.00026809  1.00109052\n",
      "  1.00077165  0.34498173  0.33618692  1.01604781  0.95865034]\n",
      "New weights: [ 1.00034016  0.99987996  0.99944116 -0.31884008  1.00027015  1.00109813\n",
      "  1.00078357  0.34497472  0.3361852   1.01615056  0.95838607]\n",
      "Loss: 727.2711556698583\n",
      "\n",
      "Gradient: [  -21.59577629   -45.8247906    -16.15044192    87.2739814\n",
      "   -20.50556632   -76.14260195  -119.09730635    70.11587501\n",
      "    17.15810639 -1026.87750611  2641.27018468]\n",
      "Old weights: [ 1.00034016  0.99987996  0.99944116 -0.31884008  1.00027015  1.00109813\n",
      "  1.00078357  0.34497472  0.3361852   1.01615056  0.95838607]\n",
      "New weights: [ 1.00034232  0.99988454  0.99944277 -0.31884881  1.0002722   1.00110575\n",
      "  1.00079548  0.34496771  0.33618349  1.01625325  0.95812194]\n",
      "Loss: 726.4646957601141\n",
      "\n",
      "Gradient: [  -21.58376523   -45.79860397   -16.14097303    87.22541697\n",
      "   -20.49383739   -76.0999536   -119.03055058    70.07691205\n",
      "    17.14850492 -1026.30637836  2639.80117621]\n",
      "Old weights: [ 1.00034232  0.99988454  0.99944277 -0.31884881  1.0002722   1.00110575\n",
      "  1.00079548  0.34496771  0.33618349  1.01625325  0.95812194]\n",
      "New weights: [ 1.00034448  0.99988912  0.99944439 -0.31885753  1.00027425  1.00111336\n",
      "  1.00080738  0.3449607   0.33618177  1.01635588  0.95785796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 725.6591326937713\n",
      "\n",
      "Gradient: [  -21.57176086   -45.7724319    -16.13150942    87.17688081\n",
      "   -20.48211499   -76.05732898  -118.96383195    70.03796877\n",
      "    17.13891204 -1025.73556826  2638.33298479]\n",
      "Old weights: [ 1.00034448  0.99988912  0.99944439 -0.31885753  1.00027425  1.00111336\n",
      "  1.00080738  0.3449607   0.33618177  1.01635588  0.95785796]\n",
      "New weights: [ 1.00034663  0.9998937   0.999446   -0.31886625  1.00027629  1.00112096\n",
      "  1.00081928  0.3449537   0.33618006  1.01645845  0.95759413]\n",
      "Loss: 724.8544654734931\n",
      "\n",
      "Gradient: [  -21.55976316   -45.7462744    -16.12205106    87.12837284\n",
      "   -20.4703991    -76.01472805  -118.89715043    69.99904528\n",
      "    17.12932756 -1025.16507563  2636.86560995]\n",
      "Old weights: [ 1.00034663  0.9998937   0.999446   -0.31886625  1.00027629  1.00112096\n",
      "  1.00081928  0.3449537   0.33618006  1.01645845  0.95759413]\n",
      "New weights: [ 1.00034879  0.99989827  0.99944761 -0.31887496  1.00027834  1.00112857\n",
      "  1.00083117  0.3449467   0.33617834  1.01656097  0.95733044]\n",
      "Loss: 724.0506931030137\n",
      "\n",
      "Gradient: [  -21.54777214   -45.72013143   -16.11259797    87.07989297\n",
      "   -20.45868974   -75.97215082  -118.83050599    69.96014166\n",
      "    17.11975131 -1024.59490031  2635.39905125]\n",
      "Old weights: [ 1.00034879  0.99989827  0.99944761 -0.31887496  1.00027834  1.00112857\n",
      "  1.00083117  0.3449467   0.33617834  1.01656097  0.95733044]\n",
      "New weights: [ 1.00035094  0.99990284  0.99944922 -0.31888367  1.00028039  1.00113616\n",
      "  1.00084305  0.3449397   0.33617663  1.01666343  0.9570669 ]\n",
      "Loss: 723.2478145872119\n",
      "\n",
      "Gradient: [  -21.53578778   -45.69400301   -16.10315013    87.03144113\n",
      "   -20.44698689   -75.92959727  -118.76389862    69.92125801\n",
      "    17.11018312 -1024.02504211  2633.93330823]\n",
      "Old weights: [ 1.00035094  0.99990284  0.99944922 -0.31888367  1.00028039  1.00113616\n",
      "  1.00084305  0.3449397   0.33617663  1.01666343  0.9570669 ]\n",
      "New weights: [ 1.0003531   0.99990741  0.99945083 -0.31889237  1.00028243  1.00114376\n",
      "  1.00085493  0.34493271  0.33617492  1.01676583  0.95680351]\n",
      "Loss: 722.4458289320364\n",
      "\n",
      "Gradient: [  -21.52381009   -45.66788912   -16.09370755    86.98301724\n",
      "   -20.43529054   -75.88706739  -118.6973283     69.88239441\n",
      "    17.10062283 -1023.45550085  2632.46838044]\n",
      "Old weights: [ 1.0003531   0.99990741  0.99945083 -0.31889237  1.00028243  1.00114376\n",
      "  1.00085493  0.34493271  0.33617492  1.01676583  0.95680351]\n",
      "New weights: [ 1.00035525  0.99991198  0.99945244 -0.31890107  1.00028448  1.00115134\n",
      "  1.00086679  0.34492572  0.33617321  1.01686818  0.95654026]\n",
      "Loss: 721.6447351445726\n",
      "\n",
      "Gradient: [  -21.51183906   -45.64178975   -16.08427021    86.93462123\n",
      "   -20.42360071   -75.84456116  -118.630795      69.84355093\n",
      "    17.0910703  -1022.88627637  2631.00426742]\n",
      "Old weights: [ 1.00035525  0.99991198  0.99945244 -0.31890107  1.00028448  1.00115134\n",
      "  1.00086679  0.34492572  0.33617321  1.01686818  0.95654026]\n",
      "New weights: [ 1.0003574   0.99991654  0.99945405 -0.31890976  1.00028652  1.00115893\n",
      "  1.00087866  0.34491874  0.3361715   1.01697046  0.95627716]\n",
      "Loss: 720.8445322329895\n",
      "\n",
      "Gradient: [  -21.49987469   -45.6157049    -16.07483813    86.88625303\n",
      "   -20.41191738   -75.80207857  -118.56429871    69.80472766\n",
      "    17.08152537 -1022.31736849  2629.54096872]\n",
      "Old weights: [ 1.0003574   0.99991654  0.99945405 -0.31890976  1.00028652  1.00115893\n",
      "  1.00087866  0.34491874  0.3361715   1.01697046  0.95627716]\n",
      "New weights: [ 1.00035955  0.9999211   0.99945566 -0.31891845  1.00028856  1.00116651\n",
      "  1.00089051  0.34491175  0.33616979  1.0170727   0.9560142 ]\n",
      "Loss: 720.0452192065723\n",
      "\n",
      "Gradient: [  -21.48791698   -45.58963455   -16.06541129    86.83791258\n",
      "   -20.40024054   -75.75961961  -118.4978394     69.76592465\n",
      "    17.07198793 -1021.74877702  2628.0784839 ]\n",
      "Old weights: [ 1.00035955  0.9999211   0.99945566 -0.31891845  1.00028856  1.00116651\n",
      "  1.00089051  0.34491175  0.33616979  1.0170727   0.9560142 ]\n",
      "New weights: [ 1.0003617   0.99992566  0.99945727 -0.31892714  1.0002906   1.00117409\n",
      "  1.00090236  0.34490478  0.33616809  1.01717487  0.9557514 ]\n",
      "Loss: 719.246795075691\n",
      "\n",
      "Gradient: [  -21.47596591   -45.56357871   -16.0559897     86.78959981\n",
      "   -20.38857021   -75.71718427  -118.43141705    69.72714197\n",
      "    17.06245784 -1021.1805018   2626.61681249]\n",
      "Old weights: [ 1.0003617   0.99992566  0.99945727 -0.31892714  1.0002906   1.00117409\n",
      "  1.00090236  0.34490478  0.33616809  1.01717487  0.9557514 ]\n",
      "New weights: [ 1.00036385  0.99993022  0.99945887 -0.31893581  1.00029264  1.00118166\n",
      "  1.00091421  0.34489781  0.33616638  1.01727699  0.95548874]\n",
      "Loss: 718.4492588518443\n",
      "\n",
      "Gradient: [  -21.4640215    -45.53753736   -16.04657335    86.74131466\n",
      "   -20.37690636   -75.67477252  -118.36503165    69.68837967\n",
      "    17.05293498 -1020.61254265  2625.15595404]\n",
      "Old weights: [ 1.00036385  0.99993022  0.99945887 -0.31893581  1.00029264  1.00118166\n",
      "  1.00091421  0.34489781  0.33616638  1.01727699  0.95548874]\n",
      "New weights: [ 1.00036599  0.99993477  0.99946048 -0.31894449  1.00029468  1.00118922\n",
      "  1.00092604  0.34489084  0.33616468  1.01737905  0.95522622]\n",
      "Loss: 717.652609547596\n",
      "\n",
      "Gradient: [  -21.45208372   -45.51151048   -16.03716223    86.69305707\n",
      "   -20.365249     -75.63238437  -118.29868318    69.64963782\n",
      "    17.04341925 -1020.04489939  2623.69590811]\n",
      "Old weights: [ 1.00036599  0.99993477  0.99946048 -0.31894449  1.00029468  1.00118922\n",
      "  1.00092604  0.34489084  0.33616468  1.01737905  0.95522622]\n",
      "New weights: [ 1.00036814  0.99993932  0.99946208 -0.31895316  1.00029671  1.00119679\n",
      "  1.00093787  0.34488387  0.33616297  1.01748105  0.95496385]\n",
      "Loss: 716.8568461766362\n",
      "\n",
      "Gradient: [  -21.44015259   -45.48549809   -16.02775635    86.644827\n",
      "   -20.35359813   -75.59001979  -118.2323716     69.61091646\n",
      "    17.03391054 -1019.47757184  2622.23667424]\n",
      "Old weights: [ 1.00036814  0.99993932  0.99946208 -0.31895316  1.00029671  1.00119679\n",
      "  1.00093787  0.34488387  0.33616297  1.01748105  0.95496385]\n",
      "New weights: [ 1.00037028  0.99994387  0.99946368 -0.31896182  1.00029875  1.00120435\n",
      "  1.0009497   0.34487691  0.33616127  1.017583    0.95470163]\n",
      "Loss: 716.0619677537346\n",
      "\n",
      "Gradient: [  -21.42822809   -45.45950016   -16.01835569    86.59662439\n",
      "   -20.34195374   -75.54767877  -118.16609691    69.57221565\n",
      "    17.02440874 -1018.91055984  2620.77825198]\n",
      "Old weights: [ 1.00037028  0.99994387  0.99946368 -0.31896182  1.00029875  1.00120435\n",
      "  1.0009497   0.34487691  0.33616127  1.017583    0.95470163]\n",
      "New weights: [ 1.00037243  0.99994842  0.99946528 -0.31897048  1.00030078  1.0012119\n",
      "  1.00096151  0.34486995  0.33615956  1.01768489  0.95443955]\n",
      "Loss: 715.2679732947647\n",
      "\n",
      "Gradient: [  -21.41631022   -45.43351669   -16.00896027    86.54844917\n",
      "   -20.33031582   -75.5053613   -118.09985908    69.53353542\n",
      "    17.01491376 -1018.3438632   2619.32064088]\n",
      "Old weights: [ 1.00037243  0.99994842  0.99946528 -0.31897048  1.00030078  1.0012119\n",
      "  1.00096151  0.34486995  0.33615956  1.01768489  0.95443955]\n",
      "New weights: [ 1.00037457  0.99995296  0.99946689 -0.31897914  1.00030281  1.00121945\n",
      "  1.00097332  0.344863    0.33615786  1.01778673  0.95417762]\n",
      "Loss: 714.4748618166983\n",
      "\n",
      "Gradient: [  -21.40439898   -45.40754767   -15.99957007    86.50030132\n",
      "   -20.31868438   -75.46306736  -118.03365809    69.49487581\n",
      "    17.00542551 -1017.77748176  2617.86384049]\n",
      "Old weights: [ 1.00037457  0.99995296  0.99946689 -0.31897914  1.00030281  1.00121945\n",
      "  1.00097332  0.344863    0.33615786  1.01778673  0.95417762]\n",
      "New weights: [ 1.00037671  0.9999575   0.99946848 -0.31898779  1.00030485  1.001227\n",
      "  1.00098513  0.34485605  0.33615616  1.01788851  0.95391583]\n",
      "Loss: 713.6826323375866\n",
      "\n",
      "Gradient: [  -21.39249437   -45.3815931    -15.9901851     86.45218078\n",
      "   -20.30705941   -75.42079695  -117.96749392    69.45623688\n",
      "    16.99594391 -1017.21141533  2616.40785035]\n",
      "Old weights: [ 1.00037671  0.9999575   0.99946848 -0.31898779  1.00030485  1.001227\n",
      "  1.00098513  0.34485605  0.33615616  1.01788851  0.95391583]\n",
      "New weights: [ 1.00037885  0.99996204  0.99947008 -0.31899643  1.00030688  1.00123454\n",
      "  1.00099692  0.3448491   0.33615446  1.01799023  0.95365419]\n",
      "Loss: 712.8912838765774\n",
      "\n",
      "Gradient: [  -21.38059638   -45.35565296   -15.98080534    86.40408751\n",
      "   -20.29544091   -75.37855005  -117.90136655    69.41761864\n",
      "    16.98646887 -1016.64566374  2614.95267003]\n",
      "Old weights: [ 1.00037885  0.99996204  0.99947008 -0.31899643  1.00030688  1.00123454\n",
      "  1.00099692  0.3448491   0.33615446  1.01799023  0.95365419]\n",
      "New weights: [ 1.00038099  0.99996658  0.99947168 -0.31900507  1.00030891  1.00124208\n",
      "  1.00100871  0.34484216  0.33615276  1.01809189  0.95339269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 712.1008154539261\n",
      "\n",
      "Gradient: [  -21.36870501   -45.32972725   -15.9714308     86.35602146\n",
      "   -20.28382886   -75.33632665  -117.83527596    69.37902113\n",
      "    16.97700033 -1016.08022681  2613.49829905]\n",
      "Old weights: [ 1.00038099  0.99996658  0.99947168 -0.31900507  1.00030891  1.00124208\n",
      "  1.00100871  0.34484216  0.33615276  1.01809189  0.95339269]\n",
      "New weights: [ 1.00038312  0.99997111  0.99947328 -0.31901371  1.00031093  1.00124961\n",
      "  1.0010205   0.34483523  0.33615107  1.0181935   0.95313134]\n",
      "Loss: 711.3112260909498\n",
      "\n",
      "Gradient: [  -21.35682025   -45.30381596   -15.96206148    86.30798259\n",
      "   -20.27222328   -75.29412673  -117.76922213    69.34044439\n",
      "    16.9675382  -1015.51510438  2612.04473699]\n",
      "Old weights: [ 1.00038312  0.99997111  0.99947328 -0.31901371  1.00031093  1.00124961\n",
      "  1.0010205   0.34483523  0.33615107  1.0181935   0.95313134]\n",
      "New weights: [ 1.00038526  0.99997564  0.99947488 -0.31902234  1.00031296  1.00125714\n",
      "  1.00103227  0.34482829  0.33614937  1.01829505  0.95287014]\n",
      "Loss: 710.5225148100814\n",
      "\n",
      "Gradient: [  -21.3449421    -45.27791908   -15.95269736    86.25997086\n",
      "   -20.26062415   -75.25195028  -117.70320504    69.30188844\n",
      "    16.95808242 -1014.95029626  2610.59198338]\n",
      "Old weights: [ 1.00038526  0.99997564  0.99947488 -0.31902234  1.00031296  1.00125714\n",
      "  1.00103227  0.34482829  0.33614937  1.01829505  0.95287014]\n",
      "New weights: [ 1.00038739  0.99998017  0.99947647 -0.31903096  1.00031499  1.00126467\n",
      "  1.00104404  0.34482136  0.33614767  1.01839655  0.95260908]\n",
      "Loss: 709.7346806348171\n",
      "\n",
      "Gradient: [  -21.33307055   -45.2520366    -15.94333845    86.21198624\n",
      "   -20.24903148   -75.20979729  -117.63722466    69.26335331\n",
      "    16.94863293 -1014.38580228  2609.14003778]\n",
      "Old weights: [ 1.00038739  0.99998017  0.99947647 -0.31903096  1.00031499  1.00126467\n",
      "  1.00104404  0.34482136  0.33614767  1.01839655  0.95260908]\n",
      "New weights: [ 1.00038953  0.99998469  0.99947807 -0.31903959  1.00031701  1.00127219\n",
      "  1.00105581  0.34481443  0.33614598  1.01849798  0.95234817]\n",
      "Loss: 708.9477225897564\n",
      "\n",
      "Gradient: [  -21.32120561   -45.22616852   -15.93398475    86.16402868\n",
      "   -20.23744525   -75.16766774  -117.57128099    69.22483902\n",
      "    16.93918966 -1013.82162227  2607.68889974]\n",
      "Old weights: [ 1.00038953  0.99998469  0.99947807 -0.31903959  1.00031701  1.00127219\n",
      "  1.00105581  0.34481443  0.33614598  1.01849798  0.95234817]\n",
      "New weights: [ 1.00039166  0.99998922  0.99947966 -0.3190482   1.00031904  1.0012797\n",
      "  1.00106757  0.34480751  0.33614429  1.01859937  0.9520874 ]\n",
      "Loss: 708.1616397005703\n",
      "\n",
      "Gradient: [  -21.30934727   -45.20031482   -15.92463626    86.11609815\n",
      "   -20.22586546   -75.12556163  -117.50537399    69.18634559\n",
      "    16.92975256 -1013.25775605  2606.2385688 ]\n",
      "Old weights: [ 1.00039166  0.99998922  0.99947966 -0.3190482   1.00031904  1.0012797\n",
      "  1.00106757  0.34480751  0.33614429  1.01859937  0.9520874 ]\n",
      "New weights: [ 1.00039379  0.99999374  0.99948125 -0.31905681  1.00032106  1.00128722\n",
      "  1.00107932  0.34480059  0.33614259  1.01870069  0.95182677]\n",
      "Loss: 707.3764309940229\n",
      "\n",
      "Gradient: [  -21.29749552   -45.17447551   -15.91529296    86.06819461\n",
      "   -20.21429212   -75.08347893  -117.43950365    69.14787304\n",
      "    16.92032157 -1012.69420345  2604.78904453]\n",
      "Old weights: [ 1.00039379  0.99999374  0.99948125 -0.31905681  1.00032106  1.00128722\n",
      "  1.00107932  0.34480059  0.33614259  1.01870069  0.95182677]\n",
      "New weights: [ 1.00039592  0.99999825  0.99948284 -0.31906542  1.00032308  1.00129473\n",
      "  1.00109106  0.34479368  0.3361409   1.01880196  0.95156629]\n",
      "Loss: 706.592095497961\n",
      "\n",
      "Gradient: [  -21.28565037   -45.14865056   -15.90595485    86.02031803\n",
      "   -20.20272522   -75.04141964  -117.37366994    69.10942139\n",
      "    16.91089664 -1012.13096429  2603.34032647]\n",
      "Old weights: [ 1.00039592  0.99999825  0.99948284 -0.31906542  1.00032308  1.00129473\n",
      "  1.00109106  0.34479368  0.3361409   1.01880196  0.95156629]\n",
      "New weights: [ 1.00039805  1.00000277  0.99948443 -0.31907402  1.0003251   1.00130223\n",
      "  1.0011028   0.34478677  0.33613921  1.01890318  0.95130596]\n",
      "Loss: 705.8086322413068\n",
      "\n",
      "Gradient: [  -21.2738118    -45.12283998   -15.89662194    85.97246838\n",
      "   -20.19116475   -74.99938375  -117.30787285    69.07099066\n",
      "    16.90147772 -1011.5680384   2601.89241417]\n",
      "Old weights: [ 1.00039805  1.00000277  0.99948443 -0.31907402  1.0003251   1.00130223\n",
      "  1.0011028   0.34478677  0.33613921  1.01890318  0.95130596]\n",
      "New weights: [ 1.00040018  1.00000728  0.99948602 -0.31908262  1.00032712  1.00130973\n",
      "  1.00111453  0.34477986  0.33613752  1.01900433  0.95104577]\n",
      "Loss: 705.0260402540637\n",
      "\n",
      "Gradient: [  -21.26197982   -45.09704376   -15.88729423    85.92464563\n",
      "   -20.17961071   -74.95737123  -117.24211236    69.03258086\n",
      "    16.89206477 -1011.0054256   2600.44530718]\n",
      "Old weights: [ 1.00040018  1.00000728  0.99948602 -0.31908262  1.00032712  1.00130973\n",
      "  1.00111453  0.34477986  0.33613752  1.01900433  0.95104577]\n",
      "New weights: [ 1.0004023   1.00001179  0.99948761 -0.31909121  1.00032914  1.00131723\n",
      "  1.00112625  0.34477296  0.33613583  1.01910543  0.95078573]\n",
      "Loss: 704.2443185673164\n",
      "\n",
      "Gradient: [  -21.25015442   -45.07126188   -15.8779717     85.87684974\n",
      "   -20.16806309   -74.91538208  -117.17638844    68.994192\n",
      "    16.88265774 -1010.44312572  2598.99900507]\n",
      "Old weights: [ 1.0004023   1.00001179  0.99948761 -0.31909121  1.00032914  1.00131723\n",
      "  1.00112625  0.34477296  0.33613583  1.01910543  0.95078573]\n",
      "New weights: [ 1.00040443  1.0000163   0.9994892  -0.3190998   1.00033115  1.00132472\n",
      "  1.00113797  0.34476606  0.33613414  1.01920648  0.95052583]\n",
      "Loss: 703.4634662132336\n",
      "\n",
      "Gradient: [  -21.2383356    -45.04549434   -15.86865435    85.82908068\n",
      "   -20.1565219    -74.87341628  -117.11070108    68.9558241\n",
      "    16.87325658 -1009.88113859  2597.55350737]\n",
      "Old weights: [ 1.00040443  1.0000163   0.9994892  -0.3190998   1.00033115  1.00132472\n",
      "  1.00113797  0.34476606  0.33613414  1.01920648  0.95052583]\n",
      "New weights: [ 1.00040655  1.0000208   0.99949079 -0.31910838  1.00033317  1.0013322\n",
      "  1.00114968  0.34475916  0.33613245  1.01930746  0.95026607]\n",
      "Loss: 702.6834822250289\n",
      "\n",
      "Gradient: [  -21.22652335   -45.01974113   -15.85934219    85.78133843\n",
      "   -20.14498713   -74.83147383  -117.04505025    68.91747716\n",
      "    16.86386127 -1009.31946403  2596.10881365]\n",
      "Old weights: [ 1.00040655  1.0000208   0.99949079 -0.31910838  1.00033317  1.0013322\n",
      "  1.00114968  0.34475916  0.33613245  1.01930746  0.95026607]\n",
      "New weights: [ 1.00040867  1.0000253   0.99949237 -0.31911696  1.00033519  1.00133969\n",
      "  1.00116139  0.34475227  0.33613077  1.0194084   0.95000646]\n",
      "Loss: 701.9043656370347\n",
      "\n",
      "Gradient: [  -21.21471767   -44.99400225   -15.8500352     85.73362296\n",
      "   -20.13345877   -74.7895547   -116.97943594    68.8791512\n",
      "    16.85447176 -1008.75810187  2594.66492346]\n",
      "Old weights: [ 1.00040867  1.0000253   0.99949237 -0.31911696  1.00033519  1.00133969\n",
      "  1.00116139  0.34475227  0.33613077  1.0194084   0.95000646]\n",
      "New weights: [ 1.00041079  1.0000298   0.99949396 -0.31912553  1.0003372   1.00134717\n",
      "  1.00117308  0.34474538  0.33612908  1.01950927  0.94974699]\n",
      "Loss: 701.1261154846277\n",
      "\n",
      "Gradient: [  -21.20291856   -44.96827768   -15.8407334     85.68593424\n",
      "   -20.12193683   -74.74765888  -116.91385812    68.84084623\n",
      "    16.84508802 -1008.19705193  2593.22183634]\n",
      "Old weights: [ 1.00041079  1.0000298   0.99949396 -0.31912553  1.0003372   1.00134717\n",
      "  1.00117308  0.34474538  0.33612908  1.01950927  0.94974699]\n",
      "New weights: [ 1.00041291  1.0000343   0.99949554 -0.3191341   1.00033921  1.00135464\n",
      "  1.00118477  0.3447385   0.3361274   1.01961009  0.94948767]\n",
      "Loss: 700.3487308042726\n",
      "\n",
      "Gradient: [  -21.191126     -44.94256742   -15.83143676    85.63827225\n",
      "   -20.11042129   -74.70578637  -116.84831678    68.80256224\n",
      "    16.83571    -1007.63631405  2591.77955185]\n",
      "Old weights: [ 1.00041291  1.0000343   0.99949554 -0.3191341   1.00033921  1.00135464\n",
      "  1.00118477  0.3447385   0.3361274   1.01961009  0.94948767]\n",
      "New weights: [ 1.00041503  1.00003879  0.99949712 -0.31914267  1.00034122  1.00136211\n",
      "  1.00119646  0.34473162  0.33612571  1.01971086  0.94922849]\n",
      "Loss: 699.5722106335031\n",
      "\n",
      "Gradient: [  -21.17934001   -44.91687146   -15.8221453     85.59063695\n",
      "   -20.09891216   -74.66393715  -116.78281189    68.76429926\n",
      "    16.82633769 -1007.07588804  2590.33806955]\n",
      "Old weights: [ 1.00041503  1.00003879  0.99949712 -0.31914267  1.00034122  1.00136211\n",
      "  1.00119646  0.34473162  0.33612571  1.01971086  0.94922849]\n",
      "New weights: [ 1.00041715  1.00004329  0.99949871 -0.31915123  1.00034323  1.00136958\n",
      "  1.00120814  0.34472474  0.33612403  1.01981156  0.94896946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 698.7965540109128\n",
      "\n",
      "Gradient: [  -21.16756058   -44.89118979   -15.812859      85.54302832\n",
      "   -20.08740944   -74.6221112   -116.71734343    68.72605727\n",
      "    16.81697105 -1006.51577373  2588.89738899]\n",
      "Old weights: [ 1.00041715  1.00004329  0.99949871 -0.31915123  1.00034323  1.00136958\n",
      "  1.00120814  0.34472474  0.33612403  1.01981156  0.94896946]\n",
      "New weights: [ 1.00041927  1.00004777  0.99950029 -0.31915978  1.00034524  1.00137704\n",
      "  1.00121981  0.34471787  0.33612235  1.01991222  0.94871057]\n",
      "Loss: 698.0217599761654\n",
      "\n",
      "Gradient: [  -21.15578769   -44.86552241   -15.80357787    85.49544635\n",
      "   -20.07591311   -74.58030852  -116.65191138    68.68783629\n",
      "    16.80761006 -1005.95597096  2587.45750973]\n",
      "Old weights: [ 1.00041927  1.00004777  0.99950029 -0.31915978  1.00034524  1.00137704\n",
      "  1.00121981  0.34471787  0.33612235  1.01991222  0.94871057]\n",
      "New weights: [ 1.00042138  1.00005226  0.99950187 -0.31916833  1.00034725  1.0013845\n",
      "  1.00123147  0.344711    0.33612067  1.02001281  0.94845182]\n",
      "Loss: 697.2478275700072\n",
      "\n",
      "Gradient: [  -21.14402135   -44.8398693    -15.79430191    85.44789099\n",
      "   -20.06442317   -74.53852909  -116.58651573    68.64963632\n",
      "    16.79825468 -1005.39647955  2586.01843131]\n",
      "Old weights: [ 1.00042138  1.00005226  0.99950187 -0.31916833  1.00034725  1.0013845\n",
      "  1.00123147  0.344711    0.33612067  1.02001281  0.94845182]\n",
      "New weights: [ 1.0004235   1.00005674  0.99950345 -0.31917687  1.00034925  1.00139195\n",
      "  1.00124313  0.34470414  0.33611899  1.02011335  0.94819322]\n",
      "Loss: 696.4747558342478\n",
      "\n",
      "Gradient: [  -21.13226156   -44.81423046   -15.7850311     85.40036223\n",
      "   -20.05293962   -74.49677289  -116.52115645    68.61145735\n",
      "    16.78890488 -1004.83729931  2584.5801533 ]\n",
      "Old weights: [ 1.0004235   1.00005674  0.99950345 -0.31917687  1.00034925  1.00139195\n",
      "  1.00124313  0.34470414  0.33611899  1.02011335  0.94819322]\n",
      "New weights: [ 1.00042561  1.00006123  0.99950503 -0.31918541  1.00035126  1.0013994\n",
      "  1.00125479  0.34469727  0.33611731  1.02021383  0.94793477]\n",
      "Loss: 695.7025438117446\n",
      "\n",
      "Gradient: [  -21.12050831   -44.78860588   -15.77576544    85.35286006\n",
      "   -20.04146247   -74.45503992  -116.45583353    68.5732994\n",
      "    16.77956065 -1004.27843009  2583.14267524]\n",
      "Old weights: [ 1.00042561  1.00006123  0.99950503 -0.31918541  1.00035126  1.0013994\n",
      "  1.00125479  0.34469727  0.33611731  1.02021383  0.94793477]\n",
      "New weights: [ 1.00042772  1.00006571  0.9995066  -0.31919395  1.00035326  1.00140685\n",
      "  1.00126643  0.34469042  0.33611563  1.02031426  0.94767645]\n",
      "Loss: 694.9311905464406\n",
      "\n",
      "Gradient: [  -21.10876159   -44.76299555   -15.76650495    85.30538443\n",
      "   -20.02999169   -74.41333016  -116.39054693    68.53516247\n",
      "    16.77022196 -1003.71987171  2581.7059967 ]\n",
      "Old weights: [ 1.00042772  1.00006571  0.9995066  -0.31919395  1.00035326  1.00140685\n",
      "  1.00126643  0.34469042  0.33611563  1.02031426  0.94767645]\n",
      "New weights: [ 1.00042983  1.00007018  0.99950818 -0.31920248  1.00035527  1.00141429\n",
      "  1.00127807  0.34468356  0.33611396  1.02041463  0.94741828]\n",
      "Loss: 694.1606950833293\n",
      "\n",
      "Gradient: [  -21.09702141   -44.73739947   -15.7572496     85.25793534\n",
      "   -20.0185273    -74.3716436   -116.32529665    68.49704655\n",
      "    16.76088879 -1003.161624    2580.27011722]\n",
      "Old weights: [ 1.00042983  1.00007018  0.99950818 -0.31920248  1.00035527  1.00141429\n",
      "  1.00127807  0.34468356  0.33611396  1.02041463  0.94741828]\n",
      "New weights: [ 1.00043194  1.00007466  0.99950976 -0.31921101  1.00035727  1.00142173\n",
      "  1.0012897   0.34467671  0.33611228  1.02051495  0.94716025]\n",
      "Loss: 693.3910564684734\n",
      "\n",
      "Gradient: [  -21.08528776   -44.71181762   -15.7479994     85.21051275\n",
      "   -20.00706928   -74.32998022  -116.26008266    68.45895164\n",
      "    16.75156112 -1002.60368677  2578.83503637]\n",
      "Old weights: [ 1.00043194  1.00007466  0.99950976 -0.31921101  1.00035727  1.00142173\n",
      "  1.0012897   0.34467671  0.33611228  1.02051495  0.94716025]\n",
      "New weights: [ 1.00043405  1.00007913  0.99951133 -0.31921953  1.00035927  1.00142916\n",
      "  1.00130133  0.34466987  0.3361106   1.02061521  0.94690237]\n",
      "Loss: 692.6222737489874\n",
      "\n",
      "Gradient: [  -21.07356063   -44.68625      -15.73875434    85.16311666\n",
      "   -19.99561764   -74.28834002  -116.19490494    68.42087774\n",
      "    16.74223892 -1002.04605987  2577.4007537 ]\n",
      "Old weights: [ 1.00043405  1.00007913  0.99951133 -0.31921953  1.00035927  1.00142916\n",
      "  1.00130133  0.34466987  0.3361106   1.02061521  0.94690237]\n",
      "New weights: [ 1.00043616  1.0000836   0.9995129  -0.31922804  1.00036127  1.00143659\n",
      "  1.00131295  0.34466303  0.33610893  1.02071542  0.94664463]\n",
      "Loss: 691.8543459730628\n",
      "\n",
      "Gradient: [  -21.06184003   -44.6606966    -15.72951443    85.11574703\n",
      "   -19.98417236   -74.24672298  -116.12976347    68.38282485\n",
      "    16.73292218 -1001.48874311  2575.96726877]\n",
      "Old weights: [ 1.00043616  1.0000836   0.9995129  -0.31922804  1.00036127  1.00143659\n",
      "  1.00131295  0.34466303  0.33610893  1.02071542  0.94664463]\n",
      "New weights: [ 1.00043827  1.00008806  0.99951448 -0.31923656  1.00036327  1.00144401\n",
      "  1.00132456  0.34465619  0.33610726  1.02081556  0.94638703]\n",
      "Loss: 691.0872721899282\n",
      "\n",
      "Gradient: [  -21.05012594   -44.63515741   -15.72027965    85.06840384\n",
      "   -19.97273345   -74.20512908  -116.06465823    68.34479297\n",
      "    16.72361088 -1000.93173633  2574.53458113]\n",
      "Old weights: [ 1.00043827  1.00008806  0.99951448 -0.31923656  1.00036327  1.00144401\n",
      "  1.00132456  0.34465619  0.33610726  1.02081556  0.94638703]\n",
      "New weights: [ 1.00044037  1.00009252  0.99951605 -0.31924506  1.00036526  1.00145143\n",
      "  1.00133617  0.34464935  0.33610558  1.02091566  0.94612958]\n",
      "Loss: 690.3210514498886\n",
      "\n",
      "Gradient: [  -21.03841837   -44.60963243   -15.71105001    85.02108709\n",
      "   -19.9613009    -74.16355832  -115.99958921    68.30678209\n",
      "    16.714305   -1000.37503935  2573.10269033]\n",
      "Old weights: [ 1.00044037  1.00009252  0.99951605 -0.31924506  1.00036526  1.00145143\n",
      "  1.00133617  0.34464935  0.33610558  1.02091566  0.94612958]\n",
      "New weights: [ 1.00044248  1.00009699  0.99951762 -0.31925356  1.00036726  1.00145885\n",
      "  1.00134777  0.34464252  0.33610391  1.02101569  0.94587227]\n",
      "Loss: 689.5556828042849\n",
      "\n",
      "Gradient: [ -21.02671732  -44.58412164  -15.70182551   84.97379674  -19.94987472\n",
      "  -74.12201068 -115.93455637   68.26879221   16.70500452 -999.818652\n",
      " 2571.67159595]\n",
      "Old weights: [ 1.00044248  1.00009699  0.99951762 -0.31925356  1.00036726  1.00145885\n",
      "  1.00134777  0.34464252  0.33610391  1.02101569  0.94587227]\n",
      "New weights: [ 1.00044458  1.00010144  0.99951919 -0.31926206  1.00036926  1.00146626\n",
      "  1.00135936  0.3446357   0.33610224  1.02111568  0.9456151 ]\n",
      "Loss: 688.7911653055318\n",
      "\n",
      "Gradient: [ -21.01502277  -44.55862505  -15.69260613   84.92653278  -19.93845488\n",
      "  -74.08048614 -115.86955971   68.23082334   16.69570944 -999.26257411\n",
      " 2570.24129752]\n",
      "Old weights: [ 1.00044458  1.00010144  0.99951919 -0.31926206  1.00036926  1.00146626\n",
      "  1.00135936  0.3446357   0.33610224  1.02111568  0.9456151 ]\n",
      "New weights: [ 1.00044668  1.0001059   0.99952076 -0.31927055  1.00037125  1.00147367\n",
      "  1.00137095  0.34462887  0.33610057  1.0212156   0.94535808]\n",
      "Loss: 688.0274980070864\n",
      "\n",
      "Gradient: [ -21.00333472  -44.53314263  -15.68339189   84.87929518  -19.9270414\n",
      "  -74.03898471 -115.8045992    68.19287545   16.68641973 -998.70680551\n",
      " 2568.81179462]\n",
      "Old weights: [ 1.00044668  1.0001059   0.99952076 -0.31927055  1.00037125  1.00147367\n",
      "  1.00137095  0.34462887  0.33610057  1.0212156   0.94535808]\n",
      "New weights: [ 1.00044878  1.00011035  0.99952233 -0.31927904  1.00037324  1.00148107\n",
      "  1.00138253  0.34462205  0.3360989   1.02131547  0.9451012 ]\n",
      "Loss: 687.2646799634676\n",
      "\n",
      "Gradient: [ -20.99165318  -44.50767439  -15.67418276   84.83208393  -19.91563427\n",
      "  -73.99750635 -115.73967481   68.15494855   16.67713538 -998.15134602\n",
      " 2567.38308679]\n",
      "Old weights: [ 1.00044878  1.00011035  0.99952233 -0.31927904  1.00037324  1.00148107\n",
      "  1.00138253  0.34462205  0.3360989   1.02131547  0.9451012 ]\n",
      "New weights: [ 1.00045088  1.0001148   0.9995239  -0.31928753  1.00037523  1.00148847\n",
      "  1.0013941   0.34461524  0.33609724  1.02141529  0.94484446]\n",
      "Loss: 686.5027102302342\n",
      "\n",
      "Gradient: [ -20.97997813  -44.48222031  -15.66497876   84.78489901  -19.90423348\n",
      "  -73.95605107 -115.67478654   68.11704264   16.66785637 -997.59619547\n",
      " 2565.9551736 ]\n",
      "Old weights: [ 1.00045088  1.0001148   0.9995239  -0.31928753  1.00037523  1.00148847\n",
      "  1.0013941   0.34461524  0.33609724  1.02141529  0.94484446]\n",
      "New weights: [ 1.00045298  1.00011925  0.99952546 -0.319296    1.00037722  1.00149587\n",
      "  1.00140567  0.34460843  0.33609557  1.02151505  0.94458786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 685.7415878640008\n",
      "\n",
      "Gradient: [ -20.96830958  -44.45678039  -15.65577988   84.73774041  -19.89283903\n",
      "  -73.91461884 -115.60993436   68.0791577    16.6585827  -997.04135369\n",
      " 2564.5280546 ]\n",
      "Old weights: [ 1.00045298  1.00011925  0.99952546 -0.319296    1.00037722  1.00149587\n",
      "  1.00140567  0.34460843  0.33609557  1.02151505  0.94458786]\n",
      "New weights: [ 1.00045507  1.0001237   0.99952703 -0.31930448  1.00037921  1.00150326\n",
      "  1.00141723  0.34460162  0.3360939   1.02161475  0.94433141]\n",
      "Loss: 684.9813119224458\n",
      "\n",
      "Gradient: [ -20.95664751  -44.43135463  -15.64658612   84.69060809  -19.88145092\n",
      "  -73.87320966 -115.54511825   68.04129374   16.64931435 -996.48682051\n",
      " 2563.10172936]\n",
      "Old weights: [ 1.00045507  1.0001237   0.99952703 -0.31930448  1.00037921  1.00150326\n",
      "  1.00141723  0.34460162  0.3360939   1.02161475  0.94433141]\n",
      "New weights: [ 1.00045717  1.00012814  0.99952859 -0.31931295  1.0003812   1.00151065\n",
      "  1.00142879  0.34459481  0.33609224  1.0217144   0.9440751 ]\n",
      "Loss: 684.2218814642654\n",
      "\n",
      "Gradient: [ -20.94499194  -44.405943    -15.63739747   84.64350205  -19.87006915\n",
      "  -73.83182351 -115.48033818   68.00345074   16.64005131 -995.93259576\n",
      " 2561.67619742]\n",
      "Old weights: [ 1.00045717  1.00012814  0.99952859 -0.31931295  1.0003812   1.00151065\n",
      "  1.00142879  0.34459481  0.33609224  1.0217144   0.9440751 ]\n",
      "New weights: [ 1.00045926  1.00013258  0.99953016 -0.31932141  1.00038319  1.00151803\n",
      "  1.00144033  0.34458801  0.33609058  1.02181399  0.94381893]\n",
      "Loss: 683.4632955492239\n",
      "\n",
      "Gradient: [ -20.93334284  -44.3805455   -15.62821393   84.59642226  -19.8586937\n",
      "  -73.79046037 -115.41559415   67.9656287    16.63079356 -995.37867926\n",
      " 2560.25145836]\n",
      "Old weights: [ 1.00045926  1.00013258  0.99953016 -0.31932141  1.00038319  1.00151803\n",
      "  1.00144033  0.34458801  0.33609058  1.02181399  0.94381893]\n",
      "New weights: [ 1.00046136  1.00013702  0.99953172 -0.31932987  1.00038517  1.00152541\n",
      "  1.00145188  0.34458122  0.33608891  1.02191353  0.94356291]\n",
      "Loss: 682.7055532381265\n",
      "\n",
      "Gradient: [ -20.92170023  -44.35516213  -15.6190355    84.54936872  -19.84732459\n",
      "  -73.74912025 -115.35088613   67.92782762   16.6215411  -994.82507084\n",
      " 2558.82751172]\n",
      "Old weights: [ 1.00046136  1.00013702  0.99953172 -0.31932987  1.00038517  1.00152541\n",
      "  1.00145188  0.34458122  0.33608891  1.02191353  0.94356291]\n",
      "New weights: [ 1.00046345  1.00014146  0.99953328 -0.31933833  1.00038716  1.00153278\n",
      "  1.00146341  0.34457442  0.33608725  1.02201301  0.94330702]\n",
      "Loss: 681.9486535928254\n",
      "\n",
      "Gradient: [ -20.91006409  -44.32979288  -15.60986217   84.50234139  -19.83596179\n",
      "  -73.70780311 -115.2862141    67.89004748   16.61229391 -994.27177034\n",
      " 2557.40435707]\n",
      "Old weights: [ 1.00046345  1.00014146  0.99953328 -0.31933833  1.00038716  1.00153278\n",
      "  1.00146341  0.34457442  0.33608725  1.02201301  0.94330702]\n",
      "New weights: [ 1.00046554  1.00014589  0.99953484 -0.31934678  1.00038914  1.00154016\n",
      "  1.00147494  0.34456764  0.33608559  1.02211244  0.94305128]\n",
      "Loss: 681.1925956762096\n",
      "\n",
      "Gradient: [ -20.89843442  -44.30443774  -15.60069394   84.45534027  -19.82460532\n",
      "  -73.66650896 -115.22157804   67.85228828   16.60305199 -993.71877758\n",
      " 2555.98199397]\n",
      "Old weights: [ 1.00046554  1.00014589  0.99953484 -0.31934678  1.00038914  1.00154016\n",
      "  1.00147494  0.34456764  0.33608559  1.02211244  0.94305128]\n",
      "New weights: [ 1.00046763  1.00015032  0.9995364  -0.31935522  1.00039113  1.00154752\n",
      "  1.00148646  0.34456085  0.33608393  1.02221181  0.94279569]\n",
      "Loss: 680.4373785522147\n",
      "\n",
      "Gradient: [ -20.88681122  -44.2790967   -15.59153082   84.40836533  -19.81325516\n",
      "  -73.62523777 -115.15697793   67.81455001   16.59381533 -993.16609239\n",
      " 2554.56042197]\n",
      "Old weights: [ 1.00046763  1.00015032  0.9995364  -0.31935522  1.00039113  1.00154752\n",
      "  1.00148646  0.34456085  0.33608393  1.02221181  0.94279569]\n",
      "New weights: [ 1.00046972  1.00015475  0.99953796 -0.31936366  1.00039311  1.00155488\n",
      "  1.00149798  0.34455407  0.33608227  1.02231113  0.94254023]\n",
      "Loss: 679.68300128581\n",
      "\n",
      "Gradient: [ -20.87519448  -44.25376976  -15.58237279   84.36141657  -19.80191131\n",
      "  -73.58398954 -115.09241375   67.77683266   16.58458391 -992.6137146\n",
      " 2553.13964064]\n",
      "Old weights: [ 1.00046972  1.00015475  0.99953796 -0.31936366  1.00039311  1.00155488\n",
      "  1.00149798  0.34455407  0.33608227  1.02231113  0.94254023]\n",
      "New weights: [ 1.00047181  1.00015917  0.99953952 -0.3193721   1.00039509  1.00156224\n",
      "  1.00150949  0.34454729  0.33608061  1.02241039  0.94228492]\n",
      "Loss: 678.9294629430244\n",
      "\n",
      "Gradient: [ -20.86358421  -44.2284569   -15.57321985   84.31449395  -19.79057378\n",
      "  -73.54276425 -115.02788548   67.73913623   16.57535772 -992.06164404\n",
      " 2551.71964954]\n",
      "Old weights: [ 1.00047181  1.00015917  0.99953952 -0.3193721   1.00039509  1.00156224\n",
      "  1.00150949  0.34454729  0.33608061  1.02241039  0.94228492]\n",
      "New weights: [ 1.00047389  1.00016359  0.99954108 -0.31938053  1.00039707  1.0015696\n",
      "  1.00152099  0.34454052  0.33607895  1.0225096   0.94202974]\n",
      "Loss: 678.1767625908994\n",
      "\n",
      "Gradient: [ -20.85198039  -44.20315813  -15.56407201   84.26759747  -19.77924255\n",
      "  -73.50156189 -114.9633931    67.70146071   16.56613677 -991.50988053\n",
      " 2550.30044823]\n",
      "Old weights: [ 1.00047389  1.00016359  0.99954108 -0.31938053  1.00039707  1.0015696\n",
      "  1.00152099  0.34454052  0.33607895  1.0225096   0.94202974]\n",
      "New weights: [ 1.00047598  1.00016802  0.99954263 -0.31938896  1.00039904  1.00157695\n",
      "  1.00153248  0.34453375  0.3360773   1.02260875  0.94177471]\n",
      "Loss: 677.4248992975411\n",
      "\n",
      "Gradient: [ -20.84038303  -44.17787342  -15.55492925   84.22072711  -19.76791762\n",
      "  -73.46038244 -114.89893659   67.66380608   16.55692103 -990.95842391\n",
      " 2548.88203626]\n",
      "Old weights: [ 1.00047598  1.00016802  0.99954263 -0.31938896  1.00039904  1.00157695\n",
      "  1.00153248  0.34453375  0.3360773   1.02260875  0.94177471]\n",
      "New weights: [ 1.00047806  1.00017243  0.99954419 -0.31939738  1.00040102  1.00158429\n",
      "  1.00154397  0.34452698  0.33607564  1.02270784  0.94151983]\n",
      "Loss: 676.6738721320602\n",
      "\n",
      "Gradient: [ -20.82879212  -44.15260278  -15.54579158   84.17388286  -19.75659899\n",
      "  -73.4192259  -114.83451593   67.62617235   16.54771051 -990.40727401\n",
      " 2547.4644132 ]\n",
      "Old weights: [ 1.00047806  1.00017243  0.99954419 -0.31939738  1.00040102  1.00158429\n",
      "  1.00154397  0.34452698  0.33607564  1.02270784  0.94151983]\n",
      "New weights: [ 1.00048015  1.00017685  0.99954574 -0.3194058   1.000403    1.00159164\n",
      "  1.00155546  0.34452022  0.33607399  1.02280689  0.94126508]\n",
      "Loss: 675.9236801646279\n",
      "\n",
      "Gradient: [ -20.81720765  -44.12734619  -15.53665899   84.12706468  -19.74528666\n",
      "  -73.37809225 -114.7701311    67.5885595    16.53850519 -989.85643065\n",
      " 2546.04757862]\n",
      "Old weights: [ 1.00048015  1.00017685  0.99954574 -0.3194058   1.000403    1.00159164\n",
      "  1.00155546  0.34452022  0.33607399  1.02280689  0.94126508]\n",
      "New weights: [ 1.00048223  1.00018126  0.9995473  -0.31941421  1.00040497  1.00159897\n",
      "  1.00156694  0.34451346  0.33607233  1.02290587  0.94101047]\n",
      "Loss: 675.1743224664472\n",
      "\n",
      "Gradient: [ -20.80562963  -44.10210365  -15.52753148   84.08027258  -19.73398062\n",
      "  -73.33698148 -114.70578208   67.55096752   16.52930507 -989.30589367\n",
      " 2544.63153206]\n",
      "Old weights: [ 1.00048223  1.00018126  0.9995473  -0.31941421  1.00040497  1.00159897\n",
      "  1.00156694  0.34451346  0.33607233  1.02290587  0.94101047]\n",
      "New weights: [ 1.00048431  1.00018567  0.99954885 -0.31942262  1.00040694  1.00160631\n",
      "  1.00157841  0.3445067   0.33607068  1.0230048   0.94075601]\n",
      "Loss: 674.4257981097373\n",
      "\n",
      "Gradient: [ -20.79405805  -44.07687515  -15.51840905   84.03350653  -19.72268087\n",
      "  -73.29589358 -114.64146885   67.5133964    16.52011013 -988.75566289\n",
      " 2543.2162731 ]\n",
      "Old weights: [ 1.00048431  1.00018567  0.99954885 -0.31942262  1.00040694  1.00160631\n",
      "  1.00157841  0.3445067   0.33607068  1.0230048   0.94075601]\n",
      "New weights: [ 1.00048639  1.00019008  0.9995504  -0.31943102  1.00040892  1.00161364\n",
      "  1.00158987  0.34449995  0.33606903  1.02310368  0.94050169]\n",
      "Loss: 673.6781061677565\n",
      "\n",
      "Gradient: [ -20.7824929   -44.05166068  -15.50929169   83.98676652  -19.7113874\n",
      "  -73.25482852 -114.57719139   67.47584614   16.51092038 -988.20573814\n",
      " 2541.8018013 ]\n",
      "Old weights: [ 1.00048639  1.00019008  0.9995504  -0.31943102  1.00040892  1.00161364\n",
      "  1.00158987  0.34449995  0.33606903  1.02310368  0.94050169]\n",
      "New weights: [ 1.00048847  1.00019448  0.99955195 -0.31943942  1.00041089  1.00162096\n",
      "  1.00160133  0.3444932   0.33606738  1.0232025   0.94024751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 672.9312457148203\n",
      "\n",
      "Gradient: [ -20.77093419  -44.02646023  -15.5001794    83.94005253  -19.70010021\n",
      "  -73.21378631 -114.51294969   67.43831673   16.50173581 -987.65611926\n",
      " 2540.38811621]\n",
      "Old weights: [ 1.00048847  1.00019448  0.99955195 -0.31943942  1.00041089  1.00162096\n",
      "  1.00160133  0.3444932   0.33606738  1.0232025   0.94024751]\n",
      "New weights: [ 1.00049054  1.00019889  0.9995535  -0.31944781  1.00041286  1.00162828\n",
      "  1.00161278  0.34448646  0.33606573  1.02330126  0.93999347]\n",
      "Loss: 672.1852158262313\n",
      "\n",
      "Gradient: [ -20.7593819   -44.0012738   -15.49107218   83.89336455  -19.6888193\n",
      "  -73.17276692 -114.44874371   67.40080815   16.4925564  -987.10680607\n",
      " 2538.97521741]\n",
      "Old weights: [ 1.00049054  1.00019889  0.9995535  -0.31944781  1.00041286  1.00162828\n",
      "  1.00161278  0.34448646  0.33606573  1.02330126  0.93999347]\n",
      "New weights: [ 1.00049262  1.00020329  0.99955505 -0.3194562   1.00041483  1.0016356\n",
      "  1.00162422  0.34447972  0.33606408  1.02339997  0.93973957]\n",
      "Loss: 671.4400155783402\n",
      "\n",
      "Gradient: [ -20.74783604  -43.97610138  -15.48197003   83.84670256  -19.67754467\n",
      "  -73.13177035 -114.38457344   67.3633204    16.48338216 -986.55779841\n",
      " 2537.56310444]\n",
      "Old weights: [ 1.00049262  1.00020329  0.99955505 -0.3194562   1.00041483  1.0016356\n",
      "  1.00162422  0.34447972  0.33606408  1.02339997  0.93973957]\n",
      "New weights: [ 1.00049469  1.00020768  0.9995566  -0.31946459  1.00041679  1.00164291\n",
      "  1.00163566  0.34447298  0.33606243  1.02349863  0.93948582]\n",
      "Loss: 670.695644048534\n",
      "\n",
      "Gradient: [ -20.73629661  -43.95094296  -15.47287293   83.80006653  -19.66627631\n",
      "  -73.09079658 -114.32043887   67.32585346   16.47421307 -986.0090961\n",
      " 2536.15177688]\n",
      "Old weights: [ 1.00049469  1.00020768  0.9995566  -0.31946459  1.00041679  1.00164291\n",
      "  1.00163566  0.34447298  0.33606243  1.02349863  0.93948582]\n",
      "New weights: [ 1.00049677  1.00021208  0.99955815 -0.31947297  1.00041876  1.00165022\n",
      "  1.00164709  0.34446625  0.33606078  1.02359723  0.9392322 ]\n",
      "Loss: 669.9521003152075\n",
      "\n",
      "Gradient: [ -20.72476359  -43.92579854  -15.4637809    83.75345648  -19.65501421\n",
      "  -73.0498456  -114.25633997   67.28840734   16.46504914 -985.46069897\n",
      " 2534.7412343 ]\n",
      "Old weights: [ 1.00049677  1.00021208  0.99955815 -0.31947297  1.00041876  1.00165022\n",
      "  1.00164709  0.34446625  0.33606078  1.02359723  0.9392322 ]\n",
      "New weights: [ 1.00049884  1.00021647  0.99955969 -0.31948134  1.00042073  1.00165753\n",
      "  1.00165852  0.34445952  0.33605913  1.02369578  0.93897873]\n",
      "Loss: 669.2093834577963\n",
      "\n",
      "Gradient: [ -20.71323698  -43.9006681   -15.45469393   83.70687235  -19.64375838\n",
      "  -73.0089174  -114.19227671   67.25098201   16.45589035 -984.91260686\n",
      " 2533.33147624]\n",
      "Old weights: [ 1.00049884  1.00021647  0.99955969 -0.31948134  1.00042073  1.00165753\n",
      "  1.00165852  0.34445952  0.33605913  1.02369578  0.93897873]\n",
      "New weights: [ 1.00050091  1.00022086  0.99956124 -0.31948971  1.00042269  1.00166483\n",
      "  1.00166994  0.3444528   0.33605749  1.02379427  0.93872539]\n",
      "Loss: 668.4674925567438\n",
      "\n",
      "Gradient: [ -20.70171679  -43.87555163  -15.44561201   83.66031417  -19.63250881\n",
      "  -72.96801196 -114.12824909   67.21357747   16.4467367  -984.36481959\n",
      " 2531.92250228]\n",
      "Old weights: [ 1.00050091  1.00022086  0.99956124 -0.31948971  1.00042269  1.00166483\n",
      "  1.00166994  0.3444528   0.33605749  1.02379427  0.93872539]\n",
      "New weights: [ 1.00050298  1.00022525  0.99956278 -0.31949808  1.00042465  1.00167213\n",
      "  1.00168135  0.34444608  0.33605584  1.0238927   0.9384722 ]\n",
      "Loss: 667.7264266935317\n",
      "\n",
      "Gradient: [ -20.690203    -43.85044914  -15.43653514   83.61378189  -19.62126549\n",
      "  -72.92712927 -114.06425709   67.1761937    16.43758818 -983.81733699\n",
      " 2530.51431198]\n",
      "Old weights: [ 1.00050298  1.00022525  0.99956278 -0.31949808  1.00042465  1.00167213\n",
      "  1.00168135  0.34444608  0.33605584  1.0238927   0.9384722 ]\n",
      "New weights: [ 1.00050505  1.00022963  0.99956433 -0.31950644  1.00042662  1.00167942\n",
      "  1.00169276  0.34443936  0.3360542   1.02399109  0.93821915]\n",
      "Loss: 666.9861849506532\n",
      "\n",
      "Gradient: [ -20.67869562  -43.82536061  -15.42746332   83.5672755   -19.61002843\n",
      "  -72.88626932 -114.00030067   67.13883071   16.4284448  -983.2701589\n",
      " 2529.10690491]\n",
      "Old weights: [ 1.00050505  1.00022963  0.99956433 -0.31950644  1.00042662  1.00167942\n",
      "  1.00169276  0.34443936  0.3360542   1.02399109  0.93821915]\n",
      "New weights: [ 1.00050712  1.00023402  0.99956587 -0.3195148   1.00042858  1.00168671\n",
      "  1.00170416  0.34443265  0.33605256  1.02408941  0.93796624]\n",
      "Loss: 666.2467664116264\n",
      "\n",
      "Gradient: [ -20.66719463  -43.80028603  -15.41839654   83.520795    -19.59879762\n",
      "  -72.8454321  -113.93637982   67.10148847   16.41930653 -982.72328515\n",
      " 2527.70028062]\n",
      "Old weights: [ 1.00050712  1.00023402  0.99956587 -0.3195148   1.00042858  1.00168671\n",
      "  1.00170416  0.34443265  0.33605256  1.02408941  0.93796624]\n",
      "New weights: [ 1.00050918  1.0002384   0.99956741 -0.31952315  1.00043054  1.00169399\n",
      "  1.00171555  0.34442594  0.33605092  1.02418769  0.93771347]\n",
      "Loss: 665.5081701609787\n",
      "\n",
      "Gradient: [ -20.65570005  -43.7752254   -15.40933481   83.47434037  -19.58757306\n",
      "  -72.80461759 -113.87249453   67.06416698   16.41017339 -982.17671557\n",
      " 2526.29443869]\n",
      "Old weights: [ 1.00050918  1.0002384   0.99956741 -0.31952315  1.00043054  1.00169399\n",
      "  1.00171555  0.34442594  0.33605092  1.02418769  0.93771347]\n",
      "New weights: [ 1.00051125  1.00024277  0.99956895 -0.3195315   1.0004325   1.00170127\n",
      "  1.00172694  0.34441923  0.33604927  1.0242859   0.93746084]\n",
      "Loss: 664.7703952842731\n",
      "\n",
      "Gradient: [ -20.64421185  -43.7501787   -15.40027811   83.42791158  -19.57635474\n",
      "  -72.76382578 -113.80864477   67.02686622   16.40104536 -981.63044998\n",
      " 2524.88937867]\n",
      "Old weights: [ 1.00051125  1.00024277  0.99956895 -0.3195315   1.0004325   1.00170127\n",
      "  1.00172694  0.34441923  0.33604927  1.0242859   0.93746084]\n",
      "New weights: [ 1.00051331  1.00024715  0.99957049 -0.31953984  1.00043445  1.00170855\n",
      "  1.00173832  0.34441253  0.33604763  1.02438407  0.93720835]\n",
      "Loss: 664.0334408680875\n",
      "\n",
      "Gradient: [ -20.63273005  -43.72514594  -15.39122646   83.38150864  -19.56514265\n",
      "  -72.72305665 -113.74483052   66.9895862    16.39192244 -981.08448822\n",
      " 2523.48510014]\n",
      "Old weights: [ 1.00051331  1.00024715  0.99957049 -0.31953984  1.00043445  1.00170855\n",
      "  1.00173832  0.34441253  0.33604763  1.02438407  0.93720835]\n",
      "New weights: [ 1.00051538  1.00025152  0.99957203 -0.31954818  1.00043641  1.00171582\n",
      "  1.00174969  0.34440583  0.336046    1.02448217  0.936956  ]\n",
      "Loss: 663.2973059999981\n",
      "\n",
      "Gradient: [ -20.62125463  -43.7001271   -15.38217984   83.33513151  -19.55393681\n",
      "  -72.68231021 -113.68105177   66.95232689   16.38280462 -980.53883012\n",
      " 2522.08160266]\n",
      "Old weights: [ 1.00051538  1.00025152  0.99957203 -0.31954818  1.00043641  1.00171582\n",
      "  1.00174969  0.34440583  0.336046    1.02448217  0.936956  ]\n",
      "New weights: [ 1.00051744  1.00025589  0.99957357 -0.31955651  1.00043837  1.00172309\n",
      "  1.00176106  0.34439913  0.33604436  1.02458023  0.93670379]\n",
      "Loss: 662.561989768594\n",
      "\n",
      "Gradient: [ -20.6097856   -43.67512218  -15.37313825   83.28878019  -19.5427372\n",
      "  -72.64158643 -113.61730849   66.91508828   16.37369191 -979.99347551\n",
      " 2520.67888579]\n",
      "Old weights: [ 1.00051744  1.00025589  0.99957357 -0.31955651  1.00043837  1.00172309\n",
      "  1.00176106  0.34439913  0.33604436  1.02458023  0.93670379]\n",
      "New weights: [ 1.0005195   1.00026026  0.99957511 -0.31956484  1.00044032  1.00173035\n",
      "  1.00177242  0.34439244  0.33604272  1.02467823  0.93645173]\n",
      "Loss: 661.8274912635177\n",
      "\n",
      "Gradient: [ -20.59832294  -43.65013116  -15.36410169   83.24245466  -19.53154381\n",
      "  -72.60088529 -113.55360066   66.87787037   16.36458429 -979.44842423\n",
      " 2519.2769491 ]\n",
      "Old weights: [ 1.0005195   1.00026026  0.99957511 -0.31956484  1.00044032  1.00173035\n",
      "  1.00177242  0.34439244  0.33604272  1.02467823  0.93645173]\n",
      "New weights: [ 1.00052156  1.00026462  0.99957664 -0.31957316  1.00044227  1.00173761\n",
      "  1.00178378  0.34438575  0.33604108  1.02477617  0.9361998 ]\n",
      "Loss: 661.0938095753703\n",
      "\n",
      "Gradient: [ -20.58686666  -43.62515405  -15.35507015   83.1961549   -19.52035665\n",
      "  -72.5602068  -113.48992827   66.84067315   16.35548176 -978.90367609\n",
      " 2517.87579216]\n",
      "Old weights: [ 1.00052156  1.00026462  0.99957664 -0.31957316  1.00044227  1.00173761\n",
      "  1.00178378  0.34438575  0.33604108  1.02477617  0.9361998 ]\n",
      "New weights: [ 1.00052362  1.00026899  0.99957818 -0.31958148  1.00044422  1.00174487\n",
      "  1.00179513  0.34437907  0.33603945  1.02487406  0.93594801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 660.3609437958058\n",
      "\n",
      "Gradient: [ -20.57541676  -43.60019082  -15.34604364   83.14988091  -19.50917572\n",
      "  -72.51955093 -113.42629129   66.80349659   16.34638432 -978.35923094\n",
      " 2516.47541453]\n",
      "Old weights: [ 1.00052362  1.00026899  0.99957818 -0.31958148  1.00044422  1.00174487\n",
      "  1.00179513  0.34437907  0.33603945  1.02487406  0.93594801]\n",
      "New weights: [ 1.00052568  1.00027335  0.99957971 -0.3195898   1.00044618  1.00175212\n",
      "  1.00180647  0.34437239  0.33603781  1.0249719   0.93569636]\n",
      "Loss: 659.628893017464\n",
      "\n",
      "Gradient: [ -20.56397322  -43.57524148  -15.33702215   83.10363266  -19.498001\n",
      "  -72.47891767 -113.3626897    66.76634071   16.33729196 -977.81508861\n",
      " 2515.07581578]\n",
      "Old weights: [ 1.00052568  1.00027335  0.99957971 -0.3195898   1.00044618  1.00175212\n",
      "  1.00180647  0.34437239  0.33603781  1.0249719   0.93569636]\n",
      "New weights: [ 1.00052773  1.0002777   0.99958125 -0.31959811  1.00044813  1.00175937\n",
      "  1.00181781  0.34436571  0.33603618  1.02506968  0.93544486]\n",
      "Loss: 658.8976563340146\n",
      "\n",
      "Gradient: [ -20.55253604  -43.55030602  -15.32800568   83.05741015  -19.4868325\n",
      "  -72.43830701 -113.29912349   66.72920547   16.32820468 -977.27124893\n",
      " 2513.67699547]\n",
      "Old weights: [ 1.00052773  1.0002777   0.99958125 -0.31959811  1.00044813  1.00175937\n",
      "  1.00181781  0.34436571  0.33603618  1.02506968  0.93544486]\n",
      "New weights: [ 1.00052979  1.00028206  0.99958278 -0.31960641  1.00045007  1.00176661\n",
      "  1.00182914  0.34435904  0.33603455  1.02516741  0.93519349]\n",
      "Loss: 658.1672328401021\n",
      "\n",
      "Gradient: [ -20.54110523  -43.52538443  -15.31899422   83.01121335  -19.47567021\n",
      "  -72.39771894 -113.23559264   66.69209088   16.31912247 -976.72771172\n",
      " 2512.27895318]\n",
      "Old weights: [ 1.00052979  1.00028206  0.99958278 -0.31960641  1.00045007  1.00176661\n",
      "  1.00182914  0.34435904  0.33603455  1.02516741  0.93519349]\n",
      "New weights: [ 1.00053184  1.00028641  0.99958431 -0.31961472  1.00045202  1.00177385\n",
      "  1.00184046  0.34435237  0.33603291  1.02526508  0.93494226]\n",
      "Loss: 657.4376216314197\n",
      "\n",
      "Gradient: [ -20.52968077  -43.50047669  -15.30998778   82.96504225  -19.46451413\n",
      "  -72.35715345 -113.17209712   66.65499691   16.31004534 -976.18447683\n",
      " 2510.88168846]\n",
      "Old weights: [ 1.00053184  1.00028641  0.99958431 -0.31961472  1.00045202  1.00177385\n",
      "  1.00184046  0.34435237  0.33603291  1.02526508  0.93494226]\n",
      "New weights: [ 1.0005339   1.00029076  0.99958584 -0.31962301  1.00045397  1.00178109\n",
      "  1.00185178  0.3443457   0.33603128  1.0253627   0.93469117]\n",
      "Loss: 656.7088218046385\n",
      "\n",
      "Gradient: [ -20.51826267  -43.47558281  -15.30098634   82.91889684  -19.45336425\n",
      "  -72.31661051 -113.10863692   66.61792357   16.30097327 -975.64154407\n",
      " 2509.4852009 ]\n",
      "Old weights: [ 1.0005339   1.00029076  0.99958584 -0.31962301  1.00045397  1.00178109\n",
      "  1.00185178  0.3443457   0.33603128  1.0253627   0.93469117]\n",
      "New weights: [ 1.00053595  1.00029511  0.99958737 -0.3196313   1.00045591  1.00178832\n",
      "  1.00186309  0.34433904  0.33602965  1.02546026  0.93444022]\n",
      "Loss: 655.9808324574383\n",
      "\n",
      "Gradient: [ -20.50685092  -43.45070278  -15.29198991   82.8727771   -19.44222058\n",
      "  -72.27609013 -113.04521201   66.58087084   16.29190626 -975.0989133\n",
      " 2508.08949005]\n",
      "Old weights: [ 1.00053595  1.00029511  0.99958737 -0.3196313   1.00045591  1.00178832\n",
      "  1.00186309  0.34433904  0.33602965  1.02546026  0.93444022]\n",
      "New weights: [ 1.000538    1.00029946  0.9995889  -0.31963959  1.00045786  1.00179555\n",
      "  1.00187439  0.34433238  0.33602802  1.02555777  0.93418942]\n",
      "Loss: 655.2536526885084\n",
      "\n",
      "Gradient: [ -20.49544551  -43.42583658  -15.28299849   82.82668302  -19.4310831\n",
      "  -72.23559228 -112.98182238   66.5438387    16.28284432 -974.55658433\n",
      " 2506.69455548]\n",
      "Old weights: [ 1.000538    1.00029946  0.9995889  -0.31963959  1.00045786  1.00179555\n",
      "  1.00187439  0.34433238  0.33602802  1.02555777  0.93418942]\n",
      "New weights: [ 1.00054005  1.0003038   0.99959043 -0.31964787  1.0004598   1.00180277\n",
      "  1.00188569  0.34432573  0.3360264   1.02565523  0.93393875]\n",
      "Loss: 654.527281597529\n",
      "\n",
      "Gradient: [ -20.48404645  -43.40098422  -15.27401206   82.78061458  -19.41995182\n",
      "  -72.19511696 -112.918468     66.50682716   16.27378742 -974.014557\n",
      " 2505.30039677]\n",
      "Old weights: [ 1.00054005  1.0003038   0.99959043 -0.31964787  1.0004598   1.00180277\n",
      "  1.00188569  0.34432573  0.3360264   1.02565523  0.93393875]\n",
      "New weights: [ 1.0005421   1.00030814  0.99959196 -0.31965615  1.00046174  1.00180999\n",
      "  1.00189698  0.34431908  0.33602477  1.02575263  0.93368822]\n",
      "Loss: 653.8017182852027\n",
      "\n",
      "Gradient: [ -20.47265373  -43.37614568  -15.26503064   82.73457177  -19.40882673\n",
      "  -72.15466415 -112.85514887   66.46983619   16.26473559 -973.47283114\n",
      " 2503.90701347]\n",
      "Old weights: [ 1.0005421   1.00030814  0.99959196 -0.31965615  1.00046174  1.00180999\n",
      "  1.00189698  0.34431908  0.33602477  1.02575263  0.93368822]\n",
      "New weights: [ 1.00054414  1.00031248  0.99959348 -0.31966443  1.00046368  1.00181721\n",
      "  1.00190827  0.34431243  0.33602314  1.02584998  0.93343783]\n",
      "Loss: 653.0769618532114\n",
      "\n",
      "Gradient: [ -20.46126735  -43.35132095  -15.2560542    82.68855458  -19.39770782\n",
      "  -72.11423384 -112.79186495   66.43286578   16.2556888  -972.93140658\n",
      " 2502.51440517]\n",
      "Old weights: [ 1.00054414  1.00031248  0.99959348 -0.31966443  1.00046368  1.00181721\n",
      "  1.00190827  0.34431243  0.33602314  1.02584998  0.93343783]\n",
      "New weights: [ 1.00054619  1.00031681  0.99959501 -0.31967269  1.00046562  1.00182442\n",
      "  1.00191955  0.34430579  0.33602152  1.02594727  0.93318757]\n",
      "Loss: 652.3530114042421\n",
      "\n",
      "Gradient: [ -20.4498873   -43.32651003  -15.24708277   82.64256298  -19.3865951\n",
      "  -72.07382601 -112.72861622   66.39591593   16.24664705 -972.39028316\n",
      " 2501.12257142]\n",
      "Old weights: [ 1.00054619  1.00031681  0.99959501 -0.31967269  1.00046562  1.00182442\n",
      "  1.00191955  0.34430579  0.33602152  1.02594727  0.93318757]\n",
      "New weights: [ 1.00054823  1.00032114  0.99959654 -0.31968096  1.00046756  1.00183162\n",
      "  1.00193082  0.34429915  0.33601989  1.02604451  0.93293746]\n",
      "Loss: 651.6298660419884\n",
      "\n",
      "Gradient: [ -20.43851358  -43.30171291  -15.23811632   82.59659697  -19.37548857\n",
      "  -72.03344067 -112.66540268   66.35898663   16.23761034 -971.84946071\n",
      " 2499.7315118 ]\n",
      "Old weights: [ 1.00054823  1.00032114  0.99959654 -0.31968096  1.00046756  1.00183162\n",
      "  1.00193082  0.34429915  0.33601989  1.02604451  0.93293746]\n",
      "New weights: [ 1.00055028  1.00032547  0.99959806 -0.31968922  1.0004695   1.00183883\n",
      "  1.00194209  0.34429251  0.33601827  1.02614169  0.93268749]\n",
      "Loss: 650.9075248711254\n",
      "\n",
      "Gradient: [ -20.42714618  -43.27692958  -15.22915486   82.55065653  -19.3643882\n",
      "  -71.99307778 -112.6022243    66.32207785   16.22857868 -971.30893906\n",
      " 2498.34122587]\n",
      "Old weights: [ 1.00055028  1.00032547  0.99959806 -0.31968922  1.0004695   1.00183883\n",
      "  1.00194209  0.34429251  0.33601827  1.02614169  0.93268749]\n",
      "New weights: [ 1.00055232  1.0003298   0.99959958 -0.31969747  1.00047144  1.00184603\n",
      "  1.00195335  0.34428588  0.33601665  1.02623883  0.93243765]\n",
      "Loss: 650.1859869973348\n",
      "\n",
      "Gradient: [ -20.41578511  -43.25216004  -15.22019838   82.50474165  -19.35329402\n",
      "  -71.95273734 -112.53908105   66.2851896    16.21955204 -970.76871804\n",
      " 2496.95171321]\n",
      "Old weights: [ 1.00055232  1.0003298   0.99959958 -0.31969747  1.00047144  1.00184603\n",
      "  1.00195335  0.34428588  0.33601665  1.02623883  0.93243765]\n",
      "New weights: [ 1.00055436  1.00033413  0.9996011  -0.31970572  1.00047337  1.00185322\n",
      "  1.0019646   0.34427925  0.33601502  1.0263359   0.93218796]\n",
      "Loss: 649.4652515272973\n",
      "\n",
      "Gradient: [ -20.40443036  -43.22740427  -15.21124688   82.4588523   -19.342206\n",
      "  -71.91241934 -112.47597293   66.24832186   16.21053044 -970.22879749\n",
      " 2495.56297339]\n",
      "Old weights: [ 1.00055436  1.00033413  0.9996011  -0.31970572  1.00047337  1.00185322\n",
      "  1.0019646   0.34427925  0.33601502  1.0263359   0.93218796]\n",
      "New weights: [ 1.0005564   1.00033845  0.99960263 -0.31971397  1.00047531  1.00186041\n",
      "  1.00197585  0.34427263  0.3360134   1.02643293  0.9319384 ]\n",
      "Loss: 648.7453175686601\n",
      "\n",
      "Gradient: [ -20.39308192  -43.20266228  -15.20230037   82.41298849  -19.33112415\n",
      "  -71.87212377 -112.4128999    66.21147462   16.20151387 -969.68917724\n",
      " 2494.17500597]\n",
      "Old weights: [ 1.0005564   1.00033845  0.99960263 -0.31971397  1.00047531  1.00186041\n",
      "  1.00197585  0.34427263  0.3360134   1.02643293  0.9319384 ]\n",
      "New weights: [ 1.00055844  1.00034277  0.99960415 -0.31972221  1.00047724  1.0018676\n",
      "  1.00198709  0.34426601  0.33601178  1.02652989  0.93168899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 648.0261842301118\n",
      "\n",
      "Gradient: [ -20.38173979  -43.17793404  -15.19335883   82.36715018  -19.32004847\n",
      "  -71.8318506  -112.34986196   66.17464787   16.19250231 -969.14985713\n",
      " 2492.78781053]\n",
      "Old weights: [ 1.00055844  1.00034277  0.99960415 -0.31972221  1.00047724  1.0018676\n",
      "  1.00198709  0.34426601  0.33601178  1.02652989  0.93168899]\n",
      "New weights: [ 1.00056048  1.00034709  0.99960566 -0.31973045  1.00047917  1.00187478\n",
      "  1.00199833  0.34425939  0.33601016  1.02662681  0.93143971]\n",
      "Loss: 647.3078506212788\n",
      "\n",
      "Gradient: [ -20.37040397  -43.15321956  -15.18442226   82.32133738  -19.30897894\n",
      "  -71.79159984 -112.28685908   66.13784159   16.18349578 -968.61083697\n",
      " 2491.40138664]\n",
      "Old weights: [ 1.00056048  1.00034709  0.99960566 -0.31973045  1.00047917  1.00187478\n",
      "  1.00199833  0.34425939  0.33601016  1.02662681  0.93143971]\n",
      "New weights: [ 1.00056252  1.0003514   0.99960718 -0.31973868  1.0004811   1.00188196\n",
      "  1.00200955  0.34425278  0.33600854  1.02672367  0.93119057]\n",
      "Loss: 646.590315852809\n",
      "\n",
      "Gradient: [ -20.35907446  -43.12851882  -15.17549066   82.27555006  -19.29791557\n",
      "  -71.75137146 -112.22389124   66.10105579   16.17449427 -968.07211662\n",
      " 2490.01573387]\n",
      "Old weights: [ 1.00056252  1.0003514   0.99960718 -0.31973868  1.0004811   1.00188196\n",
      "  1.00200955  0.34425278  0.33600854  1.02672367  0.93119057]\n",
      "New weights: [ 1.00056455  1.00035572  0.9996087  -0.31974691  1.00048303  1.00188914\n",
      "  1.00202078  0.34424617  0.33600693  1.02682048  0.93094156]\n",
      "Loss: 645.8735790363352\n",
      "\n",
      "Gradient: [ -20.34775125  -43.10383182  -15.16656403   82.2297882   -19.28685836\n",
      "  -71.71116546 -112.16095842   66.06429043   16.16549777 -967.5336959\n",
      " 2488.63085178]\n",
      "Old weights: [ 1.00056455  1.00035572  0.9996087  -0.31974691  1.00048303  1.00188914\n",
      "  1.00202078  0.34424617  0.33600693  1.02682048  0.93094156]\n",
      "New weights: [ 1.00056659  1.00036003  0.99961022 -0.31975513  1.00048496  1.00189631\n",
      "  1.00203199  0.34423956  0.33600531  1.02691723  0.9306927 ]\n",
      "Loss: 645.1576392844725\n",
      "\n",
      "Gradient: [ -20.33643433  -43.07915856  -15.15764237   82.18405181  -19.27580729\n",
      "  -71.67098182 -112.0980606    66.02754552   16.15650628 -966.99557464\n",
      " 2487.24673995]\n",
      "Old weights: [ 1.00056659  1.00036003  0.99961022 -0.31975513  1.00048496  1.00189631\n",
      "  1.00203199  0.34423956  0.33600531  1.02691723  0.9306927 ]\n",
      "New weights: [ 1.00056862  1.00036433  0.99961173 -0.31976335  1.00048689  1.00190348\n",
      "  1.0020432   0.34423296  0.33600369  1.02701393  0.93044398]\n",
      "Loss: 644.4424957108226\n",
      "\n",
      "Gradient: [ -20.32512371  -43.05449901  -15.14872567   82.13834085  -19.26476237\n",
      "  -71.63082053 -112.03519777   65.99082105   16.1475198  -966.45775269\n",
      " 2485.86339796]\n",
      "Old weights: [ 1.00056862  1.00036433  0.99961173 -0.31976335  1.00048689  1.00190348\n",
      "  1.0020432   0.34423296  0.33600369  1.02701393  0.93044398]\n",
      "New weights: [ 1.00057065  1.00036864  0.99961325 -0.31977156  1.00048881  1.00191064\n",
      "  1.00205441  0.34422636  0.33600208  1.02711058  0.93019539]\n",
      "Loss: 643.7281474299845\n",
      "\n",
      "Gradient: [ -20.31381938  -43.02985318  -15.13981392   82.09265532  -19.2537236\n",
      "  -71.59068158 -111.9723699    65.95411699   16.13853833 -965.92022986\n",
      " 2484.48082537]\n",
      "Old weights: [ 1.00057065  1.00036864  0.99961325 -0.31977156  1.00048881  1.00191064\n",
      "  1.00205441  0.34422636  0.33600208  1.02711058  0.93019539]\n",
      "New weights: [ 1.00057269  1.00037294  0.99961476 -0.31977977  1.00049074  1.0019178\n",
      "  1.0020656   0.34421976  0.33600047  1.02720717  0.92994694]\n",
      "Loss: 643.0145935575241\n",
      "\n",
      "Gradient: [ -20.30252134  -43.00522106  -15.13090714   82.04699521  -19.24269096\n",
      "  -71.55056495 -111.90957698   65.91743335   16.12956186 -965.383006\n",
      " 2483.09902176]\n",
      "Old weights: [ 1.00057269  1.00037294  0.99961476 -0.31977977  1.00049074  1.0019178\n",
      "  1.0020656   0.34421976  0.33600047  1.02720717  0.92994694]\n",
      "New weights: [ 1.00057472  1.00037724  0.99961627 -0.31978798  1.00049266  1.00192495\n",
      "  1.00207679  0.34421317  0.33599885  1.02730371  0.92969863]\n",
      "Loss: 642.3018332100019\n",
      "\n",
      "Gradient: [ -20.29122959  -42.98060264  -15.12200531   82.00136049  -19.23166446\n",
      "  -71.51047063 -111.84681898   65.88077011   16.12059038 -964.84608095\n",
      " 2481.71798669]\n",
      "Old weights: [ 1.00057472  1.00037724  0.99961627 -0.31978798  1.00049266  1.00192495\n",
      "  1.00207679  0.34421317  0.33599885  1.02730371  0.92969863]\n",
      "New weights: [ 1.00057674  1.00038154  0.99961779 -0.31979618  1.00049459  1.00193211\n",
      "  1.00208798  0.34420658  0.33599724  1.02740019  0.92945046]\n",
      "Loss: 641.5898655049594\n",
      "\n",
      "Gradient: [ -20.27994411  -42.95599791  -15.11310843   81.95575115  -19.2206441\n",
      "  -71.47039862 -111.78409589   65.84412725   16.1116239  -964.30945452\n",
      " 2480.33771975]\n",
      "Old weights: [ 1.00057674  1.00038154  0.99961779 -0.31979618  1.00049459  1.00193211\n",
      "  1.00208798  0.34420658  0.33599724  1.02740019  0.92945046]\n",
      "New weights: [ 1.00057877  1.00038584  0.9996193  -0.31980437  1.00049651  1.00193925\n",
      "  1.00209916  0.3442      0.33599563  1.02749662  0.92920243]\n",
      "Loss: 640.8786895609236\n",
      "\n",
      "Gradient: [ -20.26866491  -42.93140687  -15.10421649   81.91016719  -19.20962986\n",
      "  -71.43034889 -111.72140768   65.80750478   16.10266241 -963.77312656\n",
      " 2478.9582205 ]\n",
      "Old weights: [ 1.00057877  1.00038584  0.9996193  -0.31980437  1.00049651  1.00193925\n",
      "  1.00209916  0.3442      0.33599563  1.02749662  0.92920243]\n",
      "New weights: [ 1.0005808   1.00039013  0.99962081 -0.31981256  1.00049843  1.0019464\n",
      "  1.00211033  0.34419342  0.33599402  1.027593    0.92895453]\n",
      "Loss: 640.1683044973852\n",
      "\n",
      "Gradient: [ -20.25739198  -42.90682951  -15.09532951   81.86460858  -19.19862175\n",
      "  -71.39032144 -111.65875434   65.77090266   16.09370591 -963.23709691\n",
      " 2477.57948851]\n",
      "Old weights: [ 1.0005808   1.00039013  0.99962081 -0.31981256  1.00049843  1.0019464\n",
      "  1.00211033  0.34419342  0.33599402  1.027593    0.92895453]\n",
      "New weights: [ 1.00058283  1.00039442  0.99962232 -0.31982075  1.00050035  1.00195353\n",
      "  1.0021215   0.34418684  0.33599241  1.02768932  0.92870677]\n",
      "Loss: 639.4587094348352\n",
      "\n",
      "Gradient: [ -20.24612532  -42.88226581  -15.08644746   81.81907531  -19.18761976\n",
      "  -71.35031625 -111.59613584   65.73432091   16.0847544  -962.70136538\n",
      " 2476.20152337]\n",
      "Old weights: [ 1.00058283  1.00039442  0.99962232 -0.31982075  1.00050035  1.00195353\n",
      "  1.0021215   0.34418684  0.33599241  1.02768932  0.92870677]\n",
      "New weights: [ 1.00058485  1.00039871  0.99962383 -0.31982893  1.00050227  1.00196067\n",
      "  1.00213266  0.34418027  0.3359908   1.02778559  0.92845915]\n",
      "Loss: 638.7499034947368\n",
      "\n",
      "Gradient: [ -20.23486493  -42.85771578  -15.07757036   81.77356736  -19.17662389\n",
      "  -71.31033331 -111.53355218   65.69775949   16.07580787 -962.16593183\n",
      " 2474.82432464]\n",
      "Old weights: [ 1.00058485  1.00039871  0.99962383 -0.31982893  1.00050227  1.00196067\n",
      "  1.00213266  0.34418027  0.3359908   1.02778559  0.92845915]\n",
      "New weights: [ 1.00058687  1.00040299  0.99962533 -0.31983711  1.00050419  1.0019678\n",
      "  1.00214381  0.3441737   0.33598919  1.02788181  0.92821167]\n",
      "Loss: 638.0418857995192\n",
      "\n",
      "Gradient: [ -20.22361081  -42.8331794   -15.06869819   81.72808474  -19.16563414\n",
      "  -71.27037261 -111.47100332   65.66121842   16.06686632 -961.63079609\n",
      " 2473.44789189]\n",
      "Old weights: [ 1.00058687  1.00040299  0.99962533 -0.31983711  1.00050419  1.0019678\n",
      "  1.00214381  0.3441737   0.33598919  1.02788181  0.92821167]\n",
      "New weights: [ 1.0005889   1.00040728  0.99962684 -0.31984528  1.0005061   1.00197493\n",
      "  1.00215496  0.34416713  0.33598759  1.02797797  0.92796433]\n",
      "Loss: 637.3346554725925\n",
      "\n",
      "Gradient: [ -20.21236294  -42.80865667  -15.05983096   81.6826274   -19.15465049\n",
      "  -71.23043414 -111.40848926   65.62469766   16.05792975 -961.09595798\n",
      " 2472.07222471]\n",
      "Old weights: [ 1.0005889   1.00040728  0.99962684 -0.31984528  1.0005061   1.00197493\n",
      "  1.00215496  0.34416713  0.33598759  1.02797797  0.92796433]\n",
      "New weights: [ 1.00059092  1.00041156  0.99962835 -0.31985345  1.00050802  1.00198205\n",
      "  1.0021661   0.34416057  0.33598598  1.02807408  0.92771712]\n",
      "Loss: 636.6282116383524\n",
      "\n",
      "Gradient: [ -20.20112133  -42.78414758  -15.05096866   81.63719536  -19.14367296\n",
      "  -71.19051788 -111.34600996   65.58819721   16.04899815 -960.56141734\n",
      " 2470.69732266]\n",
      "Old weights: [ 1.00059092  1.00041156  0.99962835 -0.31985345  1.00050802  1.00198205\n",
      "  1.0021661   0.34416057  0.33598598  1.02807408  0.92771712]\n",
      "New weights: [ 1.00059294  1.00041584  0.99962985 -0.31986161  1.00050993  1.00198917\n",
      "  1.00217723  0.34415401  0.33598438  1.02817014  0.92747005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 635.9225534221505\n",
      "\n",
      "Gradient: [ -20.18988597  -42.75965212  -15.04211129   81.59178858  -19.13270154\n",
      "  -71.15062382 -111.28356541   65.55171706   16.04007152 -960.02717401\n",
      " 2469.32318533]\n",
      "Old weights: [ 1.00059294  1.00041584  0.99962985 -0.31986161  1.00050993  1.00198917\n",
      "  1.00217723  0.34415401  0.33598438  1.02817014  0.92747005]\n",
      "New weights: [ 1.00059496  1.00042011  0.99963136 -0.31986977  1.00051185  1.00199628\n",
      "  1.00218836  0.34414745  0.33598277  1.02826614  0.92722312]\n",
      "Loss: 635.2176799503347\n",
      "\n",
      "Gradient: [ -20.17865686  -42.73517028  -15.03325885   81.54640706  -19.12173621\n",
      "  -71.11075195 -111.22115559   65.5152572    16.03114986 -959.49322782\n",
      " 2467.94981227]\n",
      "Old weights: [ 1.00059496  1.00042011  0.99963136 -0.31986977  1.00051185  1.00199628\n",
      "  1.00218836  0.34414745  0.33598277  1.02826614  0.92722312]\n",
      "New weights: [ 1.00059697  1.00042439  0.99963286 -0.31987793  1.00051376  1.0020034\n",
      "  1.00219948  0.3441409   0.33598117  1.02836209  0.92697632]\n",
      "Loss: 634.5135903502006\n",
      "\n",
      "Gradient: [ -20.16743399  -42.71070206  -15.02441133   81.50105078  -19.11077699\n",
      "  -71.07090225 -111.15878049   65.47881761   16.02223317 -958.95957861\n",
      " 2466.57720308]\n",
      "Old weights: [ 1.00059697  1.00042439  0.99963286 -0.31987793  1.00051376  1.0020034\n",
      "  1.00219948  0.3441409   0.33598117  1.02836209  0.92697632]\n",
      "New weights: [ 1.00059899  1.00042866  0.99963436 -0.31988608  1.00051567  1.0020105\n",
      "  1.0022106   0.34413435  0.33597957  1.02845799  0.92672966]\n",
      "Loss: 633.8102837500154\n",
      "\n",
      "Gradient: [ -20.15621737  -42.68624745  -15.01556873   81.45571973  -19.09982386\n",
      "  -71.03107472 -111.09644008   65.4423983    16.01332144 -958.42622621\n",
      " 2465.20535732]\n",
      "Old weights: [ 1.00059899  1.00042866  0.99963436 -0.31988608  1.00051567  1.0020105\n",
      "  1.0022106   0.34413435  0.33597957  1.02845799  0.92672966]\n",
      "New weights: [ 1.00060101  1.00043293  0.99963586 -0.31989422  1.00051758  1.00201761\n",
      "  1.00222171  0.34412781  0.33597797  1.02855383  0.92648314]\n",
      "Loss: 633.1077592790452\n",
      "\n",
      "Gradient: [ -20.14500698  -42.66180644  -15.00673105   81.4104139   -19.08887682\n",
      "  -70.99126934 -111.03413434   65.40599923   16.00441467 -957.89317046\n",
      " 2463.83427457]\n",
      "Old weights: [ 1.00060101  1.00043293  0.99963586 -0.31989422  1.00051758  1.00201761\n",
      "  1.00222171  0.34412781  0.33597797  1.02855383  0.92648314]\n",
      "New weights: [ 1.00060302  1.00043719  0.99963737 -0.31990236  1.00051949  1.0020247\n",
      "  1.00223281  0.34412127  0.33597637  1.02864962  0.92623676]\n",
      "Loss: 632.4060160674906\n",
      "\n",
      "Gradient: [ -20.13380283  -42.63737903  -14.99789828   81.36513326  -19.07793587\n",
      "  -70.9514861  -110.97186326   65.36962041   15.99551285 -957.36041119\n",
      " 2462.4639544 ]\n",
      "Old weights: [ 1.00060302  1.00043719  0.99963737 -0.31990236  1.00051949  1.0020247\n",
      "  1.00223281  0.34412127  0.33597637  1.02864962  0.92623676]\n",
      "New weights: [ 1.00060503  1.00044146  0.99963886 -0.3199105   1.0005214   1.0020318\n",
      "  1.00224391  0.34411473  0.33597477  1.02874535  0.92599051]\n",
      "Loss: 631.7050532465381\n",
      "\n",
      "Gradient: [ -20.12260492  -42.6129652   -14.98907043   81.31987781  -19.06700101\n",
      "  -70.91172499 -110.90962681   65.33326182   15.98661599 -956.82794823\n",
      " 2461.0943964 ]\n",
      "Old weights: [ 1.00060503  1.00044146  0.99963886 -0.3199105   1.0005214   1.0020318\n",
      "  1.00224391  0.34411473  0.33597477  1.02874535  0.92599051]\n",
      "New weights: [ 1.00060705  1.00044572  0.99964036 -0.31991863  1.0005233   1.00203889\n",
      "  1.002255    0.3441082   0.33597317  1.02884104  0.9257444 ]\n",
      "Loss: 631.0048699483299\n",
      "\n",
      "Gradient: [ -20.11141323  -42.58856495  -14.98024749   81.27464753  -19.05607223\n",
      "  -70.87198599 -110.84742497   65.29692346   15.97772408 -956.29578143\n",
      " 2459.72560014]\n",
      "Old weights: [ 1.00060705  1.00044572  0.99964036 -0.31991863  1.0005233   1.00203889\n",
      "  1.002255    0.3441082   0.33597317  1.02884104  0.9257444 ]\n",
      "New weights: [ 1.00060906  1.00044998  0.99964186 -0.31992676  1.00052521  1.00204598\n",
      "  1.00226608  0.34410167  0.33597157  1.02893667  0.92549843]\n",
      "Loss: 630.3054653059813\n",
      "\n",
      "Gradient: [ -20.10022776  -42.56417827  -14.97142945   81.22944241  -19.04514952\n",
      "  -70.8322691  -110.78525774   65.2606053    15.96883711 -955.76391061\n",
      " 2458.35756518]\n",
      "Old weights: [ 1.00060906  1.00044998  0.99964186 -0.31992676  1.00052521  1.00204598\n",
      "  1.00226608  0.34410167  0.33597157  1.02893667  0.92549843]\n",
      "New weights: [ 1.00061107  1.00045423  0.99964336 -0.31993488  1.00052711  1.00205306\n",
      "  1.00227716  0.34409514  0.33596997  1.02903224  0.9252526 ]\n",
      "Loss: 629.6068384535669\n",
      "\n",
      "Gradient: [ -20.08904852  -42.53980516  -14.96261632   81.18426243  -19.03423289\n",
      "  -70.7925743  -110.72312508   65.22430734   15.95995509 -955.23233562\n",
      " 2456.99029112]\n",
      "Old weights: [ 1.00061107  1.00045423  0.99964336 -0.31993488  1.00052711  1.00205306\n",
      "  1.00227716  0.34409514  0.33596997  1.02903224  0.9252526 ]\n",
      "New weights: [ 1.00061308  1.00045849  0.99964486 -0.319943    1.00052902  1.00206014\n",
      "  1.00228823  0.34408862  0.33596838  1.02912777  0.9250069 ]\n",
      "Loss: 628.9089885261247\n",
      "\n",
      "Gradient: [ -20.07787549  -42.5154456   -14.95380809   81.13910758  -19.02332234\n",
      "  -70.75290157 -110.66102697   65.18802957   15.95107802 -954.70105628\n",
      " 2455.62377753]\n",
      "Old weights: [ 1.00061308  1.00045849  0.99964486 -0.319943    1.00052902  1.00206014\n",
      "  1.00228823  0.34408862  0.33596838  1.02912777  0.9250069 ]\n",
      "New weights: [ 1.00061508  1.00046274  0.99964635 -0.31995112  1.00053092  1.00206722\n",
      "  1.0022993   0.3440821   0.33596678  1.02922324  0.92476133]\n",
      "Loss: 628.2119146596585\n",
      "\n",
      "Gradient: [ -20.06670868  -42.49109959  -14.94500476   81.09397785  -19.01241785\n",
      "  -70.71325091 -110.59896341   65.15177197   15.94220588 -954.17007244\n",
      " 2454.25802398]\n",
      "Old weights: [ 1.00061508  1.00046274  0.99964635 -0.31995112  1.00053092  1.00206722\n",
      "  1.0022993   0.3440821   0.33596678  1.02922324  0.92476133]\n",
      "New weights: [ 1.00061709  1.00046699  0.99964785 -0.31995923  1.00053282  1.00207429\n",
      "  1.00231036  0.34407559  0.33596519  1.02931865  0.92451591]\n",
      "Loss: 627.515615991136\n",
      "\n",
      "Gradient: [ -20.05554808  -42.46676712  -14.93620633   81.04887322  -19.00151943\n",
      "  -70.6736223  -110.53693436   65.11553454   15.93333868 -953.63938393\n",
      " 2452.89303005]\n",
      "Old weights: [ 1.00061709  1.00046699  0.99964785 -0.31995923  1.00053282  1.00207429\n",
      "  1.00231036  0.34407559  0.33596519  1.02931865  0.92451591]\n",
      "New weights: [ 1.0006191   1.00047123  0.99964934 -0.31996733  1.00053472  1.00208135\n",
      "  1.00232141  0.34406908  0.33596359  1.02941402  0.92427062]\n",
      "Loss: 626.8200916584674\n",
      "\n",
      "Gradient: [ -20.04439369  -42.44244819  -14.92741279   81.00379367  -18.99062707\n",
      "  -70.63401574 -110.47493982   65.07931727   15.92447641 -953.10899058\n",
      " 2451.52879532]\n",
      "Old weights: [ 1.0006191   1.00047123  0.99964934 -0.31996733  1.00053472  1.00208135\n",
      "  1.00232141  0.34406908  0.33596359  1.02941402  0.92427062]\n",
      "New weights: [ 1.0006211   1.00047548  0.99965083 -0.31997543  1.00053662  1.00208842\n",
      "  1.00233246  0.34406257  0.335962    1.02950933  0.92402547]\n",
      "Loss: 626.12534080054\n",
      "\n",
      "Gradient: [ -20.0332455   -42.41814278  -14.91862414   80.9587392   -18.97974076\n",
      "  -70.5944312  -110.41297976   65.04312013   15.91561907 -952.57889223\n",
      " 2450.16531936]\n",
      "Old weights: [ 1.0006211   1.00047548  0.99965083 -0.31997543  1.00053662  1.00208842\n",
      "  1.00233246  0.34406257  0.335962    1.02950933  0.92402547]\n",
      "New weights: [ 1.0006231   1.00047972  0.99965232 -0.31998353  1.00053852  1.00209548\n",
      "  1.0023435   0.34405606  0.33596041  1.02960459  0.92378045]\n",
      "Loss: 625.431362557194\n",
      "\n",
      "Gradient: [ -20.02210351  -42.39385089  -14.90984038   80.91370979  -18.96886051\n",
      "  -70.55486868 -110.35105415   65.00694313   15.90676666 -952.04908872\n",
      " 2448.80260176]\n",
      "Old weights: [ 1.0006231   1.00047972  0.99965232 -0.31998353  1.00053852  1.00209548\n",
      "  1.0023435   0.34405606  0.33596041  1.02960459  0.92378045]\n",
      "New weights: [ 1.00062511  1.00048396  0.99965381 -0.31999162  1.00054041  1.00210253\n",
      "  1.00235454  0.34404956  0.33595882  1.02969979  0.92353557]\n",
      "Loss: 624.7381560692139\n",
      "\n",
      "Gradient: [ -20.01096772  -42.36957251  -14.9010615    80.86870543  -18.95798632\n",
      "  -70.51532816 -110.28916299   64.97078625   15.89791918 -951.51957988\n",
      " 2447.4406421 ]\n",
      "Old weights: [ 1.00062511  1.00048396  0.99965381 -0.31999162  1.00054041  1.00210253\n",
      "  1.00235454  0.34404956  0.33595882  1.02969979  0.92353557]\n",
      "New weights: [ 1.00062711  1.0004882   0.9996553  -0.3199997   1.00054231  1.00210958\n",
      "  1.00236557  0.34404307  0.33595723  1.02979494  0.92329083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 624.0457204783662\n",
      "\n",
      "Gradient: [ -19.99983812  -42.34530763  -14.89228751   80.82372609  -18.94711817\n",
      "  -70.47580964 -110.22730626   64.93464948   15.88907661 -950.99036555\n",
      " 2446.07943994]\n",
      "Old weights: [ 1.00062711  1.0004882   0.9996553  -0.3199997   1.00054231  1.00210958\n",
      "  1.00236557  0.34404307  0.33595723  1.02979494  0.92329083]\n",
      "New weights: [ 1.00062911  1.00049243  0.99965679 -0.32000779  1.0005442   1.00211663\n",
      "  1.00237659  0.34403657  0.33595564  1.02989004  0.92304622]\n",
      "Loss: 623.3540549273463\n",
      "\n",
      "Gradient: [ -19.98871471  -42.32105625  -14.8835184    80.77877177  -18.93625607\n",
      "  -70.43631309 -110.16548393   64.8985328    15.88023897 -950.46144556\n",
      " 2444.71899488]\n",
      "Old weights: [ 1.00062911  1.00049243  0.99965679 -0.32000779  1.0005442   1.00211663\n",
      "  1.00237659  0.34403657  0.33595564  1.02989004  0.92304622]\n",
      "New weights: [ 1.00063111  1.00049666  0.99965828 -0.32001587  1.0005461   1.00212368\n",
      "  1.00238761  0.34403008  0.33595405  1.02998509  0.92280175]\n",
      "Loss: 622.663158559803\n",
      "\n",
      "Gradient: [ -19.97759749  -42.29681836  -14.87475416   80.73384246  -18.9254\n",
      "  -70.39683852 -110.10369598   64.86243622   15.87140624 -949.93281976\n",
      " 2443.35930648]\n",
      "Old weights: [ 1.00063111  1.00049666  0.99965828 -0.32001587  1.0005461   1.00212368\n",
      "  1.00238761  0.34403008  0.33595405  1.02998509  0.92280175]\n",
      "New weights: [ 1.0006331   1.00050089  0.99965977 -0.32002394  1.00054799  1.00213072\n",
      "  1.00239862  0.3440236   0.33595247  1.03008008  0.92255741]\n",
      "Loss: 621.9730305203572\n",
      "\n",
      "Gradient: [ -19.96648646  -42.27259395  -14.8659948    80.68893814  -18.91454998\n",
      "  -70.35738589 -110.0419424    64.82635971   15.86257843 -949.40448797\n",
      " 2442.00037433]\n",
      "Old weights: [ 1.0006331   1.00050089  0.99965977 -0.32002394  1.00054799  1.00213072\n",
      "  1.00239862  0.3440236   0.33595247  1.03008008  0.92255741]\n",
      "New weights: [ 1.0006351   1.00050512  0.99966126 -0.32003201  1.00054988  1.00213775\n",
      "  1.00240962  0.34401711  0.33595088  1.03017502  0.92231321]\n",
      "Loss: 621.2836699545734\n",
      "\n",
      "Gradient: [ -19.9553816   -42.24838301  -14.85724031   80.64405879  -18.90370599\n",
      "  -70.31795522 -109.98022316   64.79030326   15.85375553 -948.87645004\n",
      " 2440.64219801]\n",
      "Old weights: [ 1.0006351   1.00050512  0.99966126 -0.32003201  1.00054988  1.00213775\n",
      "  1.00240962  0.34401711  0.33595088  1.03017502  0.92231321]\n",
      "New weights: [ 1.0006371   1.00050934  0.99966274 -0.32004007  1.00055177  1.00214478\n",
      "  1.00242062  0.34401063  0.33594929  1.03026991  0.92206915]\n",
      "Loss: 620.5950760089632\n",
      "\n",
      "Gradient: [ -19.94428292  -42.22418554  -14.84849069   80.59920441  -18.89286803\n",
      "  -70.27854647 -109.91853826   64.75426687   15.84493754 -948.3487058\n",
      " 2439.2847771 ]\n",
      "Old weights: [ 1.0006371   1.00050934  0.99966274 -0.32004007  1.00055177  1.00214478\n",
      "  1.00242062  0.34401063  0.33594929  1.03026991  0.92206915]\n",
      "New weights: [ 1.00063909  1.00051357  0.99966423 -0.32004813  1.00055366  1.00215181\n",
      "  1.00243161  0.34400416  0.33594771  1.03036474  0.92182522]\n",
      "Loss: 619.9072478309702\n",
      "\n",
      "Gradient: [ -19.93319041  -42.20000153  -14.83974594   80.55437497  -18.8820361\n",
      "  -70.23915964 -109.85688766   64.71825052   15.83612445 -947.82125508\n",
      " 2437.92811117]\n",
      "Old weights: [ 1.00063909  1.00051357  0.99966423 -0.32004813  1.00055366  1.00215181\n",
      "  1.00243161  0.34400416  0.33594771  1.03036474  0.92182522]\n",
      "New weights: [ 1.00064108  1.00051779  0.99966571 -0.32005619  1.00055555  1.00215883\n",
      "  1.0024426   0.34399769  0.33594613  1.03045953  0.92158142]\n",
      "Loss: 619.2201845690308\n",
      "\n",
      "Gradient: [ -19.92210407  -42.17583097  -14.83100605   80.50957047  -18.8712102\n",
      "  -70.19979472 -109.79527135   64.6822542    15.82731626 -947.29409773\n",
      " 2436.57219981]\n",
      "Old weights: [ 1.00064108  1.00051779  0.99966571 -0.32005619  1.00055555  1.00215883\n",
      "  1.0024426   0.34399769  0.33594613  1.03045953  0.92158142]\n",
      "New weights: [ 1.00064308  1.000522    0.99966719 -0.32006424  1.00055744  1.00216585\n",
      "  1.00245358  0.34399122  0.33594454  1.03055426  0.92133777]\n",
      "Loss: 618.5338853724794\n",
      "\n",
      "Gradient: [ -19.9110239   -42.15167385  -14.82227102   80.46479088  -18.86039031\n",
      "  -70.16045169 -109.73368932   64.64627791   15.81851298 -946.76723358\n",
      " 2435.2170426 ]\n",
      "Old weights: [ 1.00064308  1.000522    0.99966719 -0.32006424  1.00055744  1.00216585\n",
      "  1.00245358  0.34399122  0.33594454  1.03055426  0.92133777]\n",
      "New weights: [ 1.00064507  1.00052622  0.99966868 -0.32007228  1.00055932  1.00217287\n",
      "  1.00246455  0.34398475  0.33594296  1.03064893  0.92109425]\n",
      "Loss: 617.8483493916283\n",
      "\n",
      "Gradient: [ -19.89994989  -42.12753017  -14.81354085   80.42003621  -18.84957645\n",
      "  -70.12113055 -109.67214153   64.61032162   15.80971459 -946.24066247\n",
      " 2433.86263911]\n",
      "Old weights: [ 1.00064507  1.00052622  0.99966868 -0.32007228  1.00055932  1.00217287\n",
      "  1.00246455  0.34398475  0.33594296  1.03064893  0.92109425]\n",
      "New weights: [ 1.00064706  1.00053043  0.99967016 -0.32008033  1.00056121  1.00217988\n",
      "  1.00247552  0.34397829  0.33594138  1.03074356  0.92085086]\n",
      "Loss: 617.1635757777221\n",
      "\n",
      "Gradient: [ -19.88888204  -42.10339992  -14.80481554   80.37530642  -18.8387686\n",
      "  -70.08183128 -109.61062798   64.57438533   15.80092109 -945.71438423\n",
      " 2432.50898893]\n",
      "Old weights: [ 1.00064706  1.00053043  0.99967016 -0.32008033  1.00056121  1.00217988\n",
      "  1.00247552  0.34397829  0.33594138  1.03074356  0.92085086]\n",
      "New weights: [ 1.00064905  1.00053464  0.99967164 -0.32008836  1.00056309  1.00218689\n",
      "  1.00248648  0.34397184  0.3359398   1.03083813  0.92060761]\n",
      "Loss: 616.479563682953\n",
      "\n",
      "Gradient: [ -19.87782034  -42.07928309  -14.79609508   80.33060152  -18.82796676\n",
      "  -70.04255386 -109.54914864   64.53846902   15.79213249 -945.1883987\n",
      " 2431.15609165]\n",
      "Old weights: [ 1.00064905  1.00053464  0.99967164 -0.32008836  1.00056309  1.00218689\n",
      "  1.00248648  0.34397184  0.3359398   1.03083813  0.92060761]\n",
      "New weights: [ 1.00065103  1.00053885  0.99967312 -0.3200964   1.00056497  1.0021939\n",
      "  1.00249743  0.34396538  0.33593822  1.03093265  0.92036449]\n",
      "Loss: 615.7963122604498\n",
      "\n",
      "Gradient: [ -19.8667648   -42.05517967  -14.78737947   80.28592148  -18.81717092\n",
      "  -70.00329829 -109.48770349   64.5025727    15.78334878 -944.66270573\n",
      " 2429.80394683]\n",
      "Old weights: [ 1.00065103  1.00053885  0.99967312 -0.3200964   1.00056497  1.0021939\n",
      "  1.00249743  0.34396538  0.33593822  1.03093265  0.92036449]\n",
      "New weights: [ 1.00065302  1.00054306  0.9996746  -0.32010443  1.00056686  1.0022009\n",
      "  1.00250838  0.34395893  0.33593664  1.03102711  0.92012151]\n",
      "Loss: 615.1138206642885\n",
      "\n",
      "Gradient: [ -19.85571541  -42.03108966  -14.77866871   80.24126629  -18.8063811\n",
      "  -69.96406455 -109.42629252   64.46669634   15.77456995 -944.13730514\n",
      " 2428.45255407]\n",
      "Old weights: [ 1.00065302  1.00054306  0.9996746  -0.32010443  1.00056686  1.0022009\n",
      "  1.00250838  0.34395893  0.33593664  1.03102711  0.92012151]\n",
      "New weights: [ 1.00065501  1.00054726  0.99967607 -0.32011245  1.00056874  1.00220789\n",
      "  1.00251932  0.34395248  0.33593507  1.03112153  0.91987867]\n",
      "Loss: 614.4320880494926\n",
      "\n",
      "Gradient: [ -19.84467216  -42.00701305  -14.76996279   80.19663593  -18.79559727\n",
      "  -69.92485264 -109.36491571   64.43083993   15.76579601 -943.61219677\n",
      " 2427.10191294]\n",
      "Old weights: [ 1.00065501  1.00054726  0.99967607 -0.32011245  1.00056874  1.00220789\n",
      "  1.00251932  0.34395248  0.33593507  1.03112153  0.91987867]\n",
      "New weights: [ 1.00065699  1.00055146  0.99967755 -0.32012047  1.00057062  1.00221488\n",
      "  1.00253026  0.34394604  0.33593349  1.03121589  0.91963596]\n",
      "Loss: 613.7511135720104\n",
      "\n",
      "Gradient: [ -19.83363505  -41.98294982  -14.76126171   80.1520304   -18.78481944\n",
      "  -69.88566253 -109.30357303   64.39500346   15.75702694 -943.08738046\n",
      " 2425.75202302]\n",
      "Old weights: [ 1.00065699  1.00055146  0.99967755 -0.32012047  1.00057062  1.00221488\n",
      "  1.00253026  0.34394604  0.33593349  1.03121589  0.91963596]\n",
      "New weights: [ 1.00065897  1.00055566  0.99967903 -0.32012848  1.0005725   1.00222187\n",
      "  1.00254119  0.3439396   0.33593191  1.0313102   0.91939338]\n",
      "Loss: 613.070896388745\n",
      "\n",
      "Gradient: [ -19.82260409  -41.95889999  -14.75256548   80.10744968  -18.77404761\n",
      "  -69.84649422 -109.24226447   64.35918693   15.74826275 -942.56285605\n",
      " 2424.40288391]\n",
      "Old weights: [ 1.00065897  1.00055566  0.99967903 -0.32012848  1.0005725   1.00222187\n",
      "  1.00254119  0.3439396   0.33593191  1.0313102   0.91939338]\n",
      "New weights: [ 1.00066096  1.00055985  0.9996805  -0.3201365   1.00057437  1.00222886\n",
      "  1.00255212  0.34393317  0.33593034  1.03140445  0.91915094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 612.391435657513\n",
      "\n",
      "Gradient: [ -19.81157926  -41.93486353  -14.74387408   80.06289376  -18.76328176\n",
      "  -69.8073477  -109.18099002   64.32339032   15.73950344 -942.03862338\n",
      " 2423.05449517]\n",
      "Old weights: [ 1.00066096  1.00055985  0.9996805  -0.3201365   1.00057437  1.00222886\n",
      "  1.00255212  0.34393317  0.33593034  1.03140445  0.91915094]\n",
      "New weights: [ 1.00066294  1.00056405  0.99968198 -0.3201445   1.00057625  1.00223584\n",
      "  1.00256303  0.34392673  0.33592876  1.03149866  0.91890864]\n",
      "Loss: 611.7127305370981\n",
      "\n",
      "Gradient: [ -19.80056056  -41.91084043  -14.73518751   80.01836262  -18.75252191\n",
      "  -69.76822295 -109.11974964   64.28761361   15.730749   -941.51468229\n",
      " 2421.7068564 ]\n",
      "Old weights: [ 1.00066294  1.00056405  0.99968198 -0.3201445   1.00057625  1.00223584\n",
      "  1.00256303  0.34392673  0.33592876  1.03149866  0.91890864]\n",
      "New weights: [ 1.00066492  1.00056824  0.99968345 -0.3201525   1.00057812  1.00224282\n",
      "  1.00257395  0.34392031  0.33592719  1.03159281  0.91866647]\n",
      "Loss: 611.034780187196\n",
      "\n",
      "Gradient: [ -19.78954799  -41.8868307   -14.72650578   79.97385624  -18.74176804\n",
      "  -69.72911996 -109.05854332   64.25185681   15.72199944 -940.9910326\n",
      " 2420.35996717]\n",
      "Old weights: [ 1.00066492  1.00056824  0.99968345 -0.3201525   1.00057812  1.00224282\n",
      "  1.00257395  0.34392031  0.33592719  1.03159281  0.91866647]\n",
      "New weights: [ 1.0006669   1.00057243  0.99968492 -0.3201605   1.00058     1.00224979\n",
      "  1.00258485  0.34391388  0.33592562  1.03168691  0.91842443]\n",
      "Loss: 610.3575837684585\n",
      "\n",
      "Gradient: [ -19.77854154  -41.86283433  -14.71782887   79.92937462  -18.73102015\n",
      "  -69.69003872 -108.99737105   64.21611989   15.71325473 -940.46767417\n",
      " 2419.01382708]\n",
      "Old weights: [ 1.0006669   1.00057243  0.99968492 -0.3201605   1.00058     1.00224979\n",
      "  1.00258485  0.34391388  0.33592562  1.03168691  0.91842443]\n",
      "New weights: [ 1.00066887  1.00057661  0.99968639 -0.32016849  1.00058187  1.00225676\n",
      "  1.00259575  0.34390746  0.33592405  1.03178095  0.91818253]\n",
      "Loss: 609.6811404424424\n",
      "\n",
      "Gradient: [ -19.76754122  -41.8388513   -14.70915679   79.88491774  -18.72027824\n",
      "  -69.65097921 -108.9362328    64.18040285   15.70451489 -939.94460682\n",
      " 2417.66843569]\n",
      "Old weights: [ 1.00066887  1.00057661  0.99968639 -0.32016849  1.00058187  1.00225676\n",
      "  1.00259575  0.34390746  0.33592405  1.03178095  0.91818253]\n",
      "New weights: [ 1.00067085  1.0005808   0.99968787 -0.32017648  1.00058374  1.00226372\n",
      "  1.00260664  0.34390104  0.33592248  1.03187495  0.91794076]\n",
      "Loss: 609.005449371656\n",
      "\n",
      "Gradient: [ -19.75654701  -41.81488161  -14.70048954   79.84048559  -18.7095423\n",
      "  -69.61194143 -108.87512855   64.14470567   15.69577992 -939.4218304\n",
      " 2416.3237926 ]\n",
      "Old weights: [ 1.00067085  1.0005808   0.99968787 -0.32017648  1.00058374  1.00226372\n",
      "  1.00260664  0.34390104  0.33592248  1.03187495  0.91794076]\n",
      "New weights: [ 1.00067283  1.00058498  0.99968934 -0.32018447  1.00058561  1.00227068\n",
      "  1.00261753  0.34389463  0.33592091  1.03196889  0.91769913]\n",
      "Loss: 608.3305097195328\n",
      "\n",
      "Gradient: [ -19.74555892  -41.79092525  -14.6918271    79.79607815  -18.69881234\n",
      "  -69.57292536 -108.81405829   64.10902835   15.6870498  -938.89934474\n",
      " 2414.97989739]\n",
      "Old weights: [ 1.00067283  1.00058498  0.99968934 -0.32018447  1.00058561  1.00227068\n",
      "  1.00261753  0.34389463  0.33592091  1.03196889  0.91769913]\n",
      "New weights: [ 1.0006748   1.00058916  0.9996908  -0.32019245  1.00058748  1.00227764\n",
      "  1.00262841  0.34388821  0.33591934  1.03206278  0.91745763]\n",
      "Loss: 607.6563206504474\n",
      "\n",
      "Gradient: [ -19.73457694  -41.76698221  -14.68316949   79.75169541  -18.68808834\n",
      "  -69.533931   -108.753022     64.07337087   15.67832454 -938.37714968\n",
      " 2413.63674965]\n",
      "Old weights: [ 1.0006748   1.00058916  0.9996908  -0.32019245  1.00058748  1.00227764\n",
      "  1.00262841  0.34388821  0.33591934  1.03206278  0.91745763]\n",
      "New weights: [ 1.00067678  1.00059333  0.99969227 -0.32020042  1.00058935  1.00228459\n",
      "  1.00263929  0.34388181  0.33591777  1.03215662  0.91721627]\n",
      "Loss: 606.9828813296909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# w_init = [5000,5000,5000,5000,5000,5000,5000,5000,5000,5000,5000]\n",
    "w_init = np.ones(11)\n",
    "weights,losses = gradDescent(w_init,0.0000001,X,datY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration no. :  0\n",
      "Gradient length: 11635.223655044741\n",
      "Loss: 1663.5497868007726\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  1\n",
      "Gradient length: 196073.06397125253\n",
      "Loss: 314956.70987297205\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  2\n",
      "Gradient length: 3392087.1413169615\n",
      "Loss: 94100352.58988515\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  3\n",
      "Gradient length: 58688678.600908265\n",
      "Loss: 28168479546.353947\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  4\n",
      "Gradient length: 1015410823.1509483\n",
      "Loss: 8432149757341.05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  5\n",
      "Gradient length: 17568280038.682613\n",
      "Loss: 2524138759456322.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  6\n",
      "Gradient length: 303960186834.3184\n",
      "Loss: 7.555933730758595e+17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  7\n",
      "Gradient length: 5259011979369.809\n",
      "Loss: 2.261846117997344e+20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  8\n",
      "Gradient length: 90989570993487.73\n",
      "Loss: 6.770768569175854e+22\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  9\n",
      "Gradient length: 1574269475341839.8\n",
      "Loss: 2.0268092799314765e+25\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  10\n",
      "Gradient length: 2.7237455391128776e+16\n",
      "Loss: 6.067192838222181e+27\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-f43de9a24114>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.32\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m0.343\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.336\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.91\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-0c96f86949c0>\u001b[0m in \u001b[0;36mgradDescent\u001b[1;34m(w_init, alpha, X, y, maxiter, eps)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcuriter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgrad_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration no. : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcuriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mcurr_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_k_old\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mw_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_k_old\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcurr_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#             print(\":\",curr_grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-6186aa0e2141>\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(w_k, X, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mconstant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_sum\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp_sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_init = [ 1,1 , 0.999, -0.32,  1 , 1 ,1 , 0.343,  0.336,  1,  0.91]\n",
    "weights,losses = gradDescent(w_init,0.000001,X,datY)\n",
    "#loss increases for alpha >= 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration no. :  0\n",
      "Gradient length: 27182062.78925531\n",
      "Loss: 14008624.051782243\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  1\n",
      "Gradient length: 22566143.75643872\n",
      "Loss: 9674576.431098636\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  2\n",
      "Gradient length: 18734211.180316836\n",
      "Loss: 6685501.32935997\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  3\n",
      "Gradient length: 15553119.499081958\n",
      "Loss: 4623581.233352227\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  4\n",
      "Gradient length: 12912336.1787417\n",
      "Loss: 3200841.7377423076\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  5\n",
      "Gradient length: 10720101.238721304\n",
      "Loss: 2218795.6582802474\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  6\n",
      "Gradient length: 8900239.026174955\n",
      "Loss: 1540630.5201629985\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  7\n",
      "Gradient length: 7389511.464090839\n",
      "Loss: 1072040.6412196609\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  8\n",
      "Gradient length: 6135420.811697937\n",
      "Loss: 748016.8702358542\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  9\n",
      "Gradient length: 5094385.593901622\n",
      "Loss: 523742.2095876767\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  10\n",
      "Gradient length: 4230226.322131078\n",
      "Loss: 368317.12794512574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  11\n",
      "Gradient length: 3512908.3925659973\n",
      "Loss: 260435.041248055\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  12\n",
      "Gradient length: 2917498.4831920518\n",
      "Loss: 185401.81656917222\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  13\n",
      "Gradient length: 2423298.188967879\n",
      "Loss: 133081.55750457654\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-3b381699a725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#w_init = [ 1,1 , 0.999, -0.32,  1 , 1 ,1 , 0.343,  0.336,  1,  0.91]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0000001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#loss increases for alpha >= 0.000001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-0c96f86949c0>\u001b[0m in \u001b[0;36mgradDescent\u001b[1;34m(w_init, alpha, X, y, maxiter, eps)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcuriter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgrad_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration no. : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcuriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mcurr_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_k_old\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mw_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_k_old\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcurr_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#             print(\":\",curr_grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-6186aa0e2141>\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(w_k, X, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mconstant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtemp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_sum\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp_sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_init = np.ones(11)\n",
    "weights,losses = gradDescent(w_init,0.0000001,X,datY)\n",
    "#for these weights initial loss is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration no. :  0\n",
      "Gradient length: 11635.223655044741\n",
      "Loss: 622.0780616216131\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  1\n",
      "Gradient length: 9769.772287370626\n",
      "Loss: 620.6315525592096\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  2\n",
      "Gradient length: 8241.67967418856\n",
      "Loss: 619.4202903496746\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  3\n",
      "Gradient length: 6996.627490420768\n",
      "Loss: 618.3715737497779\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  4\n",
      "Gradient length: 5989.442198738594\n",
      "Loss: 617.4352787823873\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  5\n",
      "Gradient length: 5182.23886308577\n",
      "Loss: 616.5768432953826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  6\n",
      "Gradient length: 4542.803826821416\n",
      "Loss: 615.7724318692033\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  7\n",
      "Gradient length: 4043.2504961080563\n",
      "Loss: 615.0056033886173\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  8\n",
      "Gradient length: 3659.02517490817\n",
      "Loss: 614.26501423382\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  9\n",
      "Gradient length: 3368.3267059643294\n",
      "Loss: 613.5428352132614\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  10\n",
      "Gradient length: 3151.914455489855\n",
      "Loss: 612.8336604067342\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  11\n",
      "Gradient length: 2993.1597243285564\n",
      "Loss: 612.1337550359956\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  12\n",
      "Gradient length: 2878.1357020847677\n",
      "Loss: 611.4405369988593\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  13\n",
      "Gradient length: 2795.5864865384838\n",
      "Loss: 610.7522194506515\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  14\n",
      "Gradient length: 2736.7232454857804\n",
      "Loss: 610.0675643868984\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  15\n",
      "Gradient length: 2694.8873557294983\n",
      "Loss: 609.3857127352074\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  16\n",
      "Gradient length: 2665.156575373622\n",
      "Loss: 608.7060671844778\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  17\n",
      "Gradient length: 2643.960936016034\n",
      "Loss: 608.028211367384\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  18\n",
      "Gradient length: 2628.747888900033\n",
      "Loss: 607.3518541037322\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  19\n",
      "Gradient length: 2617.7112038514515\n",
      "Loss: 606.6767909217559\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  20\n",
      "Gradient length: 2609.5821957178173\n",
      "Loss: 606.0028774924103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  21\n",
      "Gradient length: 2603.4743600655183\n",
      "Loss: 605.3300112791036\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  22\n",
      "Gradient length: 2598.7704448052546\n",
      "Loss: 604.6581188538066\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  23\n",
      "Gradient length: 2595.0416876320264\n",
      "Loss: 603.987147122281\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  24\n",
      "Gradient length: 2591.990699750535\n",
      "Loss: 603.3170572471124\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  25\n",
      "Gradient length: 2589.4113827125207\n",
      "Loss: 602.6478204331345\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  26\n",
      "Gradient length: 2587.16094762216\n",
      "Loss: 601.9794149992765\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  27\n",
      "Gradient length: 2585.1404535995243\n",
      "Loss: 601.3118243395852\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  28\n",
      "Gradient length: 2583.281305228623\n",
      "Loss: 600.6450354992412\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  29\n",
      "Gradient length: 2581.5359000569874\n",
      "Loss: 599.9790381765508\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  30\n",
      "Gradient length: 2579.871157748695\n",
      "Loss: 599.3138240202617\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  31\n",
      "Gradient length: 2578.2640460963453\n",
      "Loss: 598.6493861320195\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  32\n",
      "Gradient length: 2576.6984888626994\n",
      "Loss: 597.9857187117035\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  33\n",
      "Gradient length: 2575.1632289721024\n",
      "Loss: 597.3228168024818\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  34\n",
      "Gradient length: 2573.6503518054938\n",
      "Loss: 596.6606761056648\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  35\n",
      "Gradient length: 2572.1542644346237\n",
      "Loss: 595.9992928448781\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  36\n",
      "Gradient length: 2570.6709897214573\n",
      "Loss: 595.3386636648723\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  37\n",
      "Gradient length: 2569.1976778521507\n",
      "Loss: 594.6787855552541\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  38\n",
      "Gradient length: 2567.73226803826\n",
      "Loss: 594.0196557919542\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  39\n",
      "Gradient length: 2566.2732539503922\n",
      "Loss: 593.3612718916384\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  40\n",
      "Gradient length: 2564.8195208334264\n",
      "Loss: 592.7036315755983\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  41\n",
      "Gradient length: 2563.3702321797546\n",
      "Loss: 592.0467327406616\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  42\n",
      "Gradient length: 2561.9247506880183\n",
      "Loss: 591.3905734354624\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  43\n",
      "Gradient length: 2560.482582962663\n",
      "Loss: 590.7351518406876\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  44\n",
      "Gradient length: 2559.0433406710354\n",
      "Loss: 590.0804662525685\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  45\n",
      "Gradient length: 2557.6067131265363\n",
      "Loss: 589.4265150688534\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  46\n",
      "Gradient length: 2556.172447819529\n",
      "Loss: 588.7732967767549\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  47\n",
      "Gradient length: 2554.740336489694\n",
      "Loss: 588.1208099425913\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  48\n",
      "Gradient length: 2553.310205074317\n",
      "Loss: 587.4690532027465\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  49\n",
      "Gradient length: 2551.881906377481\n",
      "Loss: 586.8180252557828\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  50\n",
      "Gradient length: 2550.455314659138\n",
      "Loss: 586.1677248555684\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  51\n",
      "Gradient length: 2549.030321586531\n",
      "Loss: 585.5181508051216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  52\n",
      "Gradient length: 2547.6068331592096\n",
      "Loss: 584.8693019512949\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  53\n",
      "Gradient length: 2546.184767336461\n",
      "Loss: 584.221177179993\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  54\n",
      "Gradient length: 2544.7640521761423\n",
      "Loss: 583.5737754119773\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  55\n",
      "Gradient length: 2543.3446243506514\n",
      "Loss: 582.9270955991103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  56\n",
      "Gradient length: 2541.9264279447702\n",
      "Loss: 582.2811367210671\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  57\n",
      "Gradient length: 2540.509413467037\n",
      "Loss: 581.6358977823687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  58\n",
      "Gradient length: 2539.093537026011\n",
      "Loss: 580.9913778097524\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  59\n",
      "Gradient length: 2537.6787596352765\n",
      "Loss: 580.3475758498582\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  60\n",
      "Gradient length: 2536.2650466211458\n",
      "Loss: 579.7044909671522\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  61\n",
      "Gradient length: 2534.852367113437\n",
      "Loss: 579.0621222420606\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  62\n",
      "Gradient length: 2533.44069360447\n",
      "Loss: 578.4204687693334\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  63\n",
      "Gradient length: 2532.030001564769\n",
      "Loss: 577.7795296565872\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  64\n",
      "Gradient length: 2530.620269106615\n",
      "Loss: 577.139304022996\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  65\n",
      "Gradient length: 2529.2114766882637\n",
      "Loss: 576.4997909980946\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  66\n",
      "Gradient length: 2527.8036068532665\n",
      "Loss: 575.8609897208054\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  67\n",
      "Gradient length: 2526.3966440000286\n",
      "Loss: 575.2228993384601\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  68\n",
      "Gradient length: 2524.9905741775924\n",
      "Loss: 574.5855190060012\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  69\n",
      "Gradient length: 2523.5853849048017\n",
      "Loss: 573.9488478852477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  70\n",
      "Gradient length: 2522.1810650094803\n",
      "Loss: 573.312885144222\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  71\n",
      "Gradient length: 2520.7776044856014\n",
      "Loss: 572.6776299566083\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  72\n",
      "Gradient length: 2519.374994366224\n",
      "Loss: 572.043081501178\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  73\n",
      "Gradient length: 2517.97322661042\n",
      "Loss: 571.4092389613851\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  74\n",
      "Gradient length: 2516.5722940026935\n",
      "Loss: 570.7761015248991\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  75\n",
      "Gradient length: 2515.1721900631555\n",
      "Loss: 570.1436683832887\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  76\n",
      "Gradient length: 2513.772908967887\n",
      "Loss: 569.5119387316525\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  77\n",
      "Gradient length: 2512.3744454775815\n",
      "Loss: 568.8809117683463\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  78\n",
      "Gradient length: 2510.976794874226\n",
      "Loss: 568.2505866947185\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  79\n",
      "Gradient length: 2509.579952904581\n",
      "Loss: 567.6209627148818\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  80\n",
      "Gradient length: 2508.1839157298114\n",
      "Loss: 566.9920390355115\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  81\n",
      "Gradient length: 2506.788679880519\n",
      "Loss: 566.3638148656324\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  82\n",
      "Gradient length: 2505.394242216747\n",
      "Loss: 565.7362894164925\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  83\n",
      "Gradient length: 2504.0005998923352\n",
      "Loss: 565.1094619013934\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  84\n",
      "Gradient length: 2502.607750323031\n",
      "Loss: 564.4833315355622\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  85\n",
      "Gradient length: 2501.215691158176\n",
      "Loss: 563.8578975360352\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  86\n",
      "Gradient length: 2499.82442025534\n",
      "Loss: 563.2331591215576\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  87\n",
      "Gradient length: 2498.433935657912\n",
      "Loss: 562.6091155124915\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  88\n",
      "Gradient length: 2497.0442355748273\n",
      "Loss: 561.9857659307256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  89\n",
      "Gradient length: 2495.6553183627825\n",
      "Loss: 561.3631095996116\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  90\n",
      "Gradient length: 2494.267182510144\n",
      "Loss: 560.7411457438785\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  91\n",
      "Gradient length: 2492.879826622782\n",
      "Loss: 560.1198735896048\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  92\n",
      "Gradient length: 2491.4932494113573\n",
      "Loss: 559.4992923641348\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  93\n",
      "Gradient length: 2490.1074496798874\n",
      "Loss: 558.8794012960539\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  94\n",
      "Gradient length: 2488.7224263158105\n",
      "Loss: 558.2601996151296\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  95\n",
      "Gradient length: 2487.338178280847\n",
      "Loss: 557.6416865522945\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  96\n",
      "Gradient length: 2485.954704603043\n",
      "Loss: 557.0238613395834\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  97\n",
      "Gradient length: 2484.5720043695637\n",
      "Loss: 556.4067232101357\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  98\n",
      "Gradient length: 2483.190076720341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 555.7902713981399\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  99\n",
      "Gradient length: 2481.808920842405\n",
      "Loss: 555.1745051388415\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  100\n",
      "Gradient length: 2480.428535964718\n",
      "Loss: 554.5594236684842\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  101\n",
      "Gradient length: 2479.048921353754\n",
      "Loss: 553.9450262243198\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  102\n",
      "Gradient length: 2477.6700763093836\n",
      "Loss: 553.3313120445747\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  103\n",
      "Gradient length: 2476.292000161352\n",
      "Loss: 552.7182803684465\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  104\n",
      "Gradient length: 2474.914692265985\n",
      "Loss: 552.1059304360749\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  105\n",
      "Gradient length: 2473.538152003432\n",
      "Loss: 551.4942614885565\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  106\n",
      "Gradient length: 2472.1623787750063\n",
      "Loss: 550.8832727678877\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  107\n",
      "Gradient length: 2470.787372000983\n",
      "Loss: 550.2729635170098\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  108\n",
      "Gradient length: 2469.4131311185506\n",
      "Loss: 549.663332979751\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  109\n",
      "Gradient length: 2468.0396555799985\n",
      "Loss: 549.0543804008506\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  110\n",
      "Gradient length: 2466.6669448511275\n",
      "Loss: 548.44610502594\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  111\n",
      "Gradient length: 2465.294998409802\n",
      "Loss: 547.8385061015282\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  112\n",
      "Gradient length: 2463.9238157446343\n",
      "Loss: 547.2315828750195\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  113\n",
      "Gradient length: 2462.5533963538937\n",
      "Loss: 546.625334594669\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  114\n",
      "Gradient length: 2461.1837397444374\n",
      "Loss: 546.0197605096283\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  115\n",
      "Gradient length: 2459.814845430901\n",
      "Loss: 545.4148598698911\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  116\n",
      "Gradient length: 2458.4467129347577\n",
      "Loss: 544.8106319263246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  117\n",
      "Gradient length: 2457.0793417836153\n",
      "Loss: 544.2070759306494\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  118\n",
      "Gradient length: 2455.7127315107377\n",
      "Loss: 543.6041911354317\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  119\n",
      "Gradient length: 2454.346881654247\n",
      "Loss: 543.0019767940978\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  120\n",
      "Gradient length: 2452.981791756736\n",
      "Loss: 542.4004321609164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  121\n",
      "Gradient length: 2451.6174613647877\n",
      "Loss: 541.7995564909993\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  122\n",
      "Gradient length: 2450.253890028608\n",
      "Loss: 541.1993490403061\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  123\n",
      "Gradient length: 2448.8910773015486\n",
      "Loss: 540.5998090656267\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  124\n",
      "Gradient length: 2447.5290227398964\n",
      "Loss: 540.0009358245923\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  125\n",
      "Gradient length: 2446.1677259025687\n",
      "Loss: 539.4027285756667\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  126\n",
      "Gradient length: 2444.8071863507903\n",
      "Loss: 538.8051865781534\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  127\n",
      "Gradient length: 2443.447403647917\n",
      "Loss: 538.2083090921813\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  128\n",
      "Gradient length: 2442.088377359259\n",
      "Loss: 537.6120953787079\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  129\n",
      "Gradient length: 2440.7301070518015\n",
      "Loss: 537.0165446995205\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  130\n",
      "Gradient length: 2439.372592294133\n",
      "Loss: 536.4216563172281\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  131\n",
      "Gradient length: 2438.0158326562732\n",
      "Loss: 535.8274294952827\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  132\n",
      "Gradient length: 2436.6598277095336\n",
      "Loss: 535.2338634979295\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  133\n",
      "Gradient length: 2435.3045770263793\n",
      "Loss: 534.6409575902632\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  134\n",
      "Gradient length: 2433.950080180393\n",
      "Loss: 534.0487110381821\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  135\n",
      "Gradient length: 2432.5963367461513\n",
      "Loss: 533.4571231084063\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  136\n",
      "Gradient length: 2431.2433462990716\n",
      "Loss: 532.8661930684862\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  137\n",
      "Gradient length: 2429.8911084154884\n",
      "Loss: 532.2759201867675\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  138\n",
      "Gradient length: 2428.5396226724256\n",
      "Loss: 531.6863037324348\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  139\n",
      "Gradient length: 2427.1888886476536\n",
      "Loss: 531.0973429754725\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  140\n",
      "Gradient length: 2425.8389059196056\n",
      "Loss: 530.509037186681\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  141\n",
      "Gradient length: 2424.4896740673275\n",
      "Loss: 529.9213856376773\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  142\n",
      "Gradient length: 2423.141192670414\n",
      "Loss: 529.3343876008831\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  143\n",
      "Gradient length: 2421.793461309002\n",
      "Loss: 528.7480423495417\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  144\n",
      "Gradient length: 2420.446479563752\n",
      "Loss: 528.1623491576897\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  145\n",
      "Gradient length: 2419.1002470157305\n",
      "Loss: 527.5773073001876\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  146\n",
      "Gradient length: 2417.7547632465253\n",
      "Loss: 526.9929160526864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  147\n",
      "Gradient length: 2416.4100278381275\n",
      "Loss: 526.4091746916654\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  148\n",
      "Gradient length: 2415.0660403729044\n",
      "Loss: 525.8260824943869\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  149\n",
      "Gradient length: 2413.722800433594\n",
      "Loss: 525.2436387389329\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  150\n",
      "Gradient length: 2412.380307603343\n",
      "Loss: 524.6618427041784\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  151\n",
      "Gradient length: 2411.038561465649\n",
      "Loss: 524.080693669808\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  152\n",
      "Gradient length: 2409.697561604302\n",
      "Loss: 523.5001909163078\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  153\n",
      "Gradient length: 2408.3573076034804\n",
      "Loss: 522.9203337249642\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  154\n",
      "Gradient length: 2407.0177990475904\n",
      "Loss: 522.3411213778503\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  155\n",
      "Gradient length: 2405.6790355214503\n",
      "Loss: 521.7625531578599\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  156\n",
      "Gradient length: 2404.341016610116\n",
      "Loss: 521.1846283486649\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  157\n",
      "Gradient length: 2403.003741898915\n",
      "Loss: 520.6073462347454\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  158\n",
      "Gradient length: 2401.6672109735355\n",
      "Loss: 520.030706101378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  159\n",
      "Gradient length: 2400.331423419863\n",
      "Loss: 519.4547072346281\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  160\n",
      "Gradient length: 2398.9963788240952\n",
      "Loss: 518.8793489213584\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  161\n",
      "Gradient length: 2397.662076772703\n",
      "Loss: 518.3046304492168\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  162\n",
      "Gradient length: 2396.3285168523894\n",
      "Loss: 517.7305511066658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  163\n",
      "Gradient length: 2394.99569865017\n",
      "Loss: 517.1571101829329\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  164\n",
      "Gradient length: 2393.6636217532655\n",
      "Loss: 516.5843069680469\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  165\n",
      "Gradient length: 2392.332285749164\n",
      "Loss: 516.0121407528388\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  166\n",
      "Gradient length: 2391.001690225624\n",
      "Loss: 515.4406108289027\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  167\n",
      "Gradient length: 2389.671834770642\n",
      "Loss: 514.8697164886427\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  168\n",
      "Gradient length: 2388.3427189724334\n",
      "Loss: 514.299457025239\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  169\n",
      "Gradient length: 2387.0143424195176\n",
      "Loss: 513.7298317326628\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  170\n",
      "Gradient length: 2385.68670470062\n",
      "Loss: 513.1608399056641\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  171\n",
      "Gradient length: 2384.359805404662\n",
      "Loss: 512.5924808397843\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  172\n",
      "Gradient length: 2383.0336441208983\n",
      "Loss: 512.024753831341\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  173\n",
      "Gradient length: 2381.7082204387434\n",
      "Loss: 511.4576581774475\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  174\n",
      "Gradient length: 2380.3835339479037\n",
      "Loss: 510.8911931759846\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  175\n",
      "Gradient length: 2379.059584238284\n",
      "Loss: 510.3253581256161\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  176\n",
      "Gradient length: 2377.736370900026\n",
      "Loss: 509.7601523257985\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  177\n",
      "Gradient length: 2376.413893523547\n",
      "Loss: 509.19557507674455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  178\n",
      "Gradient length: 2375.0921516994304\n",
      "Loss: 508.6316256794683\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  179\n",
      "Gradient length: 2373.7711450185457\n",
      "Loss: 508.06830343574705\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  180\n",
      "Gradient length: 2372.450873071991\n",
      "Loss: 507.5056076481411\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  181\n",
      "Gradient length: 2371.1313354510453\n",
      "Loss: 506.94353761998724\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  182\n",
      "Gradient length: 2369.812531747296\n",
      "Loss: 506.38209265538666\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  183\n",
      "Gradient length: 2368.494461552498\n",
      "Loss: 505.8212720592276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  184\n",
      "Gradient length: 2367.177124458644\n",
      "Loss: 505.26107513716175\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  185\n",
      "Gradient length: 2365.860520057993\n",
      "Loss: 504.7015011956183\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  186\n",
      "Gradient length: 2364.544647943002\n",
      "Loss: 504.14254954179887\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  187\n",
      "Gradient length: 2363.2295077063673\n",
      "Loss: 503.5842194836686\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  188\n",
      "Gradient length: 2361.915098941001\n",
      "Loss: 503.0265103299673\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  189\n",
      "Gradient length: 2360.6014212400623\n",
      "Loss: 502.4694213902051\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  190\n",
      "Gradient length: 2359.288474196906\n",
      "Loss: 501.9129519746574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  191\n",
      "Gradient length: 2357.976257405168\n",
      "Loss: 501.35710139436145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  192\n",
      "Gradient length: 2356.66477045865\n",
      "Loss: 500.801868961129\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  193\n",
      "Gradient length: 2355.354012951443\n",
      "Loss: 500.2472539875382\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  194\n",
      "Gradient length: 2354.0439844777966\n",
      "Loss: 499.693255786924\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  195\n",
      "Gradient length: 2352.734684632253\n",
      "Loss: 499.13987367338865\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient length: 2351.4261130095197\n",
      "Loss: 498.5871069617984\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  197\n",
      "Gradient length: 2350.118269204598\n",
      "Loss: 498.0349549677792\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  198\n",
      "Gradient length: 2348.811152812631\n",
      "Loss: 497.48341700771914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  199\n",
      "Gradient length: 2347.504763429078\n",
      "Loss: 496.9324923987659\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  200\n",
      "Gradient length: 2346.199100649551\n",
      "Loss: 496.3821804588266\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  201\n",
      "Gradient length: 2344.8941640699018\n",
      "Loss: 495.8324805065709\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  202\n",
      "Gradient length: 2343.5899532862836\n",
      "Loss: 495.28339186142085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  203\n",
      "Gradient length: 2342.2864678949304\n",
      "Loss: 494.73491384355617\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  204\n",
      "Gradient length: 2340.983707492446\n",
      "Loss: 494.1870457739148\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  205\n",
      "Gradient length: 2339.6816716755698\n",
      "Loss: 493.6397869741845\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  206\n",
      "Gradient length: 2338.380360041292\n",
      "Loss: 493.0931367668186\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  207\n",
      "Gradient length: 2337.07977218684\n",
      "Loss: 492.547094475012\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  208\n",
      "Gradient length: 2335.7799077096456\n",
      "Loss: 492.0016594227166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  209\n",
      "Gradient length: 2334.48076620734\n",
      "Loss: 491.45683093463674\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  210\n",
      "Gradient length: 2333.1823472778533\n",
      "Loss: 490.9126083362311\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  211\n",
      "Gradient length: 2331.884650519288\n",
      "Loss: 490.3689909537003\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  212\n",
      "Gradient length: 2330.587675529954\n",
      "Loss: 489.825978114002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  213\n",
      "Gradient length: 2329.291421908424\n",
      "Loss: 489.2835691448422\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  214\n",
      "Gradient length: 2327.9958892534737\n",
      "Loss: 488.74176337466463\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  215\n",
      "Gradient length: 2326.7010771641176\n",
      "Loss: 488.2005601326705\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  216\n",
      "Gradient length: 2325.406985239583\n",
      "Loss: 487.6599587488044\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  217\n",
      "Gradient length: 2324.113613079309\n",
      "Loss: 487.1199585537492\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  218\n",
      "Gradient length: 2322.8209602829475\n",
      "Loss: 486.5805588789452\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  219\n",
      "Gradient length: 2321.529026450436\n",
      "Loss: 486.0417590565654\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  220\n",
      "Gradient length: 2320.2378111818825\n",
      "Loss: 485.5035584195333\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  221\n",
      "Gradient length: 2318.9473140776104\n",
      "Loss: 484.9659563015023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  222\n",
      "Gradient length: 2317.6575347381845\n",
      "Loss: 484.4289520368817\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  223\n",
      "Gradient length: 2316.3684727643927\n",
      "Loss: 483.89254496080963\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  224\n",
      "Gradient length: 2315.0801277572596\n",
      "Loss: 483.356734409164\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  225\n",
      "Gradient length: 2313.792499317989\n",
      "Loss: 482.82151971857246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  226\n",
      "Gradient length: 2312.505587048054\n",
      "Loss: 482.28690022639086\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  227\n",
      "Gradient length: 2311.2193905490913\n",
      "Loss: 481.7528752707118\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  228\n",
      "Gradient length: 2309.9339094230536\n",
      "Loss: 481.21944419035935\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  229\n",
      "Gradient length: 2308.649143271991\n",
      "Loss: 480.6866063249217\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  230\n",
      "Gradient length: 2307.3650916982865\n",
      "Loss: 480.1543610146719\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  231\n",
      "Gradient length: 2306.0817543044686\n",
      "Loss: 479.6227076006674\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  232\n",
      "Gradient length: 2304.7991306933436\n",
      "Loss: 479.09164542466317\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  233\n",
      "Gradient length: 2303.5172204678925\n",
      "Loss: 478.561173829156\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  234\n",
      "Gradient length: 2302.236023231335\n",
      "Loss: 478.03129215738466\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  235\n",
      "Gradient length: 2300.9555385871236\n",
      "Loss: 477.50199975330315\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  236\n",
      "Gradient length: 2299.6757661389192\n",
      "Loss: 476.9732959616067\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  237\n",
      "Gradient length: 2298.3967054905893\n",
      "Loss: 476.44518012771124\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  238\n",
      "Gradient length: 2297.118356246257\n",
      "Loss: 475.9176515977623\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  239\n",
      "Gradient length: 2295.8407180102276\n",
      "Loss: 475.3907097186354\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  240\n",
      "Gradient length: 2294.5637903870556\n",
      "Loss: 474.8643538379306\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  241\n",
      "Gradient length: 2293.287572981502\n",
      "Loss: 474.3385833039762\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  242\n",
      "Gradient length: 2292.0120653985355\n",
      "Loss: 473.8133974658207\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  243\n",
      "Gradient length: 2290.73726724338\n",
      "Loss: 473.28879567324\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  244\n",
      "Gradient length: 2289.463178121458\n",
      "Loss: 472.764777276729\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  245\n",
      "Gradient length: 2288.189797638394\n",
      "Loss: 472.24134162750914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  246\n",
      "Gradient length: 2286.917125400076\n",
      "Loss: 471.7184880775204\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  247\n",
      "Gradient length: 2285.6451610125246\n",
      "Loss: 471.1962159794254\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  248\n",
      "Gradient length: 2284.3739040821106\n",
      "Loss: 470.67452468661196\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  249\n",
      "Gradient length: 2283.103354215305\n",
      "Loss: 470.153413553174\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  250\n",
      "Gradient length: 2281.8335110188723\n",
      "Loss: 469.6328819339269\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  251\n",
      "Gradient length: 2280.5643740997593\n",
      "Loss: 469.11292918441916\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  252\n",
      "Gradient length: 2279.295943065134\n",
      "Loss: 468.59355466089505\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  253\n",
      "Gradient length: 2278.028217522392\n",
      "Loss: 468.0747577203268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  254\n",
      "Gradient length: 2276.7611970791413\n",
      "Loss: 467.5565377204008\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  255\n",
      "Gradient length: 2275.4948813432193\n",
      "Loss: 467.038894019517\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  256\n",
      "Gradient length: 2274.229269922668\n",
      "Loss: 466.52182597678\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  257\n",
      "Gradient length: 2272.96436242576\n",
      "Loss: 466.0053329520265\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  258\n",
      "Gradient length: 2271.700158460966\n",
      "Loss: 465.48941430577986\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  259\n",
      "Gradient length: 2270.436657637006\n",
      "Loss: 464.97406939929465\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  260\n",
      "Gradient length: 2269.173859562791\n",
      "Loss: 464.45929759453253\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  261\n",
      "Gradient length: 2267.911763847441\n",
      "Loss: 463.94509825416145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  262\n",
      "Gradient length: 2266.65037010033\n",
      "Loss: 463.43147074155127\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  263\n",
      "Gradient length: 2265.389677931014\n",
      "Loss: 462.91841442079124\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  264\n",
      "Gradient length: 2264.129686949306\n",
      "Loss: 462.4059286566765\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  265\n",
      "Gradient length: 2262.870396765175\n",
      "Loss: 461.89401281469367\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  266\n",
      "Gradient length: 2261.61180698888\n",
      "Loss: 461.38266626106184\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  267\n",
      "Gradient length: 2260.353917230828\n",
      "Loss: 460.8718883626824\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  268\n",
      "Gradient length: 2259.096727101707\n",
      "Loss: 460.36167848716093\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  269\n",
      "Gradient length: 2257.8402362123556\n",
      "Loss: 459.8520360028247\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  270\n",
      "Gradient length: 2256.584444173877\n",
      "Loss: 459.3429602786884\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  271\n",
      "Gradient length: 2255.329350597604\n",
      "Loss: 458.8344506844708\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  272\n",
      "Gradient length: 2254.074955095014\n",
      "Loss: 458.3265065905971\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  273\n",
      "Gradient length: 2252.8212572778566\n",
      "Loss: 457.8191273681807\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  274\n",
      "Gradient length: 2251.5682567580834\n",
      "Loss: 457.3123123890512\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  275\n",
      "Gradient length: 2250.315953147881\n",
      "Loss: 456.8060610257239\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  276\n",
      "Gradient length: 2249.064346059632\n",
      "Loss: 456.3003726514111\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  277\n",
      "Gradient length: 2247.81343510592\n",
      "Loss: 455.7952466400326\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  278\n",
      "Gradient length: 2246.5632198995613\n",
      "Loss: 455.290682366199\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  279\n",
      "Gradient length: 2245.3137000536135\n",
      "Loss: 454.78667920521565\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  280\n",
      "Gradient length: 2244.064875181296\n",
      "Loss: 454.2832365330802\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  281\n",
      "Gradient length: 2242.816744896061\n",
      "Loss: 453.78035372648765\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  282\n",
      "Gradient length: 2241.569308811623\n",
      "Loss: 453.27803016282513\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  283\n",
      "Gradient length: 2240.3225665418404\n",
      "Loss: 452.77626522017806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  284\n",
      "Gradient length: 2239.076517700825\n",
      "Loss: 452.2750582773141\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  285\n",
      "Gradient length: 2237.8311619029173\n",
      "Loss: 451.7744087136917\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  286\n",
      "Gradient length: 2236.586498762622\n",
      "Loss: 451.27431590946895\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  287\n",
      "Gradient length: 2235.3425278947193\n",
      "Loss: 450.7747792454857\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  288\n",
      "Gradient length: 2234.0992489141604\n",
      "Loss: 450.27579810327836\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  289\n",
      "Gradient length: 2232.856661436105\n",
      "Loss: 449.7773718650531\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  290\n",
      "Gradient length: 2231.6147650759895\n",
      "Loss: 449.27949991371656\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  291\n",
      "Gradient length: 2230.373559449361\n",
      "Loss: 448.78218163287505\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  292\n",
      "Gradient length: 2229.1330441720943\n",
      "Loss: 448.28541640679316\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient length: 2227.893218860188\n",
      "Loss: 447.7892036204315\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  294\n",
      "Gradient length: 2226.6540831298994\n",
      "Loss: 447.2935426594432\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  295\n",
      "Gradient length: 2225.4156365976864\n",
      "Loss: 446.7984329101494\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  296\n",
      "Gradient length: 2224.1778788802453\n",
      "Loss: 446.303873759563\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  297\n",
      "Gradient length: 2222.9408095944164\n",
      "Loss: 445.80986459538843\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  298\n",
      "Gradient length: 2221.7044283573555\n",
      "Loss: 445.31640480598367\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  299\n",
      "Gradient length: 2220.4687347863187\n",
      "Loss: 444.8234937804085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  300\n",
      "Gradient length: 2219.2337284988816\n",
      "Loss: 444.33113090839765\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  301\n",
      "Gradient length: 2217.9994091127614\n",
      "Loss: 443.83931558036215\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  302\n",
      "Gradient length: 2216.7657762459144\n",
      "Loss: 443.3480471873932\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  303\n",
      "Gradient length: 2215.5328295164672\n",
      "Loss: 442.8573251212612\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  304\n",
      "Gradient length: 2214.300568542849\n",
      "Loss: 442.3671487744044\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  305\n",
      "Gradient length: 2213.068992943628\n",
      "Loss: 441.8775175399433\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  306\n",
      "Gradient length: 2211.838102337593\n",
      "Loss: 441.388430811672\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  307\n",
      "Gradient length: 2210.6078963437903\n",
      "Loss: 440.89988798406023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  308\n",
      "Gradient length: 2209.3783745813926\n",
      "Loss: 440.4118884522474\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  309\n",
      "Gradient length: 2208.149536669881\n",
      "Loss: 439.92443161205097\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  310\n",
      "Gradient length: 2206.9213822289025\n",
      "Loss: 439.43751685995665\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  311\n",
      "Gradient length: 2205.693910878274\n",
      "Loss: 438.9511435931158\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  312\n",
      "Gradient length: 2204.467122238105\n",
      "Loss: 438.4653112093698\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  313\n",
      "Gradient length: 2203.2410159286487\n",
      "Loss: 437.98001910720325\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  314\n",
      "Gradient length: 2202.0155915704363\n",
      "Loss: 437.4952666857876\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  315\n",
      "Gradient length: 2200.7908487841414\n",
      "Loss: 437.0110533449541\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  316\n",
      "Gradient length: 2199.566787190696\n",
      "Loss: 436.527378485209\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  317\n",
      "Gradient length: 2198.343406411223\n",
      "Loss: 436.0442415077189\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  318\n",
      "Gradient length: 2197.1207060670336\n",
      "Loss: 435.5616418143199\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  319\n",
      "Gradient length: 2195.8986857797167\n",
      "Loss: 435.0795788075157\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  320\n",
      "Gradient length: 2194.6773451710233\n",
      "Loss: 434.59805189045943\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  321\n",
      "Gradient length: 2193.4566838628734\n",
      "Loss: 434.1170604669849\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  322\n",
      "Gradient length: 2192.236701477508\n",
      "Loss: 433.6366039415864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  323\n",
      "Gradient length: 2191.017397637288\n",
      "Loss: 433.1566817194106\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  324\n",
      "Gradient length: 2189.7987719648163\n",
      "Loss: 432.6772932062788\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  325\n",
      "Gradient length: 2188.580824082884\n",
      "Loss: 432.19843780866836\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  326\n",
      "Gradient length: 2187.363553614539\n",
      "Loss: 431.7201149337109\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  327\n",
      "Gradient length: 2186.146960183005\n",
      "Loss: 431.2423239891904\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  328\n",
      "Gradient length: 2184.9310434116824\n",
      "Loss: 430.76506438357893\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  329\n",
      "Gradient length: 2183.7158029242673\n",
      "Loss: 430.288335525976\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  330\n",
      "Gradient length: 2182.501238344593\n",
      "Loss: 429.81213682615913\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  331\n",
      "Gradient length: 2181.287349296701\n",
      "Loss: 429.3364676945457\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  332\n",
      "Gradient length: 2180.0741354049255\n",
      "Loss: 428.8613275422162\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  333\n",
      "Gradient length: 2178.861596293719\n",
      "Loss: 428.38671578091015\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  334\n",
      "Gradient length: 2177.6497315877536\n",
      "Loss: 427.91263182301356\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  335\n",
      "Gradient length: 2176.4385409119586\n",
      "Loss: 427.4390750815708\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  336\n",
      "Gradient length: 2175.2280238914313\n",
      "Loss: 426.96604497027903\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  337\n",
      "Gradient length: 2174.018180151494\n",
      "Loss: 426.493540903485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  338\n",
      "Gradient length: 2172.8090093176875\n",
      "Loss: 426.02156229619106\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  339\n",
      "Gradient length: 2171.600511015704\n",
      "Loss: 425.5501085640409\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  340\n",
      "Gradient length: 2170.3926848715487\n",
      "Loss: 425.07917912333545\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  341\n",
      "Gradient length: 2169.185530511331\n",
      "Loss: 424.60877339102217\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  342\n",
      "Gradient length: 2167.9790475614086\n",
      "Loss: 424.13889078470504\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  343\n",
      "Gradient length: 2166.7732356483593\n",
      "Loss: 423.6695307226234\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  344\n",
      "Gradient length: 2165.568094398966\n",
      "Loss: 423.20069262366854\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  345\n",
      "Gradient length: 2164.3636234402065\n",
      "Loss: 422.7323759073834\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  346\n",
      "Gradient length: 2163.159822399263\n",
      "Loss: 422.2645799939445\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  347\n",
      "Gradient length: 2161.9566909035266\n",
      "Loss: 421.79730430418215\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  348\n",
      "Gradient length: 2160.7542285806408\n",
      "Loss: 421.33054825956964\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  349\n",
      "Gradient length: 2159.5524350583705\n",
      "Loss: 420.8643112822256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  350\n",
      "Gradient length: 2158.3513099647585\n",
      "Loss: 420.398592794906\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  351\n",
      "Gradient length: 2157.1508529280295\n",
      "Loss: 419.93339222100644\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  352\n",
      "Gradient length: 2155.9510635765982\n",
      "Loss: 419.46870898457274\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  353\n",
      "Gradient length: 2154.7519415391207\n",
      "Loss: 419.00454251028793\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  354\n",
      "Gradient length: 2153.5534864444376\n",
      "Loss: 418.5408922234732\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  355\n",
      "Gradient length: 2152.3556979216023\n",
      "Loss: 418.0777575500933\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  356\n",
      "Gradient length: 2151.158575599873\n",
      "Loss: 417.6151379167323\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  357\n",
      "Gradient length: 2149.9621191087203\n",
      "Loss: 417.15303275064684\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  358\n",
      "Gradient length: 2148.7663280778065\n",
      "Loss: 416.6914414796962\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  359\n",
      "Gradient length: 2147.5712021370055\n",
      "Loss: 416.23036353239615\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  360\n",
      "Gradient length: 2146.3767409164116\n",
      "Loss: 415.7697983378935\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  361\n",
      "Gradient length: 2145.1829440462852\n",
      "Loss: 415.3097453259646\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  362\n",
      "Gradient length: 2143.98981115716\n",
      "Loss: 414.8502039270301\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  363\n",
      "Gradient length: 2142.797341879709\n",
      "Loss: 414.3911735721341\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  364\n",
      "Gradient length: 2141.6055358448493\n",
      "Loss: 413.93265369295585\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  365\n",
      "Gradient length: 2140.414392683679\n",
      "Loss: 413.47464372181264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  366\n",
      "Gradient length: 2139.2239120275262\n",
      "Loss: 413.01714309164674\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  367\n",
      "Gradient length: 2138.0340935078966\n",
      "Loss: 412.5601512360324\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  368\n",
      "Gradient length: 2136.844936756531\n",
      "Loss: 412.10366758917127\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  369\n",
      "Gradient length: 2135.6564414053396\n",
      "Loss: 411.6476915858995\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  370\n",
      "Gradient length: 2134.4686070864964\n",
      "Loss: 411.192222661679\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  371\n",
      "Gradient length: 2133.2814334322834\n",
      "Loss: 410.73726025260504\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  372\n",
      "Gradient length: 2132.0949200752934\n",
      "Loss: 410.2828037953868\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  373\n",
      "Gradient length: 2130.9090666482325\n",
      "Loss: 409.82885272736905\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  374\n",
      "Gradient length: 2129.723872784096\n",
      "Loss: 409.3754064865261\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  375\n",
      "Gradient length: 2128.5393381160147\n",
      "Loss: 408.9224645114529\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  376\n",
      "Gradient length: 2127.3554622773536\n",
      "Loss: 408.47002624136286\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  377\n",
      "Gradient length: 2126.1722449016806\n",
      "Loss: 408.01809111609526\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  378\n",
      "Gradient length: 2124.98968562276\n",
      "Loss: 407.5666585761296\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  379\n",
      "Gradient length: 2123.807784074579\n",
      "Loss: 407.1157280625392\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  380\n",
      "Gradient length: 2122.6265398912833\n",
      "Loss: 406.6652990170397\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  381\n",
      "Gradient length: 2121.4459527072845\n",
      "Loss: 406.2153708819547\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  382\n",
      "Gradient length: 2120.266022157138\n",
      "Loss: 405.765943100244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  383\n",
      "Gradient length: 2119.086747875637\n",
      "Loss: 405.3170151154727\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  384\n",
      "Gradient length: 2117.9081294977964\n",
      "Loss: 404.8685863718233\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  385\n",
      "Gradient length: 2116.730166658762\n",
      "Loss: 404.42065631411657\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  386\n",
      "Gradient length: 2115.5528589939777\n",
      "Loss: 403.97322438775865\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  387\n",
      "Gradient length: 2114.3762061390007\n",
      "Loss: 403.5262900388047\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  388\n",
      "Gradient length: 2113.2002077296465\n",
      "Loss: 403.0798527139023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  389\n",
      "Gradient length: 2112.024863401933\n",
      "Loss: 402.6339118603308\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient length: 2110.850172792031\n",
      "Loss: 402.1884669259685\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  391\n",
      "Gradient length: 2109.6761355363724\n",
      "Loss: 401.7435173593226\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  392\n",
      "Gradient length: 2108.502751271561\n",
      "Loss: 401.29906260950435\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  393\n",
      "Gradient length: 2107.3300196344235\n",
      "Loss: 400.8551021262434\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  394\n",
      "Gradient length: 2106.157940261957\n",
      "Loss: 400.4116353598729\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  395\n",
      "Gradient length: 2104.9865127913813\n",
      "Loss: 399.9686617613455\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  396\n",
      "Gradient length: 2103.8157368601037\n",
      "Loss: 399.5261807822268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  397\n",
      "Gradient length: 2102.6456121057618\n",
      "Loss: 399.08419187467655\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  398\n",
      "Gradient length: 2101.47613816616\n",
      "Loss: 398.64269449147895\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  399\n",
      "Gradient length: 2100.3073146793336\n",
      "Loss: 398.20168808602875\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  400\n",
      "Gradient length: 2099.139141283501\n",
      "Loss: 397.7611721123134\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  401\n",
      "Gradient length: 2097.971617617084\n",
      "Loss: 397.3211460249357\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  402\n",
      "Gradient length: 2096.8047433187207\n",
      "Loss: 396.8816092791125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  403\n",
      "Gradient length: 2095.6385180272196\n",
      "Loss: 396.4425613306546\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  404\n",
      "Gradient length: 2094.472941381628\n",
      "Loss: 396.0040016359828\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  405\n",
      "Gradient length: 2093.308013021151\n",
      "Loss: 395.56592965212633\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  406\n",
      "Gradient length: 2092.14373258524\n",
      "Loss: 395.1283448367061\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  407\n",
      "Gradient length: 2090.980099713521\n",
      "Loss: 394.69124664796\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  408\n",
      "Gradient length: 2089.8171140458116\n",
      "Loss: 394.25463454472185\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  409\n",
      "Gradient length: 2088.654775222154\n",
      "Loss: 393.81850798642864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  410\n",
      "Gradient length: 2087.4930828827546\n",
      "Loss: 393.3828664331193\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  411\n",
      "Gradient length: 2086.3320366681023\n",
      "Loss: 392.9477093454275\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  412\n",
      "Gradient length: 2085.1716362187653\n",
      "Loss: 392.5130361845968\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  413\n",
      "Gradient length: 2084.011881175601\n",
      "Loss: 392.078846412456\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  414\n",
      "Gradient length: 2082.852771179641\n",
      "Loss: 391.64513949145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  415\n",
      "Gradient length: 2081.694305872104\n",
      "Loss: 391.2119148846042\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  416\n",
      "Gradient length: 2080.5364848944346\n",
      "Loss: 390.77917205555565\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  417\n",
      "Gradient length: 2079.3793078882563\n",
      "Loss: 390.3469104685316\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  418\n",
      "Gradient length: 2078.2227744954093\n",
      "Loss: 389.91512958834704\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  419\n",
      "Gradient length: 2077.0668843578947\n",
      "Loss: 389.4838288804256\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  420\n",
      "Gradient length: 2075.911637117949\n",
      "Loss: 389.0530078107817\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  421\n",
      "Gradient length: 2074.7570324180156\n",
      "Loss: 388.622665846017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  422\n",
      "Gradient length: 2073.603069900696\n",
      "Loss: 388.1928024533336\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  423\n",
      "Gradient length: 2072.4497492088294\n",
      "Loss: 387.76341710052293\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  424\n",
      "Gradient length: 2071.297069985427\n",
      "Loss: 387.3345092559657\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  425\n",
      "Gradient length: 2070.145031873709\n",
      "Loss: 386.90607838864366\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  426\n",
      "Gradient length: 2068.993634517102\n",
      "Loss: 386.478123968122\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  427\n",
      "Gradient length: 2067.8428775592297\n",
      "Loss: 386.0506454645438\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  428\n",
      "Gradient length: 2066.6927606438917\n",
      "Loss: 385.6236423486692\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  429\n",
      "Gradient length: 2065.543283415129\n",
      "Loss: 385.1971140918244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  430\n",
      "Gradient length: 2064.3944455171086\n",
      "Loss: 384.77106016592836\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  431\n",
      "Gradient length: 2063.2462465942676\n",
      "Loss: 384.34548004349136\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  432\n",
      "Gradient length: 2062.0986862912073\n",
      "Loss: 383.9203731976194\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  433\n",
      "Gradient length: 2060.9517642527544\n",
      "Loss: 383.4957391019738\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  434\n",
      "Gradient length: 2059.805480123875\n",
      "Loss: 383.0715772308321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  435\n",
      "Gradient length: 2058.659833549797\n",
      "Loss: 382.64788705904556\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  436\n",
      "Gradient length: 2057.5148241759025\n",
      "Loss: 382.2246680620485\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  437\n",
      "Gradient length: 2056.3704516478056\n",
      "Loss: 381.8019197158574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  438\n",
      "Gradient length: 2055.22671561126\n",
      "Loss: 381.3796414970722\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  439\n",
      "Gradient length: 2054.0836157123017\n",
      "Loss: 380.9578328828778\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  440\n",
      "Gradient length: 2052.941151597064\n",
      "Loss: 380.53649335104353\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  441\n",
      "Gradient length: 2051.7993229119857\n",
      "Loss: 380.11562237990574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  442\n",
      "Gradient length: 2050.6581293036074\n",
      "Loss: 379.69521944839596\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  443\n",
      "Gradient length: 2049.5175704187172\n",
      "Loss: 379.2752840360162\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  444\n",
      "Gradient length: 2048.3776459042792\n",
      "Loss: 378.8558156228529\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  445\n",
      "Gradient length: 2047.2383554074722\n",
      "Loss: 378.43681368956567\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  446\n",
      "Gradient length: 2046.099698575653\n",
      "Loss: 378.0182777173907\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  447\n",
      "Gradient length: 2044.9616750564037\n",
      "Loss: 377.60020718815093\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  448\n",
      "Gradient length: 2043.8242844974407\n",
      "Loss: 377.18260158423493\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  449\n",
      "Gradient length: 2042.6875265467618\n",
      "Loss: 376.76546038861585\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  450\n",
      "Gradient length: 2041.5514008524772\n",
      "Loss: 376.34878308482973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  451\n",
      "Gradient length: 2040.4159070629619\n",
      "Loss: 375.93256915699953\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  452\n",
      "Gradient length: 2039.2810448267232\n",
      "Loss: 375.5168180898128\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  453\n",
      "Gradient length: 2038.1468137925265\n",
      "Loss: 375.10152936853456\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  454\n",
      "Gradient length: 2037.013213609288\n",
      "Loss: 374.68670247900343\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  455\n",
      "Gradient length: 2035.8802439261353\n",
      "Loss: 374.2723369076246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  456\n",
      "Gradient length: 2034.7479043923897\n",
      "Loss: 373.8584321413837\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  457\n",
      "Gradient length: 2033.6161946575833\n",
      "Loss: 373.4449876678281\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  458\n",
      "Gradient length: 2032.4851143713988\n",
      "Loss: 373.0320029750737\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  459\n",
      "Gradient length: 2031.3546631837587\n",
      "Loss: 372.6194775518138\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  460\n",
      "Gradient length: 2030.2248407447628\n",
      "Loss: 372.2074108873036\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  461\n",
      "Gradient length: 2029.0956467046922\n",
      "Loss: 371.79580247137477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  462\n",
      "Gradient length: 2027.9670807140694\n",
      "Loss: 371.38465179441556\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  463\n",
      "Gradient length: 2026.8391424235356\n",
      "Loss: 370.9739583473842\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  464\n",
      "Gradient length: 2025.7118314840077\n",
      "Loss: 370.5637216218114\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  465\n",
      "Gradient length: 2024.5851475465345\n",
      "Loss: 370.15394110979065\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  466\n",
      "Gradient length: 2023.4590902623975\n",
      "Loss: 369.7446163039697\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  467\n",
      "Gradient length: 2022.3336592830465\n",
      "Loss: 369.33574669757616\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  468\n",
      "Gradient length: 2021.2088542601368\n",
      "Loss: 368.92733178439073\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  469\n",
      "Gradient length: 2020.08467484552\n",
      "Loss: 368.51937105876493\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  470\n",
      "Gradient length: 2018.9611206912484\n",
      "Loss: 368.11186401559956\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  471\n",
      "Gradient length: 2017.8381914495321\n",
      "Loss: 367.70481015037495\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  472\n",
      "Gradient length: 2016.715886772822\n",
      "Loss: 367.2982089591194\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  473\n",
      "Gradient length: 2015.5942063137297\n",
      "Loss: 366.89205993842427\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  474\n",
      "Gradient length: 2014.4731497250866\n",
      "Loss: 366.48636258544326\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  475\n",
      "Gradient length: 2013.352716659882\n",
      "Loss: 366.08111639789223\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  476\n",
      "Gradient length: 2012.2329067713015\n",
      "Loss: 365.6763208740339\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  477\n",
      "Gradient length: 2011.1137197127887\n",
      "Loss: 365.27197551270166\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  478\n",
      "Gradient length: 2009.995155137897\n",
      "Loss: 364.8680798132757\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  479\n",
      "Gradient length: 2008.8772127004045\n",
      "Loss: 364.4646332757052\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  480\n",
      "Gradient length: 2007.7598920542932\n",
      "Loss: 364.06163540048493\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  481\n",
      "Gradient length: 2006.6431928537456\n",
      "Loss: 363.6590856886683\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  482\n",
      "Gradient length: 2005.527114753088\n",
      "Loss: 363.25698364186593\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  483\n",
      "Gradient length: 2004.4116574068805\n",
      "Loss: 362.8553287622374\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  484\n",
      "Gradient length: 2003.2968204698548\n",
      "Loss: 362.4541205525012\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  485\n",
      "Gradient length: 2002.1826035969768\n",
      "Loss: 362.0533585159293\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  486\n",
      "Gradient length: 2001.0690064433377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 361.6530421563392\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  487\n",
      "Gradient length: 1999.956028664281\n",
      "Loss: 361.25317097811165\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  488\n",
      "Gradient length: 1998.843669915296\n",
      "Loss: 360.853744486163\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  489\n",
      "Gradient length: 1997.7319298520824\n",
      "Loss: 360.45476218597884\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  490\n",
      "Gradient length: 1996.6208081305501\n",
      "Loss: 360.05622358357516\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  491\n",
      "Gradient length: 1995.5103044067741\n",
      "Loss: 359.65812818553064\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  492\n",
      "Gradient length: 1994.400418337009\n",
      "Loss: 359.26047549896464\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  493\n",
      "Gradient length: 1993.2911495777682\n",
      "Loss: 358.8632650315578\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  494\n",
      "Gradient length: 1992.1824977856656\n",
      "Loss: 358.46649629152415\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  495\n",
      "Gradient length: 1991.0744626175658\n",
      "Loss: 358.0701687876276\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  496\n",
      "Gradient length: 1989.967043730509\n",
      "Loss: 357.67428202918427\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  497\n",
      "Gradient length: 1988.8602407817186\n",
      "Loss: 357.2788355260513\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  498\n",
      "Gradient length: 1987.7540534286231\n",
      "Loss: 356.88382878863024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration no. :  499\n",
      "Gradient length: 1986.6484813288325\n",
      "Loss: 356.4892613278679\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_init = [ 1,1 , 0.999, -0.32,  1 , 1 ,1 , 0.343,  0.336,  1,  0.91]\n",
    "weights,losses = gradDescent(w_init,0.0000001,X,datY)\n",
    "#initial weights taken from previous iterations output weights \n",
    "#initial loss is much less for these weights\n",
    "\n",
    "\n",
    "#compatible initial weights w_init = [ 1,1 , 0.999, -0.32,  1 , 1 ,1 , 0.343,  0.336,  1,  0.91]\n",
    "#compatible learning rate = 0.0000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the adequacy of the model we created.\n",
    "\n",
    "Choose several (no less then five) houses (inputs in your `X` matrix) and calculte predicted prices by:\n",
    "\n",
    "$$ \\hat{y}_i=w_0+w_1x_1+w_2x_2+\\ldots+w_mx_m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      Actual\n",
      "17.707511902728328        12.936033799212835\n",
      "13.635629973248484        13.222208712374044\n",
      "13.634109033703197        13.771558670435091\n",
      "2.8983297301888626        12.730801174465157\n",
      "30.769160498634434        13.805460222110773\n"
     ]
    }
   ],
   "source": [
    "final_weights = weights[-1]\n",
    "train_x = X[3:8]\n",
    "y_predicted = np.zeros(5)\n",
    "y_actual = datY[3:8]\n",
    "for j in range(5):\n",
    "        y_predicted[j] = final_weights[0] + final_weights[1]*train_x[j][0] + final_weights[2]*train_x[j][1] + final_weights[3]*train_x[j][2] + final_weights[4]*train_x[j][3] + final_weights[5]*train_x[j][4] + final_weights[6]*train_x[j][5] + final_weights[7]*train_x[j][6] + final_weights[8]*train_x[j][7] + final_weights[9]*train_x[j][8] + final_weights[10]*train_x[j][9]\n",
    "print(\"Predicted      Actual\")\n",
    "for i in range(5):\n",
    "    print(y_predicted[i],\"      \",y_actual[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig = plt.figure(figsize=(18,10))\n",
    "plt.plot(y_predicted,color='red', label='prediction')\n",
    "plt.plot(y_actual,color='blue', label='y_test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predicted values with an actual answer (stored in your `y` array). Is it satisfying enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values of preducted prices converge with the actual prices but some deviate quite a lot. If model is allowed to run more number of iterations, this divergence can be decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
